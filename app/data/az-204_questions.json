[
  {
    "number": "1",
    "question": "You have two Hyper-V hosts named Host1 and Host2. Host1 has an Azure virtual\nmachine named VM1 that was deployed by using a custom Azure Resource\nManager template.\nYou need to move VM1 to Host2.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "From the Update management blade, click Enable.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From the Overview blade, move VM1 to a different subscription.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "From the Redeploy blade, click Redeploy.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "From the Profile blade, modify the usage location.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure\nand then powers it back on, retaining all your configuration options and associated\nresources.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-\nnode"
  },
  {
    "number": "2",
    "question": "DRAG DROP -\nYou have downloaded an Azure Resource Manager template to deploy\nnumerous virtual machines. The template is based on a current virtual machine,\nbut must be adapted to reference an administrative password.\nYou need to make sure that the password is not stored in plain text.\nYou are preparing to create the necessary components to achieve your goal.\nWhich of the following should you create to achieve your goal? Answer by\ndragging the correct option from the list to the answer area.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nKey Vault + Access Policy. Using Key Vault we create a secret containing our Password:\nhttps://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-portal . Using an\nAccess Policy we allow access to the previously created secret. Documentation Guide:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/key-vault-\nparameter?tabs=azure-cli\nIs it me, or is the question misleading since it says \"option\" - singuar?\n\n\nhttps://www.secexams.com/exams/microsoft/az-104/view/2/ Answer is correct, see\nquestion in AZ104 #20\nKey Vault + Access Policy, Is the correct but the Access Policy option is legacy now, and\nmost likely will be replaced with the option of Azure role-based access control instead of\nAccess Policy\nI had the same problem",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "3",
    "question": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage\nfrom an Azure AD-joined device. The cluster is located in a resource group.\nDevelopers have created an application named MyApp. MyApp was packaged\ninto a container image.\nYou need to deploy the YAML manifest file for the application.\nSolution: You install the Azure CLI on the device and run the kubectl apply `\"f\nmyapp.yaml command.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "kubectl apply -f myapp.yaml applies a configuration change to a resource from a file or\nstdin.\nReference:\nhttps://kubernetes.io/docs/reference/kubectl/overview/\nhttps://docs.microsoft.com/en-us/cli/azure/aks"
  },
  {
    "number": "4",
    "question": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage\nfrom an Azure AD-joined device. The cluster is located in a resource group.\nDevelopers have created an application named MyApp. MyApp was packaged\ninto a container image.\nYou need to deploy the YAML manifest file for the application.\nSolution: You install the docker client on the device and run the docker run -it\nmicrosoft/azure-cli:0.10.17 command.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "5",
    "question": "Your company has a web app named WebApp1.\nYou use the WebJobs SDK to design a triggered App Service background task\nthat automatically invokes a function in the code every time new data is\nreceived in a queue.\nYou are preparing to configure the service processes a queue data item.\nWhich of the following is the service you should use?",
    "options": [
      {
        "letter": "A",
        "text": "Logic Apps",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "WebJobs",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Flow",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Functions",
        "is_correct": false
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "6",
    "question": "Your company has an Azure subscription.\nYou need to deploy a number of Azure virtual machines to the subscription by\nusing Azure Resource Manager (ARM) templates. The virtual machines will be\nincluded in a single availability set.\nYou need to ensure that the ARM template allows for as many virtual machines\nas possible to remain accessible in the event of fabric failure or maintenance.\nWhich of the following is the value that you should configure for the\nplatformFaultDomainCount property?",
    "options": [
      {
        "letter": "A",
        "text": "10",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "30",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Min Value",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Max Value",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "The number of fault domains for managed availability sets varies by region - either two\nor three per region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/manage-availability"
  },
  {
    "number": "7",
    "question": "Your company has an Azure subscription.\nYou need to deploy a number of Azure virtual machines to the subscription by\nusing Azure Resource Manager (ARM) templates. The virtual machines will be\nincluded in a single availability set.\nYou need to ensure that the ARM template allows for as many virtual machines\nas possible to remain accessible in the event of fabric failure or maintenance.\nWhich of the following is the value that you should configure for the\nplatformUpdateDomainCount property?",
    "options": [
      {
        "letter": "A",
        "text": "10",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "20",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "30",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "40",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Each virtual machine in your availability set is assigned an update domain and a fault\ndomain by the underlying Azure platform. For a given availability set, five non-user-\nconfigurable update domains are assigned by default (Resource Manager deployments\ncan then be increased to provide up to 20 update domains) to indicate groups of virtual\nmachines and underlying physical hardware that can be rebooted at the same time.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/manage-availability"
  },
  {
    "number": "8",
    "question": "DRAG DROP -\nYou are creating an Azure Cosmos DB account that makes use of the SQL API.\nData will be added to the account every day by a web application.\nYou need to ensure that an email notification is sent when information is\nreceived from IoT devices, and that compute cost is reduced.\nYou decide to deploy a function app.\nWhich of the following should you configure the function app to use? Answer by\ndragging the correct options from the list to the answer area.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAnswer is correct: Consumption plan will reduce the cost and SendGrid is used to send\nemails from azure functions. https://docs.microsoft.com/en-us/azure/azure-functions/\nfunctions-bindings-sendgrid?tabs=in-process%2Cfunctionsv2&pivots=programming-\nlanguage-csharp\nevent hub might be able to be used because it's an event, and from IoT - But generally,\nEvent hub is for a massive amount of processing from many IoT devices. Sounds\nexpensive, right?\n\n\nevent hub might be able to be used because it's an event, and from IoT - But generally,\nEvent hub is for a massive amount of processing from many IoT devices. Sounds\nexpensive, right?\nevent hub might be able to be used because it's an event, and from IoT - But generally,\nEvent hub is for a massive amount of processing from many IoT devices. Sounds\nexpensive, right?\nevent hub might be able to be used because it's an event, and from IoT - But generally,\nEvent hub is for a massive amount of processing from many IoT devices. Sounds\nexpensive, right?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "9",
    "question": "This question requires that you evaluate the underlined text to determine if it is\ncorrect.\nYou company has an on-premises deployment of MongoDB, and an Azure\nCosmos DB account that makes use of the MongoDB API.\nYou need to devise a strategy to migrate MongoDB to the Azure Cosmos DB\naccount.\nYou include the Data Management Gateway tool in your migration strategy.\nInstructions: Review the underlined text. If it makes the statement correct,\nselect `No change required.` If the statement is incorrect, select the answer\nchoice that makes the statement correct.",
    "options": [
      {
        "letter": "A",
        "text": "No change required",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "mongorestore",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Storage Explorer",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "AzCopy",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/mongodb-migrate https://\ndocs.mongodb.com/manual/reference/program/mongorestore/"
  },
  {
    "number": "10",
    "question": "You are developing an e-Commerce Web App.\nYou want to use Azure Key Vault to ensure that sign-ins to the e-Commerce Web\nApp are secured by using Azure App Service authentication and Azure Active\nDirectory (AAD).\nWhat should you do on the e-Commerce Web App?",
    "options": [
      {
        "letter": "A",
        "text": "Run the az keyvault secret command.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable Azure AD Connect.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Enable Managed Service Identity (MSI).",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create an Azure AD service principal.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "A managed identity from Azure Active Directory allows your app to easily access other\nAAD-protected resources such as Azure Key Vault.\nReference:"
  },
  {
    "number": "11",
    "question": "This question requires that you evaluate the underlined text to determine if it is\ncorrect.\nYour Azure Active Directory Azure (Azure AD) tenant has an Azure subscription\nlinked to it.\nYour developer has created a mobile application that obtains Azure AD access\ntokens using the OAuth 2 implicit grant type.\nThe mobile application must be registered in Azure AD.\nYou require a redirect URI from the developer for registration purposes.\nInstructions: Review the underlined text. If it makes the statement correct,\nselect `No change is needed.` If the statement is incorrect, select the answer\nchoice that makes the statement correct.",
    "options": [
      {
        "letter": "A",
        "text": "No change required.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a secret",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a login hint",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a client ID",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "For Native Applications you need to provide a Redirect URI, which Azure AD will use to\nreturn token responses.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/v1-protocols-oauth-\ncode"
  },
  {
    "number": "12",
    "question": "You are creating an Azure key vault using PowerShell. Objects deleted from the\nkey vault must be kept for a set period of 90 days.\nWhich two of the following parameters must be used in conjunction to meet the\nrequirement? (Choose two.)",
    "options": [
      {
        "letter": "A",
        "text": "EnabledForDeployment",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "EnablePurgeProtection",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "EnabledForTemplateDeployment",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "EnableSoftDelete",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/azurerm.keyvault/new-\nazurermkeyvault https://docs.microsoft.com/en-us/azure/key-vault/key-vault-ovw-soft-\ndelete"
  },
  {
    "number": "13",
    "question": "HOTSPOT -\nYou have an Azure Active Directory (Azure AD) tenant.\nYou want to implement multi-factor authentication by making use of a\nconditional access policy. The conditional access policy must be applied to all\nusers when they access the Azure portal.\nWhich three settings should you configure? To answer, select the appropriate\nsettings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1:\nThe conditional access policy must be applied or assigned to Users and Groups.\nBox 2:\nThe conditional access policy must be applied when users access the Azure portal, which\nis a cloud app. That is: Microsoft Azure Management\n\n\nBox 3:\nAccess control must require multi-factor authentication when granting access.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/app-based-\nmfa\nCommunity Discussion\nMicrosoft wants a developer to remember their unfortunate choice to put the\nrequirement (condition) for MFA not in the \"conditions\" tab, but in the \"grants\"? And the\npeople go along with these without complaining and just trying to remember this totally\nuseless information?\nThe answer is correct! Please refer to: https://docs.microsoft.com/en-us/azure/active-\ndirectory/conditional-access/howto-conditional-access-policy-azure-management\nThe answer seems correct: https://youtu.be/FxW8vLxAjSk?t=463\nThe answer is correct. It's explicit here: https://learn.microsoft.com/en-us/azure/active-\ndirectory/conditional-access/howto-conditional-access-policy-all-users-mfa#create-a-\nconditional-access-policy\nAssignments: Include all users or a specific group that you want to apply the policy to.\nCloud apps or actions: Select \"Microsoft Azure Management\" or \"Microsoft Azure portal\".\nAccess controls: Configure the policy to require multi-factor authentication.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "14",
    "question": "You manage an Azure SQL database that allows for Azure AD authentication.\nYou need to make sure that database developers can connect to the SQL\ndatabase via Microsoft SQL Server Management Studio (SSMS). You also need to\nmake sure the developers use their on-premises Active Directory account for\nauthentication. Your strategy should allow for authentication prompts to be\nkept to a minimum.\nWhich of the following should you implement?",
    "options": [
      {
        "letter": "A",
        "text": "Azure AD token.\nPage 40 of 1410\n41 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Multi-Factor authentication.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Active Directory integrated authentication.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "OATH software tokens.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure AD can be the initial Azure AD managed domain. Azure AD can also be an on-\npremises Active Directory Domain Services that is federated with the Azure\nAD.\nUsing an Azure AD identity to connect using SSMS or SSDT\nThe following procedures show you how to connect to a SQL database with an Azure AD\nidentity using SQL Server Management Studio or SQL Server Database\nTools.\nActive Directory integrated authentication\nUse this method if you are logged in to Windows using your Azure Active Directory\ncredentials from a federated domain.\n1. Start Management Studio or Data Tools and in the Connect to Server (or Connect to\nDatabase Engine) dialog box, in the Authentication box, select Active\nDirectory - Integrated. No password is needed or can be entered because your existing\ncredentials will be presented for the connection.\n2. Select the Options button, and on the Connection Properties page, in the Connect to"
  },
  {
    "number": "15",
    "question": "You are developing an application to transfer data between on-premises file\nservers and Azure Blob storage. The application stores keys, secrets, and\ncertificates in Azure Key Vault and makes use of the Azure Key Vault APIs.\nYou want to configure the application to allow recovery of an accidental\ndeletion of the key vault or key vault objects for 90 days after deletion.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Run the Add-AzKeyVaultKey cmdlet.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Run the az keyvault update --enable-soft-delete true --enable-purge-protection true CLI.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Implement virtual network service endpoints for Azure Key Vault.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Run the az keyvault update --enable-soft-delete false CLI.\nPage 42 of 1410\n43 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "When soft-delete is enabled, resources marked as deleted resources are retained for a\nspecified period (90 days by default). The service further provides a mechanism for\nrecovering the deleted object, essentially undoing the deletion.\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge\nprotection can only be enabled once soft-delete is enabled.\nWhen purge protection is on, a vault or an object in the deleted state cannot be purged\nuntil the retention period has passed. Soft-deleted vaults and objects can still be\nrecovered, ensuring that the retention policy will be followed.\nThe default retention period is 90 days, but it is possible to set the retention policy\ninterval to a value from 7 to 90 days through the Azure portal. Once the retention policy\ninterval is set and saved it cannot be changed for that vault.\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/overview-soft-delete"
  },
  {
    "number": "16",
    "question": "HOTSPOT -\nYou have developed a Web App for your company. The Web App provides\nservices and must run in multiple regions.\nYou want to be notified whenever the Web App uses more than 85 percent of\nthe available CPU cores over a 5 minute period. Your solution must minimize\ncosts.\nWhich command should you use? To answer, select the appropriate settings in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/sv-se/cli/azure/monitor/metrics/alert\nCommunity Discussion\nFor anyone wondering why it's --window-size and not --evaluation-frequency: you want\nthe average across 5 minutes. With --evaluation-frequency you don't go for averages, you\nsimply check what the given value is at specific intervals.\nCorrect, Eng reference is here https://docs.microsoft.com/en-us/cli/azure/monitor/\nmetrics/alert?view=azure-cli-latest\nWindow size is the period of time over which Azure Monitor will collect metrics for a\nmetric alert rule. Evaluation frequency is the frequency with which Azure Monitor will\nevaluate the metric alert rule for violations. if you set the window size to 5 minutes and\nthe evaluation frequency to 1 minute, Azure Monitor will collect metrics for the previous\n5 minutes and then evaluate the metric alert rule for violations every minute. This means\nthat if there is a violation in the 5 minute window, Azure Monitor will detect it within 1\nminute. We want to get percent “over a 5 minute period”, we have to use window size\nparameter. Only use evaluation frequency parameter is not enough for getting question\nrequest.\nAccording to 2bruhornot2bruh explanation, Windows size is the correct answer.\nAccording to 2bruhornot2bruh explanation, Windows size is the correct answer.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "17",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The\napplication makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users'\nstreaming experience is constant. You also want to configure the application to\nstore data in a geographic location that is nearest to the user.\nSolution: You include the use of Azure Redis Cache in your design.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "18",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The\napplication makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users'\nstreaming experience is constant. You also want to configure the application to\nstore data in a geographic location that is nearest to the user.\nSolution: You include the use of an Azure Content Delivery Network (CDN) in\nyour design.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-in/azure/cdn/"
  },
  {
    "number": "19",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The\napplication makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users'\nstreaming experience is constant. You also want to configure the application to\nstore data in a geographic location that is nearest to the user.\nSolution: You include the use of a Storage Area Network (SAN) in your design.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "20",
    "question": "You develop a Web App on a tier D1 app service plan.\nYou notice that page load times increase during periods of peak traffic.\nYou want to implement automatic scaling when CPU load is above 80 percent.\nYour solution must minimize costs.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Enable autoscaling on the Web App.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Switch to the Premium App Service tier plan.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Switch to the Standard App Service tier plan.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Switch to the Azure App Services consumption plan.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Configure the web app to the Standard App Service Tier. The Standard tier supports auto-"
  },
  {
    "number": "21",
    "question": "Your company's Azure subscription includes an Azure Log Analytics workspace.\nYour company has a hundred on-premises servers that run either Windows\nServer 2012 R2 or Windows Server 2016, and is linked to the Azure Log Analytics\nworkspace. The Azure Log Analytics workspace is set up to gather performance\ncounters associated with security from these linked servers.\nYou must configure alerts based on the information gathered by the Azure Log\nAnalytics workspace.\nYou have to make sure that alert rules allow for dimensions, and that alert\ncreation time should be kept to a minimum. Furthermore, a single alert\nnotification must be created when the alert is created and when the alert is\nresolved.\nYou need to make use of the necessary signal type when creating the alert\nrules.\nWhich of the following is the option you should use?",
    "options": [
      {
        "letter": "A",
        "text": "The Activity log signal type.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "The Application Log signal type.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "The Metric signal type.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "The Audit Log signal type.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Metric alerts in Azure Monitor provide a way to get notified when one of your metrics\ncross a threshold. Metric alerts work on a range of multi-dimensional platform metrics,\ncustom metrics, Application Insights standard and custom metrics.\nNote: Signals are emitted by the target resource and can be of several types. Metric,\nActivity log, Application Insights, and Log.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/platform/alerts-metric"
  },
  {
    "number": "22",
    "question": "You are developing a .NET Core MVC application that allows customers to\nresearch independent holiday accommodation providers.\nYou want to implement Azure Search to allow the application to search the\nindex by using various criteria to locate documents related to accommodation.\nYou want the application to allow customers to search the index by using\nregular expressions.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure the SearchMode property of the SearchParameters class.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure the QueryType property of the SearchParameters class.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Configure the Facets property of the SearchParameters class.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the Filter property of the SearchParameters class.\nPage 52 of 1410\n53 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The SearchParameters.QueryType Property gets or sets a value that specifies the syntax\nof the search query. The default is 'simple'. Use 'full' if your query uses the Lucene query\nsyntax.\nYou can write queries against Azure Search based on the rich Lucene Query Parser syntax\nfor specialized query forms: wildcard, fuzzy search, proximity search, regular expressions\nare a few examples.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.search.models.searchparameters https://docs.microsoft.com/en-us/\ndotnet/api/microsoft.azure.search.models.searchparameters.querytype"
  },
  {
    "number": "23",
    "question": "You are a developer at your company.\nYou need to update the definitions for an existing Logic App.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "the Enterprise Integration Pack (EIP)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "the Logic App Code View",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "the API Connections",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "the Logic Apps Designer",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Edit JSON - Azure portal -\n1. Sign in to the Azure portal.\n2. From the left menu, choose All services. In the search box, find \"logic apps\", and then\nfrom the results, select your logic app.\n3. On your logic app's menu, under Development Tools, select Logic App Code View.\n4. The Code View editor opens and shows your logic app definition in JSON format.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-\noverview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-\ndefinitions"
  },
  {
    "number": "24",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have\nimplemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service\ninstance.\nSolution: You configure Basic gateway credentials for the Azure resource.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "API Management allows to secure access to the back-end service of an API using client\ncertificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-\napi-management-rest-api-backend-entity"
  },
  {
    "number": "25",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have\nimplemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service\ninstance.\nSolution: You configure Client cert gateway credentials for the HTTP(s) endpoint.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes \nPage 56 of 1410\n57 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The API back end is hosted in an Azure App Service instance. It is an Azure resource and\nnot an HTTP(s) endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-\napi-management-rest-api-backend-entity"
  },
  {
    "number": "26",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have\nimplemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service\ninstance.\nSolution: You configure Basic gateway credentials for the HTTP(s) endpoint.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "API Management allows to secure access to the back-end service of an API using client\ncertificates. Furthermore, the API back end is hosted in an Azure App\nService instance. It is an Azure resource and not an HTTP(s) endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-\napi-management-rest-api-backend-entity"
  },
  {
    "number": "27",
    "question": "Note: The question is included in a number of questions that depicts the\nidentical set-up. However, every question has a distinctive result. Establish if\nthe solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have\nimplemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service\ninstance.\nSolution: You configure Client cert gateway credentials for the Azure resource.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 59 of 1410\n60 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "API Management allows to secure access to the back-end service of an API using client\ncertificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-\napi-management-rest-api-backend-entity"
  },
  {
    "number": "28",
    "question": "You are developing a .NET Core MVC application that allows customers to\nresearch independent holiday accommodation providers.\nYou want to implement Azure Search to allow the application to search the\nindex by using various criteria to locate documents related to accommodation\nvenues.\nYou want the application to list holiday accommodation venues that fall within\na specific price range and are within a specified distance to an airport.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure the SearchMode property of the SearchParameters class.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure the QueryType property of the SearchParameters class.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure the Facets property of the SearchParameters class.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the Filter property of the SearchParameters class.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "The Filter property gets or sets the OData $filter expression to apply to the search query.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.search.models.searchparameters https://docs.microsoft.com/en-us/\ndotnet/api/microsoft.azure.search.models.searchparameters.querytype"
  },
  {
    "number": "29",
    "question": "You are a developer at your company.\nYou need to edit the workflows for an existing Logic App.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "the Enterprise Integration Pack (EIP)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "the Logic App Code View",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "the API Connections",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "the Logic Apps Designer",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "solutions and seamless communication between\norganizations, you can build automated scalable enterprise integration workflows by\nusing the Enterprise Integration Pack (EIP) with Azure Logic Apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-\noverview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-\ndefinitions",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": "For business-to-business (B2B) solutions and seamless communication between\norganizations, you can build automated scalable enterprise integration workflows by\nusing the Enterprise Integration Pack (EIP) with Azure Logic Apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-\noverview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-\ndefinitions"
  },
  {
    "number": "30",
    "question": "DRAG DROP -\nYou are a developer for a company that provides a bookings management\nservice in the tourism industry. You are implementing Azure Search for the tour\nagencies listed in your company's solution.\nYou create the index in Azure Search. You now need to use the Azure Search\n.NET SDK to import the relevant data into the Azure Search service.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions from left to right and arrange them\nin the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\n1. The index needs to be populated. To do this, we will need a SearchIndexClient. There\nare two ways to obtain one: by constructing it, or by calling\nIndexes.GetClient on the SearchServiceClient. Here we will use the first method.\n2. Create the indexBatch with the documents\nSomething like:\nvar hotels = new Hotel[];\n{\nnew Hotel()\n{\nHotelId = \"3\",\nBaseRate = 129.99,\nDescription = \"Close to town hall and the river\"\n}\n};\nג€ג\nvar batch = IndexBatch.Upload(hotels);\n3. The next step is to populate the newly-created index\nExample:\nvar batch = IndexBatch.Upload(hotels);\ntry\n{\nindexClient.Documents.Index(batch);\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/search/search-howto-dotnet-sdk\n\n\nCommunity Discussion\nAzure search is out of scope for AZ-204\nout-of-the-scope for the AZ-204 Exam; please remove from the Question Bank\nAzure Search doesn't seem to be in the latest study guide. Best to avoid this question for\nless cram.\nCan't be true : uploading the data by putting it hardcoded in the source? Would suggest\nthe solutions that use a datasource. After all you had to extract the data from the\n\"company's solution\"\nhttps://docs.microsoft.com/en-us/azure/search/search-howto-dotnet-sdk",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "31",
    "question": "You are developing an application that applies a set of governance policies for\ninternal and external services, as well as for applications.\nYou develop a stateful ASP.NET Core 2.1 web application named PolicyApp and\ndeploy it to an Azure App Service Web App. The PolicyApp reacts to events from\nAzure Event Grid and performs policy actions based on those events.\nYou have the following requirements:\n✑ Authentication events must be used to monitor users when they sign in and\nsign out.\n✑ All authentication events must be processed by PolicyApp.\n✑ Sign outs must be processed as fast as possible.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a new Azure Event Grid subscription for all authentication events. Use the subscription\nto process sign-out events.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a separate Azure Event Grid handler for sign-in and sign-out events.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create separate Azure Event Grid topics and subscriptions for sign-in and sign-out events.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add a subject prefix to sign-out events. Create an Azure Event Grid subscription. Configure\nthe subscription to use the subjectBeginsWith filter. \nPage 66 of 1410\n67 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema"
  },
  {
    "number": "32",
    "question": "HOTSPOT -\nYou are developing a C++ application that compiles to a native application\nnamed process.exe. The application accepts images as input and returns images\nin one of the following image formats: GIF, PNG, or JPEG.\nYou must deploy the application as an Azure Function.\nYou need to configure the function and host json files.\nHow should you complete the json files? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: \"type\": \"http\"\nBox 2: \"customHandler\": { \"description\":{\nA custom handler is defined by configuring the host.json file with details on how to run\nthe web server via the customHandler section.\nThe customHandler section points to a target as defined by the defaultExecutablePath.\nExample:\n\"customHandler\": {\n\"description\": {\n\"defaultExecutablePath\": \"handler.exe\"\n\n\nBox 3: \"enableForwardingHttpRequest\": false\nIncorrect:\nFor HTTP-triggered functions with no additional bindings or outputs, you may want your\nhandler to work directly with the HTTP request and response instead of the custom\nhandler request and response payloads. This behavior can be configured in host.json\nusing the enableForwardingHttpRequest setting.\nAt the root of the app, the host.json file is configured to run handler.exe and\nenableForwardingHttpRequest is set to true.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers\nCommunity Discussion\nI just hope the real questions are not this deep to know if enableForwardingHttpRequest\nis true or false. I use Azure functions every day but I never used a custom handler and I\nhope most people do not have the burning need to use this when there are so many\nother solutions to handle this.\nOn my exam 2022-11-02\nThanks for mentioning the date\nThanks for mentioning the date\nNOTHING infuriates me more than statements like this.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "33",
    "question": "HOTSPOT -\nYou are implementing a software as a service (SaaS) ASP.NET Core web service\nthat will run as an Azure Web App. The web service will use an on-premises\nSQL Server database for storage. The web service also includes a WebJob that\nprocesses data updates. Four customers will use the web service.\n✑ Each instance of the WebJob processes data for a single customer and must\nrun as a singleton instance.\n✑ Each deployment must be tested by using deployment slots prior to serving\nproduction data.\n✑ Azure costs must be minimized.\n✑ Azure resources must be located in an isolated network.\nYou need to configure the App Service plan for the Web App.\nHow should you configure the App Service plan? To answer, select the\nappropriate settings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nNumber of VM instances: 4 -\nYou are not charged extra for deployment slots.\nPricing tier: Isolated -\nThe App Service Environment (ASE) is a powerful feature offering of the Azure App Service\nthat gives network isolation and improved scale capabilities. It is essentially a\ndeployment of the Azure App Service into a subnet of a customer's Azure Virtual Network\n(VNet).\nReference:\nhttps://azure.microsoft.com/sv-se/blog/announcing-app-service-isolated-more-power-\nscale-and-ease-of-use/\nCommunity Discussion\nBox 1: 4 There are four customers that use this service, and each instance of the WebJob\nprocesses data for a single customer and must run as a singleton instance. So, the\nnumber of VM should be 4. WebJobs is a feature of Azure App Service that enables you to\nrun a program or script in the same instance as a web app. Like running background\n\n\ntasks. Box 2: Isolated Azure resources must be located in an isolated network . In the\nIsolated tier, the App Service Environment defines the number of isolated workers that\nrun your apps, and each worker is charged. In addition, there's a flat Stamp Fee for the\nrunning the App Service Environment itself. Isolated: This tier runs dedicated Azure VMs\non dedicated Azure Virtual Networks. It provides network isolation on top of compute\nisolation to your apps. It provides the maximum scale-out capabilities.\nGot this in the exam yesterday.\n4 & isolated\n4 & isolated\n4 & isolated",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "34",
    "question": "DRAG DROP -\nYou are a developer for a software as a service (SaaS) company that uses an\nAzure Function to process orders. The Azure Function currently runs on an Azure\nFunction app that is triggered by an Azure Storage queue.\nYou are preparing to migrate the Azure Function to Kubernetes using\nKubernetes-based Event Driven Autoscaling (KEDA).\nYou need to configure Kubernetes Custom Resource Definitions (CRD) for the\nAzure Function.\nWhich CRDs should you configure? To answer, drag the appropriate CRD types to\nthe correct locations. Each CRD type may be used once, more than once, or not\nat all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Deployment -\nTo deploy Azure Functions to Kubernetes use the func kubernetes deploy command has\nseveral attributes that directly control how our app scales, once it is deployed to\nKubernetes.\nBox 2: ScaledObject -\nWith --polling-interval, we can control the interval used by KEDA to check Azure Service\nBus Queue for messages.\nExample of ScaledObject with polling interval\napiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\nname: transformer-fn\nnamespace: tt\nlabels:\ndeploymentName: transformer-fn\nspec:\nscaleTargetRef:\ndeploymentName: transformer-fn\npollingInterval: 5\nminReplicaCount: 0\nmaxReplicaCount: 100\nBox 3: Secret -\nStore connection strings in Kubernetes Secrets.\nExample: to create the Secret in our demo Namespace:\n# create the k8s demo namespace\nkubectl create namespace tt\n# grab connection string from Azure Service Bus\nKEDA_SCALER_CONNECTION_STRING=$(az servicebus queue authorization-rule keys list \\\n-g $RG_NAME \\\n--namespace-name $SBN_NAME \\\n--queue-name inbound \\\n\n\n-n keda-scaler \\\n--query \"primaryConnectionString\" \\\n-o tsv)\n# create the kubernetes secret\nkubectl create secret generic tt-keda-auth \\\n--from-literal KedaScaler=$KEDA_SCALER_CONNECTION_STRING \\\n--namespace tt\nReference:\nhttps://www.thinktecture.com/en/kubernetes/serverless-workloads-with-keda/\nCommunity Discussion\nBox 1: Deployment To deploy Azure Functions to Kubernetes use the func kubernetes\ndeploy command has several attributes that directly control how our app scales, once it\nis deployed to Kubernetes. Box 2: ScaledObject With --polling-interval, we can control the\ninterval used by KEDA to check Azure Service Bus Queue for messages. Box 3: Secret\nStore connection strings in Kubernetes Secrets.\nkubernetes is marked as out of scope but still there are few questions coming in the\nexam from this section\n1. Azure Function code - Deployment -To deploy Azure Functions to Kubernetes use the\nfunc kubernetes deploy command 2. Polling interval - ScaledObject - This is the interval\nto check each trigger on. By default KEDA will check each trigger source on every\nScaleObject every 30 seconds. 3. Azure Storage connection string - Secret - Store\nconnection string in Kubernetes secret Source for ScaledObject: https://keda.sh/docs/\n1.4/concepts/scaling-deployments/\nhttps://www.thinktecture.com/en/kubernetes/serverless-workloads-with-keda/\nserverless-workloads-with-keda/ is the right link\nout-of-the-scope for the AZ-204 Exam; please remove from the Question Bank",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "35",
    "question": "HOTSPOT -\nYou are creating a CLI script that creates an Azure web app and related services\nin Azure App Service. The web app uses the following variables:\nYou need to automatically deploy code from GitHub to the newly created web\napp.\nHow should you complete the script? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: az appservice plan create\nThe azure group creates command successfully returns JSON result. Now we can use\nresource group to create a azure app service plan\nBox 2: az webapp create -\nCreate a new web app..\nBox 3: --plan $webappname -\n..with the serviceplan we created in step 1.\nBox 4: az webapp deployment -\nContinuous Delivery with GitHub. Example:\naz webapp deployment source config --name firstsamplewebsite1 --resource-group\nwebsites--repo-url $gitrepo --branch master --git-token $token\nBox 5: --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://medium.com/@satish1v/devops-your-way-to-azure-web-apps-with-azure-\ncli-206ed4b3e9b1\nCommunity Discussion\n\n\nGiven answer is correct, got this on my test yesterday\n#!/bin/bash # Replace the following URL with a public GitHub repo URL gitrepo=https://\ngithub.com/Azure-Samples/php-docs-hello-world webappname=mywebapp$RANDOM #\nCreate a resource group. az group create --location westeurope --name\nmyResourceGroup # Create an App Service plan in `FREE` tier. az appservice plan create --\nname $webappname --resource-group myResourceGroup --sku FREE # Create a web app.\naz webapp create --name $webappname --resource-group myResourceGroup --plan\n$webappname # Deploy code from a public GitHub repository. az webapp deployment\nsource config --name $webappname --resource-group myResourceGroup \\ --repo-url\n$gitrepo --branch master --manual-integration # Copy the result of the following\ncommand into a browser to see the web app. echo http://\n$webappname.azurewebsites.net\ncorrect, in 2023Mar24, score: 904/1000\ncorrect, in 2023Mar24, score: 904/1000\nGot this question on 30-Sep-2022 exam. Answer is correct. Passed with 870 score.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "36",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs.\nUsers upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a\nmobile-friendly version of the image. The process to produce a mobile-friendly\nversion of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Trigger the photo processing from Blob storage events.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "is correct. Because, the trick is in the \"less than one minute\" detail.\nYou can read about \"..10-minute delay in processing new blobs..\" in \"3-Minimizing latency\"\ndescription. Microsoft says: \".....Use Event Grid instead of the Blob storage trigger for the\nfollowing scenarios:\" 1-Blob-only storage accounts: Blob-only storage accounts are\nsupported for blob input and output bindings but not for blob triggers. 2-High-scale:\nHigh scale can be loosely defined as containers that have more than 100,000 blobs in\nthem or storage accounts that have more than 100 blob updates per second. 3-\nMinimizing latency: If your function app is on the Consumption plan, there can be up to a\n##10-minute delay in processing new blobs## if a function app has gone idle. To avoid\nthis latency, you can switch to an App Service plan with Always On enabled. You can also\nuse an Event Grid trigger with your Blob storage account. For an example, see the Event\nGrid tutorial. REFENCE: https://docs.microsoft.com/en-us/azure/azure-functions/\nfunctions-bindings-storage-blob-trigger?tabs=csharp#event-grid-trigger I wish you a\ngood day.\nI've experienced this in one project I was participating in. If the amount of blobs\nbecomes high enough (like 50k+) blob trigger becomes completely unreliable. It may not\ntrigger at all or delay can be greater than 1 hour. It had nothing to do with Consumption\nPlan. Switching to Event Grid resolved that issue. So blob trigger DOESN'T guarantee\nprocessing within 1 minute, so anwer (B) is correct\nI've experienced this in one project I was participating in. If the amount of blobs\nbecomes high enough (like 50k+) blob trigger becomes completely unreliable. It may not\ntrigger at all or delay can be greater than 1 hour. It had nothing to do with Consumption\nPlan. Switching to Event Grid resolved that issue. So blob trigger DOESN'T guarantee\nprocessing within 1 minute, so anwer (B) is correct\nExcept the question says what triggers the processing, not what does the processing.\nQuestion/answer is poorly written.\nExcept the question says what triggers the processing, not what does the processing.\nQuestion/answer is poorly written.\nPage 82 of 1410\n83 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "You need to catch the triggered event, so move the photo processing to an Azure\nFunction triggered from the blob upload.\nNote: Azure Storage events allow applications to react to events. Common Blob storage\nevent scenarios include image or video processing, search indexing, or any file-oriented\nworkflow.\nEvents are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure\nLogic Apps, or even to your own http listener.\nHowever, the processing must start in less than one minute.\nNote: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage\nsupport event integration. Storage (general purpose v1) does not support integration\nwith Event Grid.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview"
  },
  {
    "number": "37",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted\ndeployment slot named Development. You create additional deployment slots\nnamed Testing and Production. You enable auto swap on the Production\ndeployment slot.\nYou need to ensure that scripts run and resources are available before a swap\noperation occurs.\nSolution: Update the web.config file to include the applicationInitialization\nconfiguration element. Specify custom initialization actions to run the scripts.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "No",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Yes",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Specify custom warm-up.\nSome apps might require custom warm-up actions before the swap. The\napplicationInitialization configuration element in web.config lets you specify custom\ninitialization actions. The swap operation waits for this custom warm-up to finish before\nswapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-\nswaps"
  },
  {
    "number": "38",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted\ndeployment slot named Development. You create additional deployment slots\nnamed Testing and Production. You enable auto swap on the Production\ndeployment slot.\nYou need to ensure that scripts run and resources are available before a swap\noperation occurs.\nSolution: Enable auto swap for the Testing slot. Deploy the app to the Testing\nslot.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "No",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Yes",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Instead update the web.config file to include the applicationInitialization configuration\nelement. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The\napplicationInitialization configuration element in web.config lets you specify custom\ninitialization actions. The swap operation waits for this custom warm-up to finish before\nswapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-\nswaps"
  },
  {
    "number": "39",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted\ndeployment slot named Development. You create additional deployment slots\nnamed Testing and Production. You enable auto swap on the Production\ndeployment slot.\nYou need to ensure that scripts run and resources are available before a swap\noperation occurs.\nSolution: Disable auto swap. Update the app with a method named statuscheck\nto run the scripts. Re-enable auto swap and deploy the app to the Production\nslot.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "No \nPage 86 of 1410\n87 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Yes",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Instead update the web.config file to include the applicationInitialization configuration\nelement. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The\napplicationInitialization configuration element in web.config lets you specify custom\ninitialization actions. The swap operation waits for this custom warm-up to finish before\nswapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-\nswaps"
  },
  {
    "number": "40",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs.\nUsers upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a\nmobile-friendly version of the image. The process to produce a mobile-friendly\nversion of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Convert the Azure Storage account to a BlockBlobStorage storage\naccount.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Not necessary to convert the account, instead move photo processing to an Azure\nFunction triggered from the blob upload..\nAzure Storage events allow applications to react to events. Common Blob storage event\nscenarios include image or video processing, search indexing, or any file- oriented\nworkflow.\nNote: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage"
  },
  {
    "number": "41",
    "question": "HOTSPOT -\nYou are developing an Azure Web App. You configure TLS mutual authentication\nfor the web app.\nYou need to validate the client certificate in the web app. To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nAccessing the client certificate from App Service.\nIf you are using ASP.NET and configure your app to use client certificate authentication,\nthe certificate will be available through the HttpRequest.ClientCertificate property. For\nother application stacks, the client cert will be available in your app through a base64\nencoded value in the \"X-ARR-ClientCert\" request header. Your application can create a\ncertificate from this value and then use it for authentication and authorization purposes\nin your application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-\nmutual-auth\nCommunity Discussion\nBox 1: HTTP request header If you are using ASP.NET and configure your app to use client\ncertificate authentication, the certificate will be available through the\nHttpRequest.ClientCertificate property. Box 2: Base64 For other application stacks, the\nclient cert will be available in your app through a base64 encoded value in the \"X-ARR-\nClientCert\" request header. Your application can create a certificate from this value and\nthen use it for authentication and authorization purposes in your application. Reference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-\nmutual-auth\nWith client certificates enabled, App Service injects an X-ARR-ClientCert request header\nwith the client certificate.\n\n\nGot this question on 2-Dec-2022 exam. Answer is correct. Passed with 857 score.\nHad this question today: 2023-07-26\nReceived this in my exam today (22/02/2023). Selected HTTP request header, and Base64.\nScore 927.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "42",
    "question": "DRAG DROP -\nYou are developing a Docker/Go using Azure App Service Web App for\nContainers. You plan to run the container in an App Service on Linux. You\nidentify a\nDocker container image to use.\nNone of your current resource groups reside in a location that supports Linux.\nYou must minimize the number of resource groups required.\nYou need to create the application and perform an initial deployment.\nWhich three Azure CLI commands should you use to develop the solution? To\nanswer, move the appropriate commands from the list of commands to the\nanswer area and arrange them in the correct order.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nYou can host native Linux applications in the cloud by using Azure Web Apps. To create a\nWeb App for Containers, you must run Azure CLI commands that create a group, then a\nservice plan, and finally the web app itself.\nStep 1: az group create -\nIn the Cloud Shell, create a resource group with the az group create command.\nStep 2: az appservice plan create\nIn the Cloud Shell, create an App Service plan in the resource group with the az\nappservice plan create command.\nStep 3: az webapp create -\nIn the Cloud Shell, create a web app in the myAppServicePlan App Service plan with the\naz webapp create command. Don't forget to replace with a unique app name, and\n<docker-ID> with your Docker ID.\nReference:\nhttps://docs.microsoft.com/mt-mt/azure/app-service/containers/quickstart-docker-go?\nview=sql-server-ver15\nCommunity Discussion\nWithin the same resource group, you can't mix Windows and Linux apps in the same\nregion. https://docs.microsoft.com/en-us/azure/app-service/overview#app-service-on-\n\n\nlinux \"None of your current resource groups reside in a location that supports Linux\" So\nyou have to create new resource group. Answer is correct.\nMany are missing the point here, it says \"None of your current resource groups reside in\na location that supports Linux\". So you need to create a group in an area that does\nsupport Linux. Doesn't matter if you mix them or not, there is currently nothing that\nsupports Linux, so something new is required. For me the answer is correct.\nMany are missing the point here, it says \"None of your current resource groups reside in\na location that supports Linux\". So you need to create a group in an area that does\nsupport Linux. Doesn't matter if you mix them or not, there is currently nothing that\nsupports Linux, so something new is required. For me the answer is correct.\nMany are missing the point here, it says \"None of your current resource groups reside in\na location that supports Linux\". So you need to create a group in an area that does\nsupport Linux. Doesn't matter if you mix them or not, there is currently nothing that\nsupports Linux, so something new is required. For me the answer is correct.\nMany are missing the point here, it says \"None of your current resource groups reside in\na location that supports Linux\". So you need to create a group in an area that does\nsupport Linux. Doesn't matter if you mix them or not, there is currently nothing that\nsupports Linux, so something new is required. For me the answer is correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "43",
    "question": "DRAG DROP -\nFourth Coffee has an ASP.NET Core web app that runs in Docker. The app is\nmapped to the www.fourthcoffee.com domain.\nFourth Coffee is migrating this application to Azure.\nYou need to provision an App Service Web App to host this docker image and\nmap the custom domain to the App Service web app.\nA resource group named FourthCoffeePublicWebResourceGroup has been\ncreated in the WestUS region that contains an App Service Plan named\nAppServiceLinuxDockerPlan.\nWhich order should the CLI commands be used to develop the solution? To\nanswer, move all of the Azure CLI commands from the list of commands to the\nanswer area and arrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: #bin/bash -\nThe appName is used when the webapp-name is created in step 2.\nStep 2: az webapp create -\nCreate a web app. In the Cloud Shell, create a web app in the myAppServicePlan App\nService plan with the az webapp create command.\nStep 3: az webapp config container set\nIn Create a web app, you specified an image on Docker Hub in the az webapp create\ncommand. This is good enough for a public image. To use a private image, you need to\nconfigure your Docker account ID and password in your Azure web app.\nStep 4: az webapp config hostname add\nThe webapp-name is used when the webapp is created in step 2.\nIn the Cloud Shell, follow the az webapp create command with az webapp config\ncontainer set.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-\ndocker-image https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-\ncontainer?pivots=container-linux https://docs.microsoft.com/en-us/azure/app-service/\nscripts/cli-configure-custom-domain\nCommunity Discussion\n\n\n1. /bin/bash 2. az webpp create 3. ~ config container set 4. ~ config hostname add\nShouldn' it be az webapp create as 2nd, az webapp config container set as 3rd and az\nwebapp config hostname as last ?\nreceived 2023-04-17 went above answer, score 926\nreceived 2023-04-17 went above answer, score 926\nI would agree with the propose solution 1. /bin/bash 2. az webpp create 3. ~ config\ncontainer set URL: https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-\ncontainer?pivots=container-linux At the bottom of tutorial, link to next tutorial In the\nnext tutorial, you learn how to map a custom DNS name to your app. 4. ~ config\nhostname add https://docs.microsoft.com/en-us/azure/app-service/app-service-web-\ntutorial-custom-domain",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "44",
    "question": "DRAG DROP -\nYou are developing a serverless Java application on Azure. You create a new\nAzure Key Vault to work with secrets from a new Azure Functions application.\nThe application must meet the following requirements:\n✑ Reference the Azure Key Vault without requiring any changes to the Java\ncode.\n✑ Dynamically add and remove instances of the Azure Functions host based on\nthe number of incoming application events.\n✑ Ensure that instances are perpetually warm to avoid any cold starts.\n✑ Connect to a VNet.\n✑ Authentication to the Azure Key Vault instance must be removed if the Azure\nFunction application is deleted.\nYou need to grant the Azure Functions application access to the Azure Key Vault.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nStep 1: Create the Azure Functions app with a Consumption plan type.\nUse the Consumption plan for serverless.\nStep 2: Create a system-assigned managed identity for the application.\nCreate a system-assigned managed identity for your application.\nKey Vault references currently only support system-assigned managed identities. User-\nassigned identities cannot be used.\nStep 3: Create an access policy in Key Vault for the application identity.\nCreate an access policy in Key Vault for the application identity you created earlier.\nEnable the \"Get\" secret permission on this policy. Do not configure the\n\"authorized application\" or applicationId settings, as this is not compatible with a\nmanaged identity.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references\nCommunity Discussion\n1. create ~Premium plan Type (Consumption X) 2. create system-assigned ~ (user-assigned\nX) 3. create an access policy in Azure Key Vault~\nI agree with you. 1. Premium plan (avoid any cold starts and connect to a VNet) Overview\nof plans here: https://docs.microsoft.com/th-th/azure/azure-functions/functions-scale 2.\n\n\ncreate system-assigned => \"A system-assigned identity is tied to your application and is\ndeleted if your app is deleted.\" 3. create an access policy https://docs.microsoft.com/en-\nus/azure/app-service/app-service-key-vault-references?toc=%2Fazure%2Fazure-\nfunctions%2Ftoc.json&tabs=azure-cli\nI agree with you. 1. Premium plan (avoid any cold starts and connect to a VNet) Overview\nof plans here: https://docs.microsoft.com/th-th/azure/azure-functions/functions-scale 2.\ncreate system-assigned => \"A system-assigned identity is tied to your application and is\ndeleted if your app is deleted.\" 3. create an access policy https://docs.microsoft.com/en-\nus/azure/app-service/app-service-key-vault-references?toc=%2Fazure%2Fazure-\nfunctions%2Ftoc.json&tabs=azure-cli\nReceived this in my exam today (22/02/2023). Selected 'Created the Azure Function app\nwith Premium plan type', 'Create a system-assigned managed identity for the application',\nand 'Create an access policy in Azure Key Vault for the application identity'. Score 927.\nApp Service plan can connect to Vnet but it won't \"Ensure that instances are perpetually\nwarm to avoid any cold starts\".",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "45",
    "question": "You develop a website. You plan to host the website in Azure. You expect the\nwebsite to experience high traffic volumes after it is published.\nYou must ensure that the website remains available and responsive while\nminimizing cost.\nYou need to deploy the website.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Deploy the website to a virtual machine. Configure the virtual machine to automatically scale\nwhen the CPU load is high.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Deploy the website to an App Service that uses the Shared service tier. Configure the App\nService plan to automatically scale when the CPU load is high.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Deploy the website to a virtual machine. Configure a Scale Set to increase the virtual\nmachine instance count when the CPU load is high.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Deploy the website to an App Service that uses the Standard service tier. Configure the App\nService plan to automatically scale when the CPU load is high. \nPage 100 of 1410\n101 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Windows Azure Web Sites (WAWS) offers 3 modes: Standard, Free, and Shared.\nStandard mode carries an enterprise-grade SLA (Service Level Agreement) of 99.9%\nmonthly, even for sites with just one instance.\nStandard mode runs on dedicated instances, making it different from the other ways to\nbuy Windows Azure Web Sites.\nIncorrect Answers:\nB: Shared and Free modes do not offer the scaling flexibility of Standard, and they have\nsome important limits.\nShared mode, just as the name states, also uses shared Compute resources, and also has\na CPU limit. So, while neither Free nor Shared is likely to be the best choice for your\nproduction environment due to these limits."
  },
  {
    "number": "46",
    "question": "HOTSPOT -\nA company is developing a Java web app. The web app code is hosted in a\nGitHub repository located at https://github.com/Contoso/webapp.\nThe web app must be evaluated before it is moved to production. You must\ndeploy the initial code release to a deployment slot named staging.\nYou need to create the web app and deploy the code.\nHow should you complete the commands? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: group -\n# Create a resource group.\naz group create --location westeurope --name myResourceGroup\nBox 2: appservice plan -\n# Create an App Service plan in STANDARD tier (minimum required by deployment slots).\naz appservice plan create --name $webappname --resource-group myResourceGroup --\nsku S1\nBox 3: webapp -\n# Create a web app.\naz webapp create --name $webappname --resource-group myResourceGroup \\\n--plan $webappname\nBox 4: webapp deployment slot -\n#Create a deployment slot with the name \"staging\".\naz webapp deployment slot create --name $webappname --resource-group\nmyResourceGroup \\\n--slot staging\nBox 5: webapp deployment source -\n# Deploy sample code to \"staging\" slot from GitHub.\naz webapp deployment source config --name $webappname --resource-group\n\n\nmyResourceGroup \\\n--slot staging --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-\nenvironment\nCommunity Discussion\nBecause some answers are wrong and commenting when correct is just comforting for\nothers.\nBecause some answers are wrong and commenting when correct is just comforting for\nothers.\nGiven answer is correct.\nAgree! love this site because of the community and the discussions. they also help to\nunderstand the reasoning!!\nAgree! love this site because of the community and the discussions. they also help to\nunderstand the reasoning!!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "47",
    "question": "HOTSPOT -\nYou have a web service that is used to pay for food deliveries. The web service\nuses Azure Cosmos DB as the data store.\nYou plan to add a new feature that allows users to set a tip amount. The new\nfeature requires that a property named tip on the document in Cosmos DB must\nbe present and contain a numeric value.\nThere are many existing websites and mobile apps that use the web service that\nwill not be updated to set the tip property for some time.\nHow should you complete the trigger?\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nRight answer in second drop down is the first one (...\"tip\" in i...) Similiar example can be\nfound on https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-write-stored-\nprocedures-triggers-udfs\n\n\nAdmin, requst you to update answers to avoid any further confusion. 1. getRequest 2.\n(!\"tip\" in i) 3. setBody\nSadheesh88 has a valid point. Although the question states \"there will be many apps not\nupdating their value for some time\", a good solution should always validate user input. If\nthe value of \"tip\" is null or \"junk\", the first option will fail. Option 3 has a typo and is\nmissing a closing parenthesis. If we make a small change to option 3, so it reads:\nif(isNaN(i[\"tip\"]) || i[\"tip\"] === null){ then this becomes the best answer. Check for\nyourself with this fiddle: https://jsfiddle.net/d4x3fota/2/ So I think it comes down to if\nthe answers were copied correctly.\nSadheesh88 has a valid point. Although the question states \"there will be many apps not\nupdating their value for some time\", a good solution should always validate user input. If\nthe value of \"tip\" is null or \"junk\", the first option will fail. Option 3 has a typo and is\nmissing a closing parenthesis. If we make a small change to option 3, so it reads:\nif(isNaN(i[\"tip\"]) || i[\"tip\"] === null){ then this becomes the best answer. Check for\nyourself with this fiddle: https://jsfiddle.net/d4x3fota/2/ So I think it comes down to if\nthe answers were copied correctly.\nSadheesh88 has a valid point. Although the question states \"there will be many apps not\nupdating their value for some time\", a good solution should always validate user input. If\nthe value of \"tip\" is null or \"junk\", the first option will fail. Option 3 has a typo and is\nmissing a closing parenthesis. If we make a small change to option 3, so it reads:\nif(isNaN(i[\"tip\"]) || i[\"tip\"] === null){ then this becomes the best answer. Check for\nyourself with this fiddle: https://jsfiddle.net/d4x3fota/2/ So I think it comes down to if\nthe answers were copied correctly.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "48",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage\nblob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the\nblob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Use the Durable Function async pattern to process the blob data.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed\nby a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best\npractices include:\nWhenever possible, refactor large functions into smaller function sets that work together\nand return responses fast. For example, a webhook or HTTP trigger function might\nrequire an acknowledgment response within a certain time limit; it's common for\nwebhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This\napproach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices"
  },
  {
    "number": "49",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage\nblob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the\nblob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Pass the HTTP trigger payload into an Azure Service Bus queue to be\nprocessed by a queue trigger function and return an immediate HTTP success\nresponse.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Large, long-running functions can cause unexpected timeout issues. General best\npractices include:\nWhenever possible, refactor large functions into smaller function sets that work together\nand return responses fast. For example, a webhook or HTTP trigger function might\nrequire an acknowledgment response within a certain time limit; it's common for\nwebhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This\napproach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices"
  },
  {
    "number": "50",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage\nblob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the\nblob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Configure the app to use an App Service hosting plan and enable the\nAlways On setting.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No \nPage 111 of 1410\n112 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed\nby a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best\npractices include:\nWhenever possible, refactor large functions into smaller function sets that work together\nand return responses fast. For example, a webhook or HTTP trigger function might\nrequire an acknowledgment response within a certain time limit; it's common for\nwebhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This\napproach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices"
  },
  {
    "number": "51",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs.\nUsers upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a\nmobile-friendly version of the image. The process to produce a mobile-friendly\nversion of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Move photo processing to an Azure Function triggered from the blob\nupload.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure Storage events allow applications to react to events. Common Blob storage event"
  },
  {
    "number": "52",
    "question": "You are developing an application that uses Azure Blob storage.\nThe application must read the transaction logs of all the changes that occur to\nthe blobs and the blob metadata in the storage account for auditing purposes.\nThe changes must be in the order in which they occurred, include only create,\nupdate, delete, and copy operations and be retained for compliance reasons.\nYou need to process the transaction logs asynchronously.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Process all Azure Blob storage events by using Azure Event Grid with a subscriber Azure\nFunction app.\nPage 114 of 1410\n115 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable the change feed on the storage account and process all changes for available\nevents.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Process all Azure Storage Analytics logs for successful blob events.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use the Azure Monitor HTTP Data Collector API and scan the request body for successful blob\nevents.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Change feed support in Azure Blob Storage\nThe purpose of the change feed is to provide transaction logs of all the changes that\noccur to the blobs and the blob metadata in your storage account. The change feed\nprovides ordered, guaranteed, durable, immutable, read-only log of these changes. Client\napplications can read these logs at any time, either in streaming or in batch mode. The\nchange feed enables you to build efficient and scalable solutions that process change\nevents that occur in your Blob Storage account at a low cost.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed"
  },
  {
    "number": "53",
    "question": "DRAG DROP -\nYou plan to create a Docker image that runs an ASP.NET Core application named\nContosoApp. You have a setup script named setupScript.ps1 and a series of\napplication files including ContosoApp.dll.\nYou need to create a Dockerfile document that meets the following\nrequirements:\n✑ Call setupScripts.ps1 when the container is built.\n✑ Run ContosoApp.dll when the container starts.\nThe Dockerfile document must be created in the same folder where\nContosoApp.dll and setupScript.ps1 are stored.\nWhich five commands should you use to develop the solution? To answer, move\nthe appropriate commands from the list of commands to the answer area and\narrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: CMD [..]\n\n\nCmd starts a new instance of the command interpreter, Cmd.exe.\nSyntax: CMD <string>\nSpecifies the command you want to carry out.\nBox 2: FROM microsoft/aspnetcore-build:latest\nBox 3: WORKDIR /apps/ContosoApp -\nBxo 4: COPY ./ .\nBox 5: RUN powershell ./setupScript.ps1\nCommunity Discussion\nIt should be: - FROM - WORKDIR - COPY - RUN - CMD Same question on: https://\nwww.secexams.com/discussions/microsoft/view/13131-exam-az-300-topic-3-question-4-\ndiscussion/ And: https://www.secexams.com/discussions/microsoft/view/11045-exam-\naz-203-topic-1-question-7-discussion/\nYou are correct. This has been discussed in Udemy course as well as follows, The first\nstatement in the Dockefile must be the FROM statement to specify the image to use as\nthe base image. Then specify the Image working directory Then copy all of the\napplication contents using the COPY command And then use the CMD command to run\nthe PowerShell command and the ENTRYPOINT statement to run the dotnet application.\nYou are correct. This has been discussed in Udemy course as well as follows, The first\nstatement in the Dockefile must be the FROM statement to specify the image to use as\nthe base image. Then specify the Image working directory Then copy all of the\napplication contents using the COPY command And then use the CMD command to run\nthe PowerShell command and the ENTRYPOINT statement to run the dotnet application.\nJust wondering who put the answers for the questions in this site? most of them are not\ncorrect.\nThis is correct answer It should be: - FROM - WORKDIR - COPY - RUN - CMD because\nWORKDIR will create directory if it doesn't exist https://docs.docker.com/engine/\nreference/builder/#workdir",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "54",
    "question": "You are developing an Azure Function App that processes images that are\nuploaded to an Azure Blob container.\nImages must be processed as quickly as possible after they are uploaded, and\nthe solution must minimize latency. You create code to process images when\nthe\nFunction App is triggered.\nYou need to configure the Function App.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use an App Service plan. Configure the Function App to use an Azure Blob Storage input\ntrigger.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Use a Consumption plan. Configure the Function App to use an Azure Blob Storage trigger.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Use a Consumption plan. Configure the Function App to use a Timer trigger.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use an App Service plan. Configure the Function App to use an Azure Blob Storage trigger.\nE) Use a Consumption plan. Configure the Function App to use an Azure Blob Storage input\ntrigger.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The Blob storage trigger starts a function when a new or updated blob is detected. The\nblob contents are provided as input to the function.\nThe Consumption plan limits a function app on one virtual machine (VM) to 1.5 GB of\nmemory.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-\nblob-trigger"
  },
  {
    "number": "55",
    "question": "HOTSPOT -\nYou are configuring a new development environment for a Java application.\nThe environment requires a Virtual Machine Scale Set (VMSS), several storage\naccounts, and networking components.\nThe VMSS must not be created until the storage accounts have been\nsuccessfully created and an associated load balancer and virtual network is\nconfigured.\nHow should you complete the Azure Resource Manager template? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: copyIndex -\nNotice that the name of each resource includes the copyIndex() function, which returns\nthe current iteration in the loop. copyIndex() is zero-based.\nBox 2: copy -\nBy adding the copy element to the resources section of your template, you can\ndynamically set the number of resources to deploy.\nBox 3: dependsOn -\nExample:\n\n\n\"type\": \"Microsoft.Compute/virtualMachineScaleSets\",\n\"apiVersion\": \"2020-06-01\",\n\"name\": \"[variables('namingInfix')]\",\n\"location\": \"[parameters('location')]\",\n\"sku\": {\n\"name\": \"[parameters('vmSku')]\",\n\"tier\": \"Standard\",\n\"capacity\": \"[parameters('instanceCount')]\"\n},\n\"dependsOn\": [\n\"[resourceId('Microsoft.Network/loadBalancers', variables('loadBalancerName'))]\",\n\"[resourceId('Microsoft.Network/virtualNetworks', variables('virtualNetworkName'))]\"\n],\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/copy-\nresources https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/quick-\ncreate-template-windows\nCommunity Discussion\nAnswer: copyIndex, copy, Dependson. Check this link https://docs.microsoft.com/en-us/\nazure/azure-resource-manager/templates/define-resource-dependency.\nBox 1: copyIndex Notice that the name of each resource includes the copyIndex()\nfunction, which returns the current iteration in the loop. copyIndex() is zero-based. Box 2:\ncopy By adding copy loop to the resources section of your template, you can dynamically\nset the number of resources to deploy. You also avoid having to repeat template syntax.\nBox 3: dependsOn Within your Azure Resource Manager template (ARM template), the\ndependsOn element enables you to define one resource as a dependent on one or more\nresources. Reference: https://docs.microsoft.com/en-us/azure/azure-resource-manager/\ntemplates/copy-resources https://docs.microsoft.com/en-us/azure/virtual-machine-\nscale-sets/quick-create-template-windows https://docs.microsoft.com/en-us/azure/\nazure-resource-manager/templates/define-resource-dependency\nReceived this in my exam today (22/02/2023). Selected copyIndex, copy, and dependsOn.\nScore 927.\nDo I need to remember these functions? The options are made in a way to confuse you\nand make a mistake. These are things developers just lookup in documentation. Not\nwriting an ARM template every day\n\n\nGot this on 20 Apr 2022.. (copyIndex, copy, dependsOn)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "56",
    "question": "HOTSPOT -\nYou are developing an Azure Function App by using Visual Studio. The app will\nprocess orders input by an Azure Web App. The web app places the order\ninformation into Azure Queue Storage.\nYou need to review the Azure Function App code shown below.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nExpirationTime - The time that the message expires.\nInsertionTime - The time that the message was added to the queue.\nBox 2: Yes -\nmaxDequeueCount - The number of times to try processing a message before moving it\nto the poison queue. Default value is 5.\nBox 3: Yes -\nWhen there are multiple queue messages waiting, the queue trigger retrieves a batch of\nmessages and invokes function instances concurrently to process them.\nBy default, the batch size is 16. When the number being processed gets down to 8, the\nruntime gets another batch and starts processing those messages. So the maximum\nnumber of concurrent messages being processed per function on one virtual machine\n(VM) is 24.\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-\nqueue\nCommunity Discussion\nBox 1: No It logs the following: - ExpirationTime - The time that the message expires. -\nInsertionTime - The time that the message was added to the queue. Box 2: Yes\nmaxDequeueCount: The number of times to try processing a message before moving it to\nthe poison queue. Default value is 5. Box 3: Yes When there are multiple queue messages\nwaiting, the queue trigger retrieves a batch of messages and invokes function instances\nconcurrently to process them. By default, the batch size is 16. When the number being\nprocessed gets down to 8, the runtime gets another batch and starts processing those\nmessages. So the maximum number of concurrent messages being processed per\nfunction on one virtual machine (VM) is 24. Box 4: Yes [Table(\"Orders\")]ICollector<Order>\n\n\ntable bindings And in the code it adds the order:\ntableBindings.Add(JsonConvert.DeserializeObject<Object>(myQueueItem.AsString));\nMicrosoft thinks that a good developer should remember the default value for the\ndequeCount (and not forget that in a stress exam situation that there is that property,\nwhich is not shown in the code, very convenient).\nSeems correct https://docs.microsoft.com/fr-fr/azure/azure-functions/functions-\nbindings-storage-queue#hostjson-settings\nWould love to answer this correctly but the full code isn't showing. The\nmaxDequeueCount value isn't showing in the question codeset provided. Does anyone\nhave the full codeset?\nImage isn't showing a full code? Please send me full code",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "57",
    "question": "DRAG DROP -\nYou are developing a solution for a hospital to support the following use cases:\n✑ The most recent patient status details must be retrieved even if multiple\nusers in different locations have updated the patient record.\n✑ Patient health monitoring data retrieved must be the current version or the\nprior version.\n✑ After a patient is discharged and all charges have been assessed, the patient\nbilling record contains the final charges.\nYou provision a Cosmos DB NoSQL database and set the default consistency\nlevel for the database account to Strong. You set the value for Indexing Mode to\nConsistent.\nYou need to minimize latency and any impact to the availability of the solution.\nYou must override the default consistency level at the query level to meet the\nrequired consistency guarantees for the scenarios.\nWhich consistency levels should you implement? To answer, drag the\nappropriate consistency levels to the correct requirements. Each consistency\nlevel may be used once, more than once, or not at all. You may need to drag the\nsplit bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: Strong -\n\n\nStrong: Strong consistency offers a linearizability guarantee. The reads are guaranteed to\nreturn the most recent committed version of an item. A client never sees an\nuncommitted or partial write. Users are always guaranteed to read the latest committed\nwrite.\nBox 2: Bounded staleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee.\nThe reads might lag behind writes by at most \"K\" versions (that is\n\"updates\") of an item or by \"t\" time interval. When you choose bounded staleness, the\n\"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (t) by which the reads might lag behind the writes\nBox 3: Eventual -\nEventual: There's no ordering guarantee for reads. In the absence of any further writes,\nthe replicas eventually converge.\nIncorrect Answers:\nConsistent prefix: Updates that are returned contain some prefix of all the updates, with\nno gaps. Consistent prefix guarantees that reads never see out-of-order writes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\nCommunity Discussion\nAnswers are correct\nBox 1: Strong Box 2: Bounded staleness Box 3: Eventual Note: Consistent prefix: Updates\nthat are returned contain some prefix of all the updates, with no gaps. Consistent prefix\nguarantees that reads never see out-of-order writes. Reference: https://\ndocs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\nwhat is the Note for in your comment? you just copied it from the correct answer\ndescription which doesn't make sense to me.\nwhat is the Note for in your comment? you just copied it from the correct answer\ndescription which doesn't make sense to me.\nGot this on 6/28/2023 and passed with 850. Went with answer.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "58",
    "question": "HOTSPOT -\nYou are configuring a development environment for your team. You deploy the\nlatest Visual Studio image from the Azure Marketplace to your Azure\nsubscription.\nThe development environment requires several software development kits\n(SDKs) and third-party components to support application development across\nthe organization. You install and customize the deployed virtual machine (VM)\nfor your development team. The customized VM must be saved to allow\nprovisioning of a new team member development environment.\nYou need to save the customized VM for future provisioning.\nWhich tools or services should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Azure Powershell -\nCreating an image directly from the VM ensures that the image includes all of the disks\nassociated with the VM, including the OS disk and any data disks.\nBefore you begin, make sure that you have the latest version of the Azure PowerShell\nmodule.\nYou use Sysprep to generalize the virtual machine, then use Azure PowerShell to create\nthe image.\nBox 2: Azure Blob Storage -\nYou can store images in Azure Blob Storage.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-\nresource#create-an-image-of-a-vm-using-powershell\nCommunity Discussion\nThe answer is right, it is show in AZ-900 as well.\nBox 1: Azure Powershell Creating an image directly from the VM ensures that the image\nincludes all of the disks associated with the VM, including the OS disk and any data\ndisks. Before you begin, make sure that you have the latest version of the Azure\nPowerShell module. You use Sysprep to generalize the virtual machine, then use Azure\nPowerShell to create the image. Box 2: Azure Blob Storage A VM Image is a collection of\nmetadata and pointers to a set of VHDs (one VHD per disk) stored as page blobs in Azure\nStorage. Reference: https://azure.microsoft.com/en-us/blog/vm-image-blog-post https://\ndocs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource\nPowershell is correct. https://docs.microsoft.com/en-us/azure/virtual-machines/\nwindows/capture-image-resource\n\n\nPowershell is correct. https://docs.microsoft.com/en-us/azure/virtual-machines/\nwindows/capture-image-resource\nIs it still relevant for az-204 since they removed VMs from scope in 28th of april?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "59",
    "question": "You are preparing to deploy a website to an Azure Web App from a GitHub\nrepository. The website includes static content generated by a script.\nYou plan to use the Azure Web App continuous deployment feature.\nYou need to run the static generation script before the website starts serving\ntraffic.\nWhat are two possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Add the path to the static content generation tool to WEBSITE_RUN_FROM_PACKAGE setting\nin the host.json file.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Add a PreBuild target in the websites csproj project file that runs the static content\ngeneration script.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a file named run.cmd in the folder /run that calls a script which generates the static\ncontent and deploys the website.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a file named .deployment in the root of the repository that calls a script which\ngenerates the static content and deploys the website.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D\nA: In Azure, you can run your functions directly from a deployment package file in your\nfunction app. The other option is to deploy your files in the d:\\home\\site\n\\wwwroot directory of your function app (see A above).\nTo enable your function app to run from a package, you just add a\nWEBSITE_RUN_FROM_PACKAGE setting to your function app settings.\nNote: The host.json metadata file contains global configuration options that affect all\nfunctions for a function app.\nD: To customize your deployment, include a .deployment file in the repository root.\nYou just need to add a file to the root of your repository with the name .deployment and\nthe content:"
  },
  {
    "number": "60",
    "question": "DRAG DROP -\nYou are developing an application to use Azure Blob storage. You have\nconfigured Azure Blob storage to include change feeds.\nA copy of your storage account must be created in another region. Data must be\ncopied from the current storage account to the new storage account directly\nbetween the storage servers.\nYou need to create a copy of the storage account in another region and copy the\ndata.\nIn which order should you perform the actions? To answer, move all actions\nfrom the list of actions to the answer area and arrange them in the correct\norder.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nTo move a storage account, create a copy of your storage account in another region.\nThen, move your data to that account by using AzCopy, or another tool of your choice.\nThe steps are:\n✑ Export a template.\n✑ Modify the template by adding the target region and storage account name.\n✑ Deploy the template to create the new storage account.\n✑ Configure the new storage account.\n✑ Move data to the new storage account.\n✑ Delete the resources in the source region.\nNote: You must enable the change feed on your storage account to begin capturing and\nrecording changes. You can enable and disable changes by using Azure\nResource Manager templates on Portal or Powershell.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-move\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed\nCommunity Discussion\nI think you first have to export the Resource Manager template before you can create a\nnew template deployment. So, swap options 1 and 2 from the solution. In the first link of\nthe solution's text, exporting is also considere as the first step.\nAnswer is Wrong. correct Sequence is Export Create Modify Deploy AZ copy\nNo need to be this aggressive tho\nNo need to be this aggressive tho\n\n\nNo need to be this aggressive tho",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "61",
    "question": "DRAG DROP -\nYou are preparing to deploy an Azure virtual machine (VM)-based application.\nThe VMs that run the application have the following requirements:\n✑ When a VM is provisioned the firewall must be automatically configured\nbefore it can access Azure resources.\n✑ Supporting services must be installed by using an Azure PowerShell script\nthat is stored in Azure Storage.\nYou need to ensure that the requirements are met.\nWhich features should you use? To answer, drag the appropriate features to the\ncorrect requirements. Each feature may be used once, more than once, or not at\nall. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-\nworker https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-\ncommand\nCommunity Discussion\n1. Run Command 2. Customer Script Extension Some question: https://\nwww.secexams.com/discussions/microsoft/view/12062-exam-az-203-topic-1-question-11-\ndiscussion/\nBox 1: Run Command This capability is useful in all scenarios where you want to run a\nscript within a VM. It's one of the only ways to troubleshoot and remediate a VM that\ndoesn't have the RDP or SSH port open, because of improper network or administrative\nuser configuration. Box 2: Customer Script Extension The Custom Script Extension\ndownloads and executes scripts on Azure virtual machines. This extension is useful for\npost deployment configuration, software installation, or any other configuration or\nmanagement tasks. Scripts can be downloaded from Azure storage or GitHub, or provided\nto the Azure portal at extension run time. The Custom Script Extension integrates with\nAzure Resource Manager templates, and can be run using the Azure CLI, PowerShell,\nAzure portal, or the Azure Virtual Machine REST API.\nReference: https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/\ncustom-script-windows https://docs.microsoft.com/en-us/azure/virtual-machines/\nwindows/run-scripts-in-vm\nReference: https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/\ncustom-script-windows https://docs.microsoft.com/en-us/azure/virtual-machines/\nwindows/run-scripts-in-vm\n\n\nbut it says the firewall cofiguration has to happen automatically. How does \"Run\ncommand\" do that?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "62",
    "question": "HOTSPOT -\nA company is developing a Node.js web app. The web app code is hosted in a\nGitHub repository located at https://github.com/TailSpinToys/webapp.\nThe web app must be reviewed before it is moved to production. You must\ndeploy the initial code release to a deployment slot named review.\nYou need to create the web app and deploy the code.\nHow should you complete the commands? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: New-AzResourceGroup -\nThe New-AzResourceGroup cmdlet creates an Azure resource group.\nBox 2: New-AzAppServicePlan -\nThe New-AzAppServicePlan cmdlet creates an Azure App Service plan in a given location\nBox 3: New-AzWebApp -\nThe New-AzWebApp cmdlet creates an Azure Web App in a given a resource group\nBox 4: New-AzWebAppSlot -\nThe New-AzWebAppSlot cmdlet creates an Azure Web App slot.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.resources/new-\nazresourcegroup?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/\naz.websites/new-azappserviceplan?view=azps-2.3.2 https://docs.microsoft.com/en-us/\npowershell/module/az.websites/new-azwebapp?view=azps-2.3.2 https://\ndocs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?\nview=azps-2.3.2\nCommunity Discussion\nAnswer is correct\nBox 1: New-AzResourceGroup The New-AzResourceGroup cmdlet creates an Azure\nresource group. Box 2: New-AzAppServicePlan The New-AzAppServicePlan cmdlet creates\n\n\nan Azure App Service plan in a given location Box 3: New-AzWebApp The New-AzWebApp\ncmdlet creates an Azure Web App in a given a resource group Box 4: New-AzWebAppSlot\nThe New-AzWebAppSlot cmdlet creates an Azure Web App slot.\nReference: https://docs.microsoft.com/en-us/powershell/module/az.resources/new-\nazresourcegroup?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/\naz.websites/new-azappserviceplan?view=azps-2.3.2 https://docs.microsoft.com/en-us/\npowershell/module/az.websites/new-azwebapp?view=azps-2.3.2 https://\ndocs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?\nview=azps-2.3.2\nReference: https://docs.microsoft.com/en-us/powershell/module/az.resources/new-\nazresourcegroup?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/\naz.websites/new-azappserviceplan?view=azps-2.3.2 https://docs.microsoft.com/en-us/\npowershell/module/az.websites/new-azwebapp?view=azps-2.3.2 https://\ndocs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?\nview=azps-2.3.2\nQuestion was in Exam 2023-03-30",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "63",
    "question": "HOTSPOT -\nYou are developing an application that needs access to an Azure virtual\nmachine (VM).\nThe access lifecycle for the application must be associated with the VM service\ninstance.\nYou need to enable managed identity for the VM.\nHow should you complete the PowerShell segment? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: -IdentityType -\nEnable system-assigned managed identity on an existing Azure VM:\nTo enable a system-assigned managed identity, use the -IdentityType switch on the\nUpdate-AzVM cmdlet (see below).\nBox 2: $SystemAssigned -\n$vm = Get-AzVM -ResourceGroupName myResourceGroup -Name myVM\nUpdate-AzVM -ResourceGroupName myResourceGroup -VM $vm -IdentityType\nSystemAssigned\nReference:\n\n\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-\nresources/qs-configure-powershell-windows-vm\nCommunity Discussion\nThe parameter should be \"IdentityType\", not \"IdentityId\" as it's stated in the reference\nlink. \"SystemAssigned\" is correct\nBox 1: -IdentityType -IdentityType: The type of identity used for the virtual machine. Valid\nvalues are SystemAssigned, UserAssigned, SystemAssignedUserAssigned, and None. -\nIdentityId: Specifies the list of user identities associated with the virtual machine. The\nuser identity references will be ARM resource IDs in the form: Box 2: $SystemAssigned\nThere are two types of managed identities: - System-assigned: Some Azure services allow\nyou to enable a managed identity directly on a service instance. When you enable a\nsystem-assigned managed identity an identity is created in Azure AD that is tied to the\nlifecycle of that service instance. So when the resource is deleted, Azure automatically\ndeletes the identity for you. By design, only that Azure resource can use this identity to\nrequest tokens from Azure AD. - User-assigned: You may also create a managed identity\nas a standalone Azure resource. You can create a user-assigned managed identity and\nassign it to one or more instances of an Azure service. In the case of user-assigned\nmanaged identities, the identity is managed separately from the resources that use it.\n1- You can think there are two distinct errors in the question. 2- ...or then you can try to\nanswer thinking both IdentityId and the \"$\" in the second ddl are ok. The $ means that's\na variable. So it must contains a specific User Managed Identity path for the paramenter\nIdentityId. Can't be System Assigned. I'll go for the second one...\n1- You can think there are two distinct errors in the question. 2- ...or then you can try to\nanswer thinking both IdentityId and the \"$\" in the second ddl are ok. The $ means that's\na variable. So it must contains a specific User Managed Identity path for the paramenter\nIdentityId. Can't be System Assigned. I'll go for the second one...\ncorrect, in 2023Mar24, score: 904/1000",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "64",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs.\nUsers upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a\nmobile-friendly version of the image. The process to produce a mobile-friendly\nversion of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Create an Azure Function app that uses the Consumption hosting\nmodel and that is triggered from the blob upload.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "In the Consumption hosting plan, resources are added dynamically as required by your\nfunctions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-\ntriggered-function"
  },
  {
    "number": "65",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted\ndeployment slot named Development. You create additional deployment slots\nnamed Testing and Production. You enable auto swap on the Production\ndeployment slot.\nYou need to ensure that scripts run and resources are available before a swap\noperation occurs.\nSolution: Update the app with a method named statuscheck to run the scripts.\nUpdate the app settings for the app. Set the\nWEBSITE_SWAP_WARMUP_PING_PATH and\nWEBSITE_SWAP_WARMUP_PING_STATUSES with a path to the new method and\nappropriate response codes.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "No",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Yes",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "These are valid warm-up behavior options, but are not helpful in fixing swap problems.\nInstead update the web.config file to include the applicationInitialization configuration\nelement. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The\napplicationInitialization configuration element in web.config lets you specify custom\ninitialization actions. The swap operation waits for this custom warm-up to finish before\nswapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />"
  },
  {
    "number": "66",
    "question": "HOTSPOT -\nYou create the following PowerShell script:\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nThe AzScheduledQueryRuleSource is Heartbeat, not CPU.\nBox 2: Yes -\nThe AzScheduledQueryRuleSource is Heartbeat!\nNote: New-AzScheduledQueryRuleTriggerCondition creates an object of type Trigger\nCondition. This object is to be passed to the command that creates Alerting\nAction object.\nBox 3: No -\nThe schedule is 60 minutes, not two hours.\n-FrequencyInMinutes: The alert frequency.\n-TimeWindowInMinutes: The alert time window\nThe New-AzAscheduledQueryRuleSchedule command creates an object of type Schedule.\nThis object is to be passed to the command that creates Log Alert\nRule.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.monitor/new-\nazscheduledqueryrule https://docs.microsoft.com/en-us/powershell/module/\naz.monitor/new-azscheduledqueryruletriggercondition\nCommunity Discussion\nI agree. No Yes No\nI agree. NO YES NO\ncorrect, in 2023Mar24, score: 904/1000\n\n\ncorrect, in 2023Mar24, score: 904/1000\nNo, the answer should be No, Yes, No as per this link https://learn.microsoft.com/en-us/\njavascript/api/@azure/arm-monitor/schedule?view=azure-node-latest",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "67",
    "question": "DRAG DROP -\nYou are developing an Azure Function app.\nThe app must meet the following requirements:\n✑ Enable developers to write the functions by using the Rust language.\n✑ Declaratively connect to an Azure Blob Storage account.\nYou need to implement the app.\nWhich Azure Function app features should you use? To answer, drag the\nappropriate features to the correct requirements. Each feature may be used\nonce, more than once, or not at all. You may need to drag the split bar between\npanes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Custom handler -\nCustom handlers can be used to create functions in any language or runtime by running\nan HTTP server process, for example Go or Rust.\nBox 2: Trigger -\nFunctions are invoked by a trigger and can have exactly one. In addition to invoking the\nfunction, certain triggers also serve as bindings. You may also define multiple bindings in\naddition to the trigger. Bindings provide a declarative way to connect data to your code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-\nother https://docs.microsoft.com/en-us/dotnet/architecture/serverless/azure-functions\nCommunity Discussion\nBox 1: Custom handler Custom handlers can be used to create functions in any language\nor runtime by running an HTTP server process, for example Go or Rust. Box 2: extension\nbundles is needed to support the bindings and triggers that you use https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers?\nWT.mc_id=thomasmaurer-blog-thmaure#bindings-support\nAnswer is correct. Box 1: Custom handler Custom handlers can be used to create\nfunctions in any language or runtime by running an HTTP server process, for example Go\nor Rust. Box 2: Trigger Functions are invoked by a trigger and can have exactly one. In\naddition to invoking the function, certain triggers also serve as bindings. You may also\ndefine multiple bindings in addition to the trigger. Bindings provide a declarative way to\nconnect data to your code. Reference: https://docs.microsoft.com/en-us/azure/azure-\nfunctions/create-first-function-vs-code-other https://docs.microsoft.com/en-us/dotnet/\narchitecture/serverless/azure-functions\nWhat's your answer? You can just point out incorrect without any updates.\n\n\nWhat's your answer? You can just point out incorrect without any updates.\nWhat's your answer? You can just point out incorrect without any updates.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "68",
    "question": "HOTSPOT -\nYou are developing an ASP.NET Core web application. You plan to deploy the\napplication to Azure Web App for Containers.\nThe application needs to store runtime diagnostic data that must be persisted\nacross application restarts. You have the following code:\nYou need to configure the application settings so that diagnostic data is stored\nas required.\nHow should you configure the web app's settings? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: If WEBSITES_ENABLE_APP_SERVICE_STORAGE\nIf WEBSITES_ENABLE_APP_SERVICE_STORAGE setting is unspecified or set to true, the /\nhome/ directory will be shared across scale instances, and files written will persist\nacross restarts\nBox 2: /home -\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-faq\nCommunity Discussion\nI think the printed image is creating confusion. The correct answers (according to the\nimage) are: WEBSITES_ENABLE_APP_SERVICE_STORAGE=true DIAGDATA=/home\nGiven answer correct.\nFor Windows,it would be C:\\Home, which is not a provided option.. but /home is\nprovided. https://docs.microsoft.com/en-us/azure/app-service/configure-custom-\ncontainer?pivots=container-windows\nFor Windows,it would be C:\\Home, which is not a provided option.. but /home is\nprovided. https://docs.microsoft.com/en-us/azure/app-service/configure-custom-\ncontainer?pivots=container-windows\nFor Windows,it would be C:\\Home, which is not a provided option.. but /home is\nprovided. https://docs.microsoft.com/en-us/azure/app-service/configure-custom-\ncontainer?pivots=container-windows",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "69",
    "question": "You are developing a web app that is protected by Azure Web Application\nFirewall (WAF). All traffic to the web app is routed through an Azure Application\nGateway instance that is used by multiple web apps. The web app address is\ncontoso.azurewebsites.net.\nAll traffic must be secured with SSL. The Azure Application Gateway instance is\nused by multiple web apps.\nYou need to configure the Azure Application Gateway for the web app.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "In the Azure Application Gateway's HTTP setting, enable the Use for App service setting.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Convert the web app to run in an Azure App service environment (ASE).",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add an authentication certificate for contoso.azurewebsites.net to the Azure Application\nGateway.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "In the Azure Application Gateway's HTTP setting, set the value of the Override backend\npath option to contoso22.azurewebsites.net.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D\nD: The ability to specify a host override is defined in the HTTP settings and can be\napplied to any back-end pool during rule creation.\nThe ability to derive the host name from the IP or FQDN of the back-end pool members.\nHTTP settings also provide an option to dynamically pick the host name from a back-end\npool member's FQDN if configured with the option to derive host name from an\nindividual back-end pool member.\nA (not C): SSL termination and end to end SSL with multi-tenant services.\nIn case of end to end SSL, trusted Azure services such as Azure App service web apps do\nnot require whitelisting the backends in the application gateway.\nTherefore, there is no need to add any authentication certificates."
  },
  {
    "number": "70",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs.\nUsers upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a\nmobile-friendly version of the image. The process to produce a mobile-friendly\nversion of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Use the Azure Blob Storage change feed to trigger photo processing.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes\nPage 154 of 1410\n155 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "The change feed is a log of changes that are organized into hourly segments but\nappended to and updated every few minutes. These segments are created only when\nthere are blob change events that occur in that hour.\nInstead catch the triggered event, so move the photo processing to an Azure Function\ntriggered from the blob upload.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview"
  },
  {
    "number": "71",
    "question": "You are developing a web application that runs as an Azure Web App. The web\napplication stores data in Azure SQL Database and stores files in an Azure\nStorage account. The web application makes HTTP requests to external services\nas part of normal operations.\nThe web application is instrumented with Application Insights. The external\nservices are OpenTelemetry compliant.\nYou need to ensure that the customer ID of the signed in user is associated with\nall operations throughout the overall system.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Add the customer ID for the signed in user to the CorrelationContext in the web application",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "On the current SpanContext, set the TraceId to the customer ID for the signed in user",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Set the header Ocp-Apim-Trace to the customer ID for the signed in user",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a new SpanContext with the TraceFlags value set to the customer ID for the signed in\nuser",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/correlation"
  },
  {
    "number": "72",
    "question": "HOTSPOT -\nYou are developing an Azure Function App. You develop code by using a\nlanguage that is not supported by the Azure Function App host. The code\nlanguage supports HTTP primitives.\nYou must deploy the code to a production Azure Function App environment.\nYou need to configure the app for deployment.\nWhich configuration values should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Docker container -\nA custom handler can be deployed to every Azure Functions hosting option. If your\nhandler requires operating system or platform dependencies (such as a language\nruntime), you may need to use a custom container. You can create and deploy your code\nto Azure Functions as a custom Docker container.\nBox 2: PowerShell core -\nWhen creating a function app in Azure for custom handlers, we recommend you select\n.NET Core as the stack. A \"Custom\" stack for custom handlers will be added in the future.\nPowerShell Core (PSC) is based on the new .NET Core runtime.\nBox 3: 7.0 -\nOn Windows: The Azure Az PowerShell module is also supported for use with PowerShell\n\n\n5.1 on Windows.\nOn Linux: PowerShell 7.0.6 LTS, PowerShell 7.1.3, or higher is the recommended version of\nPowerShell for use with the Azure Az PowerShell module on all platforms.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-\nlinux-custom-image https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?\nview=azps-7.1.0\nCommunity Discussion\nanswer is wrong: if you pick docker container you cant specify stack and version, So ill go\nwith >Code >Custom Handler >custom (only option when you pick Custom Handler)\nI checked it on Azure portal and the answer should be Code > Custom Handler > custom\ncorrect, in 2023Mar24, score: 904/1000\ncorrect, in 2023Mar24, score: 904/1000\nGot this today. Went with answer here. Score 927",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "73",
    "question": "DRAG DROP -\nYou provision virtual machines (VMs) as development environments.\nOne VM does not start. The VM is stuck in a Windows update process. You attach\nthe OS disk for the affected VM to a recovery VM.\nYou need to correct the issue.\nIn which order should you perform the actions? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nRemove the update that causes the problem\n1. Take a snapshot of the OS disk of the affected VM as a backup.\n\n\n2. Attach the OS disk to a recovery VM.\n3. Once the OS disk is attached on the recovery VM, run diskmgmt.msc to open Disk\nManagement, and ensure the attached disk is ONLINE.\n4. (Step 1) Open an elevated command prompt instance (Run as administrator). Run the\nfollowing command to get the list of the update packages that are on the attached OS\ndisk: dism /image:<Attached OS disk>:\\ /get-packages > c:\\temp\\Patch_level\n5. (Step 2) Open the C:\\temp\\Patch_level.txt file, and then read it from the bottom up.\nLocate the update that's in Install Pending or Uninstall Pending state.\n6. Remove the update that caused the problem:\ndism /Image:<Attached OS disk>:\\ /Remove-Package /PackageName:<PACK\n7. (Step 4) Detach the OS disk and recreate the VM. Then check whether the issue is\nresolved.\nReference:\nhttps://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-\nstuck-updating-boot-error\nCommunity Discussion\nsure i will use this knowledge for my developer work...\nCorrect. All steps are listed here - https://docs.microsoft.com/en-us/troubleshoot/azure/\nvirtual-machines/troubleshoot-stuck-updating-boot-error\nQuestion says \"The VM is stuck in a Windows update process\". I ques it's common ;)\nQuestion says \"The VM is stuck in a Windows update process\". I ques it's common ;)\nIs this a part of AZ-204? Then I must have missed something....",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "74",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage\nblob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the\nblob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Update the functionTimeout property of the host.json project file to 10\nminutes.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed\nby a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best\npractices include:\nWhenever possible, refactor large functions into smaller function sets that work together\nand return responses fast. For example, a webhook or HTTP trigger function might\nrequire an acknowledgment response within a certain time limit; it's common for\nwebhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This\napproach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices"
  },
  {
    "number": "75",
    "question": "HOTSPOT -\nYou are developing an Azure Durable Function based application that processes\na list of input values. The application is monitored using a console application\nthat retrieves JSON data from an Azure Function diagnostic endpoint.\nDuring processing a single instance of invalid input does not cause the function\nto fail. Invalid input must be available to the monitoring application.\nYou need to implement the Azure Durable Function and the monitoring console\napplication.\nHow should you complete the code segments? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: await context.CallEntityAsync(input[errindex],\"error\")\nOrchestration signals and calls an entity\nOrchestrator functions can access entities by using APIs on the orchestration trigger\nbinding.\nExample:\n[FunctionName(\"CounterOrchestration\")]\npublic static async Task Run(\n[OrchestrationTrigger] IDurableOrchestrationContext context)\n{\nvar entityId = new EntityId(nameof(Counter), \"myCounter\");\n// Two-way call to the entity which returns a value - awaits the response int currentValue\n= await context.CallEntityAsync<int>(entityId, \"Get\");\n\n\nBox 2: Failed -\nDuring processing a single instance of invalid input does not cause the function to fail.\nNote: RuntimeStatus: One of the following values:\nFailed: The instance failed with an error.\nCompleted: The instance has completed normally.\nTerminated: The instance was stopped abruptly.\nPending: The instance has been scheduled but has not yet started running.\nRunning: The instance has started running.\nContinuedAsNew: The instance has restarted itself with a new history. This state is a\ntransient state.\nBox 3: Input -\nInvalid input must be available to the monitoring application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-\nentities https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-\nfunctions-instance-management\nCommunity Discussion\nI've not seen any of this stuff in Microsoft's Learning Path's for AZ-204\nI'd say: SetOutput() == Completed result.output The function needs to return the one\ninvalid input, so you could use either SetOutput or SetCustomStatus, but custom status\nis not available after the function completes. The one invalid input does not cause the\nfunction to fail, therefore Completed.\nThe learning path and the preparation videos are showing things you need not know for\nthe exam.\nThe learning path and the preparation videos are showing things you need not know for\nthe exam.\nThis question was in today's exam at 10-June-2023",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "76",
    "question": "You are developing an Azure Durable Function to manage an online ordering\nprocess.\nThe process must call an external API to gather product discount information.\nYou need to implement the Azure Durable Function.\nWhich Azure Durable Function types should you use? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Orchestrator",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Entity",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Client",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Activity",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B\nThe Durable Functions extension exposes a set of built-in HTTP APIs that can be used to\nperform management tasks on orchestrations, entities, and task hubs.\nThese HTTP APIs are extensibility webhooks that are authorized by the Azure Functions\nhost but handled directly by the Durable Functions extension.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-\nhttp-api"
  },
  {
    "number": "77",
    "question": "DRAG DROP -\nYou are authoring a set of nested Azure Resource Manager templates to deploy\nmultiple Azure resources.\nThe templates must be tested before deployment and must follow\nrecommended practices.\nYou need to validate and test the templates before deployment.\nWhich tools should you use? To answer, drag the appropriate tools to the\ncorrect requirements. Each tool may be used once, more than once, or not at\nall. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: Azure Resource Manager test toolkit\nUse ARM template test toolkit -\nThe Azure Resource Manager template (ARM template) test toolkit checks whether your\ntemplate uses recommended practices. When your template isn't compliant with\nrecommended practices, it returns a list of warnings with the suggested changes. By\nusing the test toolkit, you can learn how to avoid common problems in template\ndevelopment.\n\n\nBox 2: What-if operation -\nARM template deployment what-if operation\nBefore deploying an Azure Resource Manager template (ARM template), you can preview\nthe changes that will happen. Azure Resource Manager provides the what-if operation to\nlet you see how resources will change if you deploy the template. The what-if operation\ndoesn't make any changes to existing resources.\nInstead, it predicts the changes if the specified template is deployed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/test-toolkit\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-\nwhat-if\nCommunity Discussion\nLooks Correct https://docs.microsoft.com/en-us/azure/azure-resource-manager/\ntemplates/test-toolkit https://docs.microsoft.com/en-us/azure/azure-resource-\nmanager/templates/deploy-what-if?tabs=azure-powershell\nreceived 2023-04-17 went given answer, score 926\nreceived 2023-04-17 went given answer, score 926\non exam oct 2022\ngot it on my exam 30-12-2022 score: 818",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "78",
    "question": "HOTSPOT -\nYou are developing a solution that uses the Azure Storage Client library for .NET.\nYou have the following code: (Line numbers are included for reference only.)\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Yes -\nAcquireLeaseAsync does not specify leaseTime.\nleaseTime is a TimeSpan representing the span of time for which to acquire the lease,\nwhich will be rounded down to seconds. If null, an infinite lease will be acquired. If not\nnull, this must be 15 to 60 seconds.\nBox 2: No -\nThe GetBlockBlobReference method just gets a reference to a block blob in this\ncontainer.\nBox 3: Yes -\nThe BreakLeaseAsync method initiates an asynchronous operation that breaks the\ncurrent lease on this container.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.blob.cloudblobcontainer.acquireleaseasync https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.blob.cloudblobcontainer.getblockblobreference https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.blob.cloudblobcontainer.breakleaseasync\nCommunity Discussion\n\n\nI think the answer is correct: Optional. Version 2012-02-12 and newer. For a break\noperation, this is the proposed duration of seconds that the lease should continue\nbefore it is broken, between 0 and 60 seconds. This break period is only used if it is\nshorter than the time remaining on the lease. If longer, the time remaining on the lease\nis used. A new lease will not be available before the break period has expired, but the\nlease may be held for longer than the break period. If this header does not appear with a\nbreak operation, a fixed-duration lease breaks after the remaining lease period elapses,\nand an infinite lease breaks immediately. From: https://docs.microsoft.com/en-us/rest/\napi/storageservices/lease-blob\nI think Box 3 should be no insetad of yes. BreakRelease don`t release the lease directly.\nYou use ReleaseLease to do this: https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.blob.cloudblobcontainer.releaselease?view=azure-dotnet-legacy\nLast few words \"infinite lease breaks immediately\" is the key for this context.\nLast few words \"infinite lease breaks immediately\" is the key for this context.\nAh I think modele is right: BreakLeaseAsync (TimeSpan? breakPeriod) breakPeriod\nNullable<TimeSpan> A TimeSpan representing the amount of time to allow the lease to\nremain, which will be rounded down to seconds. If null, the break period is the\nremainder of the current lease, or zero for infinite leases.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "79",
    "question": "You are building a website that uses Azure Blob storage for data storage. You\nconfigure Azure Blob storage lifecycle to move all blobs to the archive tier after\n30 days.\nCustomers have requested a service-level agreement (SLA) for viewing data\nolder than 30 days.\nYou need to document the minimum SLA for data recovery.\nWhich SLA should you use?",
    "options": [
      {
        "letter": "A",
        "text": "at least two days",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "between one and 15 hours",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "at least one day",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "between zero and 60 minutes\nPage 175 of 1410\n176 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The archive access tier has the lowest storage cost. But it has higher data retrieval costs\ncompared to the hot and cool tiers. Data in the archive tier can take several hours to\nretrieve depending on the priority of the rehydration. For small objects, a high priority\nrehydrate may retrieve the object from archive in under 1 hour.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?\ntabs=azure-portal"
  },
  {
    "number": "80",
    "question": "HOTSPOT -\nYou are developing a ticket reservation system for an airline.\nThe storage solution for the application must meet the following requirements:\n✑ Ensure at least 99.99% availability and provide low latency.\n✑ Accept reservations even when localized network outages or other\nunforeseen failures occur.\n✑ Process reservations in the exact sequence as reservations are submitted to\nminimize overbooking or selling the same seat to multiple travelers.\n✑ Allow simultaneous and out-of-order reservations with a maximum five-\nsecond tolerance window.\nYou provision a resource group named airlineResourceGroup in the Azure\nSouth-Central US region.\nYou need to provision a SQL API Cosmos DB account to support the app.\nHow should you complete the Azure CLI commands? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: BoundedStaleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee.\nThe reads might lag behind writes by at most \"K\" versions (that is,\n\"updates\") of an item or by \"T\" time interval. In other words, when you choose bounded\nstaleness, the \"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (T) by which the reads might lag behind the writes\nIncorrect Answers:\nStrong -\nStrong consistency offers a linearizability guarantee. Linearizability refers to serving\nrequests concurrently. The reads are guaranteed to return the most recent committed\nversion of an item. A client never sees an uncommitted or partial write. Users are always\nguaranteed to read the latest committed write.\n\n\nBox 2: --enable-automatic-failover true\\\nFor multi-region Cosmos accounts that are configured with a single-write region, enable\nautomatic-failover by using Azure CLI or Azure portal. After you enable automatic failover,\nwhenever there is a regional disaster, Cosmos DB will automatically failover your\naccount.\nQuestion: Accept reservations event when localized network outages or other unforeseen\nfailures occur.\nBox 3: --locations'southcentralus=0 eastus=1 westus=2\nNeed multi-region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels https://\ngithub.com/MicrosoftDocs/azure-docs/blob/master/articles/cosmos-db/manage-with-\ncli.md\nCommunity Discussion\nAnswer is correct.\nNo need to overthink here, max-interval, indicates this must be bounded-slateness,\nenable-automatic-failover, indicated this must be multi-region\nThe strong consistency doesn't work here: 1) there is a --max-interval property which is\nbeing used with bounded staleness only 2) there is a requirement \"Accept reservations\nevent when localized network outages or other unforeseen failures occur.\" which points\nus to multiple writes for multiple regions which is not being supported by the Strong\nconsistency. --locations syntax looks like obsolete as for late march 2021\nEventual is definitely not correct\nEventual is definitely not correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "81",
    "question": "HOTSPOT -\nYou are preparing to deploy a Python website to an Azure Web App using a\ncontainer. The solution will use multiple containers in the same container\ngroup. The\nDockerfile that builds the container is as follows:\nYou build a container by using the following command. The Azure Container\nRegistry instance named images is a private registry.\nThe user name and password for the registry is admin.\nThe Web App must always run the same version of the website regardless of\nfuture builds.\nYou need to create an Azure Web App to run the website.\nHow should you complete the commands? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: --SKU B1 --hyper-v -\n--hyper-v\nHost web app on Windows container.\nBox 2: --deployment-source-url images.azurecr.io/website:v1.0.0\n--deployment-source-url -u\nGit repository URL to link with manual integration.\nThe Web App must always run the same version of the website regardless of future\nbuilds.\nIncorrect:\n--deployment-container-image-name -i\nLinux only. Container image name from Docker Hub, e.g. publisher/image-name:tag.\nBox 3: az webapp config container set -url https://images.azurecr.io -u admin -p admin az\nwebapp config container set\nSet a web app container's settings.\nParemeter: --docker-registry-server-url -r\nThe container registry server url.\nThe Azure Container Registry instance named images is a private registry.\nExample:\naz webapp config container set --docker-registry-server-url https://{azure-container-\nregistry-name}.azurecr.io\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/appservice/plan\n\n\nCommunity Discussion\n--sku B1 --is-linux --deployment-container-image-name images.azurecr.io/website:v1.0.0\n-- container set --docker-registry-server-url https://images.azurecr.io -u admin -p admin\n\"use multiple containers in the same container group\" this not is possible in windows.\nSolution is: --is-linux --deployment-container-image-name\nThat's true. \"Multi-container groups are currently restricted to Linux containers.\" https://\ndocs.microsoft.com/en-us/azure/container-instances/container-instances-multi-\ncontainer-group\nThat's true. \"Multi-container groups are currently restricted to Linux containers.\" https://\ndocs.microsoft.com/en-us/azure/container-instances/container-instances-multi-\ncontainer-group\nmicrosoft promote good practises for -u and -p ( ͡° ͜ʖ ͡°)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "82",
    "question": "HOTSPOT -\nYou are developing a back-end Azure App Service that scales based on the\nnumber of messages contained in a Service Bus queue.\nA rule already exists to scale up the App Service when the average queue length\nof unprocessed and valid queue messages is greater than 1000.\nYou need to add a new rule that will continuously scale down the App Service as\nlong as the scale up condition is not met.\nHow should you configure the Scale rule? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Service bus queue -\nYou are developing a back-end Azure App Service that scales based on the number of\nmessages contained in a Service Bus queue.\nBox 2: ActiveMessage Count -\nActiveMessageCount: Messages in the queue or subscription that are in the active state\nand ready for delivery.\nBox 3: Count -\nBox 4: Less than or equal to -\n\n\nYou need to add a new rule that will continuously scale down the App Service as long as\nthe scale up condition is not met.\nBox 5: Decrease count by\nCommunity Discussion\nThe correct answers are 1) Service bus queue 2) Active message count 3) Average 4) Less\nthan or equal to 5) Decrease count by\nFull image: https://vceguide.com/wp-content/uploads/2019/10/Microsoft-AZ-203-\ndate-01-06-2019-00001_Page_062_Image_0001.jpg\nthis is correct (the \"official\" solution is wrong) Also check the proper image https://\nvceguide.com/wp-content/uploads/2019/10/Microsoft-AZ-203-\ndate-01-06-2019-00001_Page_062_Image_0001.jpg\nthis is correct (the \"official\" solution is wrong) Also check the proper image https://\nvceguide.com/wp-content/uploads/2019/10/Microsoft-AZ-203-\ndate-01-06-2019-00001_Page_062_Image_0001.jpg\nBox 1: Service bus queue You are developing a back-end Azure App Service that scales\nbased on the number of messages contained in a Service Bus queue. Box 2:\nActiveMessage Count ActiveMessageCount: Number of messages in the queue or\nsubscription that are in the active state and ready for delivery. Box 3: Average For special\nmetrics such as Storage or Service Bus Queue length metric, the threshold is the average\nnumber of messages available per current number of instances. Box 4: Less than or\nequal to You need to add a new rule that will continuously scale down the App Service,\nas long as the scale up condition is not met. Box 5: Decrease count by",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "83",
    "question": "DRAG DROP -\nYou have an application that uses Azure Blob storage.\nYou need to update the metadata of the blobs.\nWhich three methods should you use to develop the solution? To answer, move\nthe appropriate methods from the list of methods to the answer area and\narrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nMetadata.Add example:\n// Add metadata to the dictionary by calling the Add method\nmetadata.Add(\"docType\", \"textDocuments\");\nSetMetadataAsync example:\n// Set the blob's metadata.\n\n\nawait blob.SetMetadataAsync(metadata);\n// Set the blob's properties.\nawait blob.SetPropertiesAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-\nmetadata\nCommunity Discussion\nSince we're talking about updating the metadata, - first we need to fetch it, to populate\nblob's properties and metadata (we want to update it - without fetching we would just\nset the new metadata): FetchAttributesAsync - second, we need to manipulate the\nmetadatas to update them and the best fitting is Metadata.Add - third, we have to persist\nour changes. We can use a method that initiates an asynchronous operation to update\nthe blob's metadata, which is SetMetadataAsync\nI suppose there are two correct answers, depending on the version of\nAzure.Storage.Blobs. For v11: FetchAttributesAsync, Metadata.Add, SetMetadataAsync. For\nv12: GetPropertiesAsync, Metadata.Add, SetMetadataAsync. Just look here, there are two\ntabs with source code. One for v11, one for v12: https://docs.microsoft.com/en-us/azure/\nstorage/blobs/storage-blob-properties-metadata?tabs=dotnet#set-and-retrieve-\nmetadata\nCorrect. But GetPropertiesAsync not mentioned in answer options. We can go with\nFetchAttributesAsync\nCorrect. But GetPropertiesAsync not mentioned in answer options. We can go with\nFetchAttributesAsync\nThis is correct answer. Others answers suggesting GetProperties/SetProperties are wrong,\nbecause GetProperties is no in options at all, and there is no SetProperties method.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "84",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data\nfrom 2,000 stores located throughout the world. A single device can produce\n2 megabytes (MB) of data every 24 hours. Each store location has one to five\ndevices that send data.\nYou must store the device data in Azure Blob storage. Device data must be\ncorrelated based on a device identifier. Additional stores are expected to open\nin the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Grid. Configure the machine identifier as the\npartition key and enable capture.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services"
  },
  {
    "number": "85",
    "question": "You develop Azure solutions.\nA .NET application needs to receive a message each time an Azure virtual\nmachine finishes processing data. The messages must NOT persist after being\nprocessed by the receiving application.\nYou need to implement the .NET object that will receive the messages.\nWhich object should you use?",
    "options": [
      {
        "letter": "A",
        "text": "QueueClient",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "SubscriptionClient",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "TopicClient",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "CloudQueueClient",
        "is_correct": false
      },
      {
        "letter": "A",
        "text": "as it supports \"At-Most-Once\" deliver mode\nwhile Azure.Storage.Queues.CloudQueueClient doesn't.\nMicrosoft creates all these random naming convention showing how disorganized they\nare in individual islands and they dare to ask questions along this fault lines.\nIt's QueueClient of Service Bus.\nIt's QueueClient of Service Bus.\nPage 194 of 1410\n195 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "A queue allows processing of a message by a single consumer. Need a CloudQueueClient\nto access the Azure VM.\nIncorrect Answers:\nB, C: In contrast to queues, topics and subscriptions provide a one-to-many form of\ncommunication in a publish and subscribe pattern. It's useful for scaling to large\nnumbers of recipients.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-\ntopics-subscriptions"
  },
  {
    "number": "86",
    "question": "DRAG DROP -\nYou are maintaining an existing application that uses an Azure Blob GPv1\nPremium storage account. Data older than three months is rarely used.\nData newer than three months must be available immediately. Data older than a\nyear must be saved but does not need to be available immediately.\nYou need to configure the account to support a lifecycle management rule that\nmoves blob data to archive storage for data not modified in the last year.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Upgrade the storage account to GPv2\nObject storage data tiering between hot, cool, and archive is supported in Blob Storage\nand General Purpose v2 (GPv2) accounts. General Purpose v1 (GPv1) accounts don't\nsupport tiering.\nYou can easily convert your existing GPv1 or Blob Storage accounts to GPv2 accounts\nthrough the Azure portal.\nStep 2: Copy the data to be archived to a Standard GPv2 storage account and then delete\nthe data from the original storage account\nStep 3: Change the storage account access tier from hot to cool\nNote: Hot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30\ndays.\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180\ndays with flexible latency requirements, on the order of hours.\nOnly the hot and cool access tiers can be set at the account level. The archive access tier\ncan only be set at the blob level.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\nCommunity Discussion\nSince we already have a premium P1 account with gpv1. Why not: - Upgrade the existing\none to GPv2 - Create a new GPV2 standard account with default access level to cool - And\nthen copy archive data to the GPV2 and delete the data from original storage account.\nThat makes sense to me.\nStep 1: Upgrade the storage account to GPv2 Object storage data tiering between hot,\ncool, and archive is supported in Blob Storage and General Purpose v2 (GPv2) accounts.\nGeneral Purpose v1 (GPv1) accounts don't support tiering. You can easily convert your\n\n\nexisting GPv1 or Blob Storage accounts to GPv2 accounts through the Azure portal. Step 2:\nCreate a new GPV2 standard account with default access level to cool Step 3: Copy the\ndata to be archived to a Standard GPv2 storage account and then delete the data from\nthe original storage account\nWhy we need to Upgrade the existing one to GPv2 as we are creating a new one?\nWhy we need to Upgrade the existing one to GPv2 as we are creating a new one?\nAlso look at this blog: https://www.apptio.com/blog/essential-guide-azure-blob-storage-\npricing/ Only GPv2 and Blob storage accounts support tiering. If you are using GPv1, and\nyou want to leverage tiering, convert your account to GPv2 through the Azure portal.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "87",
    "question": "You develop Azure solutions.\nYou must connect to a No-SQL globally-distributed database by using the .NET\nAPI.\nYou need to create an object to configure and execute requests in the database.\nWhich code segment should you use?",
    "options": [
      {
        "letter": "A",
        "text": "new Container(EndpointUri, PrimaryKey);",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "new Database(EndpointUri, PrimaryKey);",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "new CosmosClient(EndpointUri, PrimaryKey);",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "Example:\n// Create a new instance of the Cosmos Client\nthis.cosmosClient = new CosmosClient(EndpointUri, PrimaryKey)\n//ADD THIS PART TO YOUR CODE\nawait this.CreateDatabaseAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-get-started"
  },
  {
    "number": "88",
    "question": "You have an existing Azure storage account that stores large volumes of data\nacross multiple containers.\nYou need to copy all data from the existing storage account to a new storage\naccount. The copy process must meet the following requirements:\n✑ Automate data movement.\n✑ Minimize user input required to perform the operation.\n✑ Ensure that the data movement process is recoverable.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "AzCopy",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure Storage Explorer",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure portal",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": ".NET Storage Client Library",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "89",
    "question": "DRAG DROP -\nYou are developing a web service that will run on Azure virtual machines that\nuse Azure Storage. You configure all virtual machines to use managed identities.\nYou have the following requirements:\n✑ Secret-based authentication mechanisms are not permitted for accessing an\nAzure Storage account.\n✑ Must use only Azure Instance Metadata Service endpoints.\nYou need to write code to retrieve an access token to access Azure Storage. To\nanswer, drag the appropriate code segments to the correct locations. Each code\nsegment may be used once or not at all. You may need to drag the split bar\nbetween panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nAzure Instance Metadata Service endpoints \"/oauth2/token\"\nBox 1: http://169.254.169.254/metadata/identity/oauth2/token\nSample request using the Azure Instance Metadata Service (IMDS) endpoint\n(recommended):\nGET 'http://169.254.169.254/metadata/identity/oauth2/token?api-\nversion=2018-02-01&resource=https://management.azure.com/' HTTP/1.1 Metadata: true\nBox 2: JsonConvert.DeserializeObject<Dictionary<string,string>>(payload);\nDeserialized token response; returning access code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-\nresources/how-to-use-vm-token https://docs.microsoft.com/en-us/azure/service-fabric/\nhow-to-managed-identity-service-fabric-app-code\nCommunity Discussion\nBox 1: http://169.254.169.254/metadata/identity/oauth2/token Sample request using the\nAzure Instance Metadata Service (IMDS) endpoint (recommended): GET 'http://\n169.254.169.254/metadata/identity/oauth2/token?api-\nversion=2018-02-01&resource=https://management.azure.com/' HTTP/1.1 Metadata: true\nBox 2: JsonConvert.DeserializeObject<Dictionary<string,string>>(payload); Deserialized\ntoken response; returning access code. Reference: https://docs.microsoft.com/en-us/\nazure/active-directory/managed-identities-azure-resources/how-to-use-vm-token\nhttps://docs.microsoft.com/en-us/azure/service-fabric/how-to-managed-identity-\nservice-fabric-app-code\nI got this same question. Provided answers are correct. (Note: I failed the exam 20/9/23. I\nonly scored 644 and I felt bad. I think because many questions here in Examtopics are\nnot accurate. I suggest following the most voted answers and don't just not rely on\nExamtopics answers. At the beginning of the exam, you will be asked which programming\nlanguages you want to use. C#/Python. I chose C#. Also, I just want to add that some\n\n\nquestions here are really in the actual exams, but the choices are written and formatted\ndifferently. Please be aware of that. Goodluck. I feel bad for failing it, but I want to retake\nnext month. I will try Python. T_T\nIMDS is a REST API that's available at a well-known, non-routable IP address (\n169.254.169.254 ). You can only access it from within the VM. https://docs.microsoft.com/\nen-us/azure/virtual-machines/windows/instance-metadata-service?tabs=windows\nIMDS is a REST API that's available at a well-known, non-routable IP address (\n169.254.169.254 ). You can only access it from within the VM. https://docs.microsoft.com/\nen-us/azure/virtual-machines/windows/instance-metadata-service?tabs=windows\nIt was there in 13 Feb 2023 exam",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "90",
    "question": "DRAG DROP -\nYou are developing a new page for a website that uses Azure Cosmos DB for\ndata storage. The feature uses documents that have the following format:\nYou must display data for the new page in a specific order. You create the\nfollowing query for the page:\nYou need to configure a Cosmos DB policy to support the query.\nHow should you configure the policy? To answer, drag the appropriate JSON\nsegments to the correct locations. Each JSON segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: compositeIndexes -\nYou can order by multiple properties. A query that orders by multiple properties requires\na composite index.\nBox 2: descending -\nExample: Composite index defined for (name ASC, age ASC):\nIt is optional to specify the order. If not specified, the order is ascending.\n{\n\"automatic\":true,\n\"indexingMode\":\"Consistent\",\n\"includedPaths\":[\n{\n\"path\":\"/*\"\n}\n],\n\"excludedPaths\":[],\n\"compositeIndexes\":[\n[\n{\n\"path\":\"/name\",\n},\n{\n\"path\":\"/age\",\n\n\n}\n]\n]\n}\nCommunity Discussion\nORDER BY queries on multiple properties: The composite index also supports an ORDER\nBY clause with the opposite order on all paths. So I think it's about reversed index to the\nquery. Answer should be 'ascending'. You cannot support ASC (default), DESC query with\nDESC, DESC index.\n\"name\" field should be marked ascending (default if not specified). It's mislabeled\nThe problem here is the SQL that makes many people think that \"ORDER BY p.name,\np.city DESC\" means it's ordered by name and city both descending. But the DESC only\napplies to city. name is ASC - this would be less confusing: \"ORDER BY p.name ASC, p.city\nDESC\" Thus in the JSON you can only state ascending+descending or the opposite:\ndescending+ascending. Since descending for name is already set the answer is\n\"ascending\". At first I had misread the SQL wrong myself and didn't understand\nkayleena's comment right away.\nThe problem here is the SQL that makes many people think that \"ORDER BY p.name,\np.city DESC\" means it's ordered by name and city both descending. But the DESC only\napplies to city. name is ASC - this would be less confusing: \"ORDER BY p.name ASC, p.city\nDESC\" Thus in the JSON you can only state ascending+descending or the opposite:\ndescending+ascending. Since descending for name is already set the answer is\n\"ascending\". At first I had misread the SQL wrong myself and didn't understand\nkayleena's comment right away.\nNO. Box 2 is \"ascending\" See explanation here: https://docs.microsoft.com/en-us/azure/\ncosmos-db/index-policy#order-by-queries-on-multiple-properties \"The composite index\nalso supports an ORDER BY clause with the __opposite order on all paths__.\" The table in\nthe section also shows an example similar to this question.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "91",
    "question": "HOTSPOT -\nYou are building a traffic monitoring system that monitors traffic along six\nhighways. The system produces time series analysis-based reports for each\nhighway.\nData from traffic sensors are stored in Azure Event Hub.\nTraffic data is consumed by four departments. Each department has an Azure\nWeb App that displays the time series-based reports and contains a WebJob\nthat processes the incoming data from Event Hub. All Web Apps run on App\nService Plans with three instances.\nData throughput must be maximized. Latency must be minimized.\nYou need to implement the Azure Event Hub.\nWhich settings should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: 6 -\nThe number of partitions is specified at creation and must be between 2 and 32.\nThere are 6 highways.\nBox 2: Highway -\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features\nCommunity Discussion\nPartitions relate to producers - and the logical way to partition the incoming data is by\nthe only value you have at that point, the highway name/id. So the selected answer is\ncorrect (6 Partitions, by Highway). People are getting confused by the departments which\nwould actually each be an event consumer with an associated Consumer Group which\nwould have it's own isolated view of each of the highway partitions.\nThe answer should be 4 and Highway. Sec Exams - Please provide correct answers. What\nis the use of buying questions on your site...if you are not sure of the answer yourself\nThere are 6 highways and 6 reports. Each department only needs to read one partition to\nproduce their report on that one highway. If you had 4 partitions you would have to\nduplicate all the data 4 times (BAD) and then each department would need to read all\nthe data and filter the data for one report (VERY BAD).\n\n\nThere are 6 highways and 6 reports. Each department only needs to read one partition to\nproduce their report on that one highway. If you had 4 partitions you would have to\nduplicate all the data 4 times (BAD) and then each department would need to read all\nthe data and filter the data for one report (VERY BAD).\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-faq The number of\npartitions in an event hub directly relates to the number of concurrent readers you\nexpect to have",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "92",
    "question": "DRAG DROP -\nYou are developing a microservices solution. You plan to deploy the solution to\na multinode Azure Kubernetes Service (AKS) cluster.\nYou need to deploy a solution that includes the following features:\n✑ reverse proxy capabilities\n✑ configurable traffic routing\n✑ TLS termination with a custom certificate\nWhich components should you use? To answer, drag the appropriate\ncomponents to the correct requirements. Each component may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Helm -\nTo create the ingress controller, use Helm to install nginx-ingress.\nBox 2: kubectl -\nTo find the cluster IP address of a Kubernetes pod, use the kubectl get pod command on\nyour local machine, with the option -o wide .\nBox 3: Ingress Controller -\nAn ingress controller is a piece of software that provides reverse proxy, configurable\ntraffic routing, and TLS termination for Kubernetes services. Kubernetes ingress resources\nare used to configure the ingress rules and routes for individual Kubernetes services.\nIncorrect Answers:\nVirtual Kubelet: Virtual Kubelet is an open-source Kubernetes kubelet implementation\nthat masquerades as a kubelet. This allows Kubernetes nodes to be backed by Virtual\nKubelet providers such as serverless cloud container platforms.\nCoreDNS: CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes\ncluster DNS. Like Kubernetes, the CoreDNS project is hosted by the\nCNCF.\nReference:\nhttps://docs.microsoft.com/bs-cyrl-ba/azure/aks/ingress-basic https://\nwww.digitalocean.com/community/tutorials/how-to-inspect-kubernetes-networking\nCommunity Discussion\n\n\nBox 1: Helm Helm helps you manage Kubernetes applications — Helm Charts help you\ndefine, install, and upgrade even the most complex Kubernetes application. To create the\ningress controller, use Helm to install nginx-ingress. Box 2: Kubectl The Kubernetes\ncommand-line tool, kubectl, allows you to run commands against Kubernetes clusters. To\nfind the cluster IP address of a Kubernetes pod, use the kubectl get pod command on\nyour local machine, with the option -o wide . Box 3: Ingress Controller An ingress\ncontroller is a piece of software that provides reverse proxy, configurable traffic routing,\nand TLS termination for Kubernetes services. Kubernetes ingress resources are used to\nconfigure the ingress rules and routes for individual Kubernetes services. Using an\ningress controller and ingress rules, a single IP address can be used to route traffic to\nmultiple services in a Kubernetes cluster.\nI believe there is no AKS question in the exam: https://query.prod.cms.rt.microsoft.com/\ncms/api/am/binary/RE4oZ7B\nCorrect, I am a certified CKA und CKAD. The question isn't that well written though as you\ncould also deploy a solution with kubectl...\nCorrect, I am a certified CKA und CKAD. The question isn't that well written though as you\ncould also deploy a solution with kubectl...\nReference: https://helm.sh https://kubernetes.io/docs/tasks/tools https://\nkubernetes.io/docs/concepts/services-networking/ingress-controllers https://\ndocs.microsoft.com/bs-cyrl-ba/azure/aks/ingress-basic https://www.digitalocean.com/\ncommunity/tutorials/how-to-inspect-kubernetes-networking",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "93",
    "question": "DRAG DROP -\nYou are implementing an order processing system. A point of sale application\npublishes orders to topics in an Azure Service Bus queue. The Label property for\nthe topic includes the following data:\nThe system has the following requirements for subscriptions:\nYou need to implement filtering and maximize throughput while evaluating\nfilters.\nWhich filter types should you implement? To answer, drag the appropriate filter\ntypes to the correct subscriptions. Each filter type may be used once, more than\nonce, or not at all. You may need to drag the split bar between panes or scroll\nto view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nFutureOrders: SQLFilter -\nHighPriortyOrders: CorrelationFilter\nCorrelationID only -\nInternationalOrders: SQLFilter -\nCountry NOT USA requires an SQL Filter\nHighQuantityOrders: SQLFilter -\nNeed to use relational operators so an SQL Filter is needed.\nAllOrders: No Filter -\nSQL Filter: SQL Filters - A SqlFilter holds a SQL-like conditional expression that is\nevaluated in the broker against the arriving messages' user-defined properties and\nsystem properties. All system properties must be prefixed with sys. in the conditional\nexpression. The SQL-language subset for filter conditions tests for the existence of\nproperties (EXISTS), as well as for null-values (IS NULL), logical NOT/AND/OR, relational\noperators, simple numeric arithmetic, and simple text pattern matching with LIKE.\nCorrelation Filters - A CorrelationFilter holds a set of conditions that are matched against\none or more of an arriving message's user and system properties. A common use is to\nmatch against the CorrelationId property, but the application can also choose to match\nagainst ContentType, Label, MessageId, ReplyTo,\nReplyToSessionId, SessionId, To, and any user-defined properties. A match exists when an\narriving message's value for a property is equal to the value specified in the correlation\nfilter. For string expressions, the comparison is case-sensitive. When specifying multiple\nmatch properties, the filter combines them as a logical\nAND condition, meaning for the filter to match, all conditions must match.\nBoolean filters - The TrueFilter and FalseFilter either cause all arriving messages (true) or\n\n\nnone of the arriving messages (false) to be selected for the subscription.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters\nCommunity Discussion\nI think that it should be -Correlation Filter (with the not existing value of any filed to\navoid getting any message) -SQL filter (as we need to get all high priority AND\ninternational orders, but for Correlation filter: A match exists when an arriving message's\nvalue for a property is equal to the value specified in the correlation filter and we need\nnot equal) -SQL filter -SQL filter -No Filter\nThe Correct answers are: No Filter Correleation Filter SQL filter SQL filter SQL filter\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters \"If you don't\nexplicitly specify a filter condition for the rule, the applied filter is the true filter that\nenables all messages to be selected into the subscription.\"\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters \"If you don't\nexplicitly specify a filter condition for the rule, the applied filter is the true filter that\nenables all messages to be selected into the subscription.\"\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters \"If you don't\nexplicitly specify a filter condition for the rule, the applied filter is the true filter that\nenables all messages to be selected into the subscription.\"",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "94",
    "question": "DRAG DROP -\nYour company has several websites that use a company logo image. You use\nAzure Content Delivery Network (CDN) to store the static image.\nYou need to determine the correct process of how the CDN and the Point of\nPresence (POP) server will distribute the image and list the items in the correct\norder.\nIn which order do the actions occur? To answer, move all actions from the list of\nactions to the answer area and arrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: A user requests the image..\nA user requests a file (also called an asset) by using a URL with a special domain name,\nsuch as <endpoint name>.azureedge.net. This name can be an endpoint hostname or a\ncustom domain. The DNS routes the request to the best performing POP location, which\nis usually the POP that is geographically closest to the user.\nStep 2: If no edge servers in the POP have the..\nIf no edge servers in the POP have the file in their cache, the POP requests the file from\nthe origin server. The origin server can be an Azure Web App, Azure\nCloud Service, Azure Storage account, or any publicly accessible web server.\nStep 3: The origin server returns the..\nThe origin server returns the file to an edge server in the POP.\nAn edge server in the POP caches the file and returns the file to the original requestor\n(Alice). The file remains cached on the edge server in the POP until the time-to-live (TTL)\nspecified by its HTTP headers expires. If the origin server didn't specify a TTL, the default\nTTL is seven days.\nStep 4: Subsequent requests for..\nAdditional users can then request the same file by using the same URL that the original\nuser used, and can also be directed to the same POP.\nIf the TTL for the file hasn't expired, the POP edge server returns the file directly from the\ncache. This process results in a faster, more responsive user experience.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-overview\nCommunity Discussion\nGiven ans is correct\n\n\nIt feels so good when there are no debates on what is the correct answer.\nIt feels so good when there are no debates on what is the correct answer.\nThe given answer is correct.\ntime travel!!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "95",
    "question": "You are developing an Azure Cosmos DB solution by using the Azure Cosmos DB\nSQL API. The data includes millions of documents. Each document may contain\nhundreds of properties.\nThe properties of the documents do not contain distinct values for partitioning.\nAzure Cosmos DB must scale individual containers in the database to meet the\nperformance needs of the application by spreading the workload evenly across\nall partitions over time.\nYou need to select a partition key.\nWhich two partition keys can you use? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "a single property value that does not appear frequently in the documents",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a value containing the collection name",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a single property value that appears frequently in the documents",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a concatenation of multiple property values with a random suffix appended (Correct\nAnswer)\nE) a hash suffix appended to a property value",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "E\nYou can form a partition key by concatenating multiple property values into a single\nartificial partitionKey property. These keys are referred to as synthetic keys.\nAnother possible strategy to distribute the workload more evenly is to append a random\nnumber at the end of the partition key value. When you distribute items in this way, you\ncan perform parallel write operations across partitions."
  },
  {
    "number": "96",
    "question": "HOTSPOT -\nYou are developing an Azure-hosted e-commerce web application. The\napplication will use Azure Cosmos DB to store sales orders. You are using the\nlatest SDK to manage the sales orders in the database.\nYou create a new Azure Cosmos DB instance. You include a valid endpoint and\nvalid authorization key to an appSettings.json file in the code project.\nYou are evaluating the following application code: (Line number are included\nfor reference only.)\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Yes -\nThe createDatabaseIfNotExistsAsync method checks if a database exists, and if it doesn't,\ncreate it.\nThe Database.CreateContainerAsync method creates a container as an asynchronous\noperation in the Azure Cosmos service.\nBox 2: Yes -\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous\noperation in the Azure Cosmos service.\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/\n\n\nmicrosoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-\nus/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync\nCommunity Discussion\nBox 1: Yes The createDatabaseIfNotExistsAsync method checks if a database exists, and if\nit doesn't, create it. (Line 22) The Database.CreateContainerAsync method creates a\ncontainer as an asynchronous operation in the Azure Cosmos service. (Line 23 and 24)\nBox 2: Yes The CosmosContainer.CreateItemAsync method creates an item as an\nasynchronous operation in the Azure Cosmos service. (Line 26 and 28) Box 3: Yes The\nCosmosContainer.CreateItemAsync method creates an item as an asynchronous\noperation in the Azure Cosmos service. (Line 30)\nReference: https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-\nus/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync\nReference: https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-\nus/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync\nLine 21 is tricky, it assumes the database is already created as it calls DeleteStreamAsync.\nI'm confused.\nOn my exam 2023-08-20. Scored 925 Yes Yes Yes",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "97",
    "question": "DRAG DROP -\nYou develop an Azure solution that uses Cosmos DB.\nThe current Cosmos DB container must be replicated and must use a partition\nkey that is optimized for queries.\nYou need to implement a change feed processor solution.\nWhich change feed processor components should you use? To answer, drag the\nappropriate components to the correct requirements. Each component may be\nused once, more than once, or not at all. You may need to drag the split bar\nbetween panes or scroll to view the content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: The monitored container -\nThe monitored container has the data from which the change feed is generated. Any\ninserts and updates to the monitored container are reflected in the change feed of the\ncontainer.\nBox 2: The lease container -\n\n\nThe lease container acts as a state storage and coordinates processing the change feed\nacross multiple workers. The lease container can be stored in the same account as the\nmonitored container or in a separate account.\nBox 3: The host: A host is an application instance that uses the change feed processor to\nlisten for changes. Multiple instances with the same lease configuration can run in\nparallel, but each instance should have a different instance name.\nBox 4: The delegate -\nThe delegate is the code that defines what you, the developer, want to do with each\nbatch of changes that the change feed processor reads.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor\nCommunity Discussion\nThe given answer is correct.\nAnd your given answer, that MattXu is right, is also correct.\nthat observation is correct, john\nAnd your given answer, that MattXu is right, is also correct.\nAnd your given answer, that MattXu is right, is also correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "98",
    "question": "HOTSPOT -\nYou are developing a web application that will use Azure Storage. Older data\nwill be less frequently used than more recent data.\nYou need to configure data storage for the application. You have the following\nrequirements:\n✑ Retain copies of data for five years.\n✑ Minimize costs associated with storing data that is over one year old.\n✑ Implement Zone Redundant Storage for application data.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/\nazure/storage/blobs/toc.json\nCommunity Discussion\nSo, because it is ZRS, and that does not support arhive tier, it cannot be moved to\narchive tier even though the questions mention the red-herring key-word \"infrequently\naccessed\" (which triggers feelings for archive tier). For no logically apparent reason\nMicrosoft decided not to support archive tier in ZRS and unfortunately I have to\nremember that Microsoft \"feature\"?\nZRS, and that does not support arhive tier - this is key point, Thanks\nZRS, and that does not support arhive tier - this is key point, Thanks\nGot in exam. go with given answer\nCorrect answer. Got this in exam on 30/12/2023. Case study: Contoso Ltd. Total questions:\n46 Time: 1:40 minutes Score: 940 43 questions from Sec Exams. Just 3 questions outside\nof it.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "99",
    "question": "HOTSPOT -\nA company develops a series of mobile games. All games use a single\nleaderboard service.\nYou have the following requirements:\n✑ Code must be scalable and allow for growth.\n✑ Each record must consist of a playerId, gameId, score, and time played.\n✑ When users reach a new high score, the system will save the new score using\nthe SaveScore function below.\nEach game is assigned an Id based on the series title.\nYou plan to store customer information in Azure Cosmos DB. The following data\nalready exists in the database:\nYou develop the following code to save scores in the data store. (Line numbers\nare included for reference only.)\nYou develop the following code to query the database. (Line numbers are\nincluded for reference only.)\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Yes -\nCreate a table.\nA CloudTableClient object lets you get reference objects for tables and entities. The\nfollowing code creates a CloudTableClient object and uses it to create a new\nCloudTable object, which represents a table\n// Retrieve storage account from connection-string.\nCloudStorageAccount storageAccount =\nCloudStorageAccount.parse(storageConnectionString);\n// Create the table client.\nCloudTableClient tableClient = storageAccount.createCloudTableClient();\n// Create the table if it doesn't exist.\n\n\nString tableName = \"people\";\nCloudTable cloudTable = tableClient.getTableReference(tableName);\ncloudTable.createIfNotExists();\nBox 2: No -\nNew records are inserted with TableOperation.insert. Old records are not updated.\nTo update old records TableOperation.insertOrReplace should be used instead.\nBox 3: No -\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-how-to-use-java\nCommunity Discussion\ngiven answer is correct.\nthe given answer seems correct\nInserting will fail, this would require InsertOrReplace to work\nWhat code should be there if doing automatic partitioning (C) remains a total secret to\nme. Should be somewhere on the table level...\nIncorrect. First one is \"No\"",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "100",
    "question": "You develop and deploy a web application to Azure App Service. The application\naccesses data stored in an Azure Storage account. The account contains several\ncontainers with several blobs with large amounts of data. You deploy all Azure\nresources to a single region.\nYou need to move the Azure Storage account to the new region. You must copy\nall data to the new region.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Export the Azure Storage account Azure Resource Manager template",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Initiate a storage account failover",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure object replication for all blobs",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use the AzCopy command line tool\nE) Create a new Azure Storage account in the current region\nF) Create a new subscription in the current region",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "To move a storage account, create a copy of your storage account in another region.\nThen, move your data to that account by using AzCopy, or another tool of your choice\nand finally, delete the resources in the source region.\nTo get started, export, and then modify a Resource Manager template.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-move?\ntabs=azure-portal"
  },
  {
    "number": "101",
    "question": "HOTSPOT -\nYou are developing an application to collect the following telemetry data for\ndelivery drivers: first name, last name, package count, item id, and current\nlocation coordinates. The app will store the data in Azure Cosmos DB.\nYou need to configure Azure Cosmos DB to query the data.\nWhich values should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Core (SQL)\nCore(SQL) API stores data in document format. It offers the best end-to-end experience\nas we have full control over the interface, service, and the SDK client libraries. SQL API\nsupports analytics and offers performance isolation between operational and analytical\nworkloads.\nBox 2: item id -\nitem id is a unique identifier and is suitable for the partition key.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/choose-api\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview\nCommunity Discussion\nWithout knowing the functionality or the usage pattern or what it is for. Good lord,\nMicrosoft\nGot this in 09/22 , went with SQL and Item Id, score 927.\n\n\nThey changed names again. \"Core (SQL)\" is \"Api for NoSQL\" now. Its wonderful to choose\n\"SQL\" for \"NoSQL\". https://www.c-sharpcorner.com/article/road-to-az-2044/\n#:~:text=Core%20SQL%20API%2C%20default%20API%20for%20using%20Azure%20Cosmos%20DB%20enables%20querying%20your%20data%20with%20a%20language%20very%20close%20to%20SQL%3B\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api#coresql-\napi:~:text=API%20for%20NoSQL%20is%20native%20to%20Azure%20Cosmos%20DB.\nOn my exam 2023sept\nAnswer seems correct, but the question is very bad. It doesn't even tell about the usage,\nso it could be Table API as well",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "102",
    "question": "DRAG DROP -\nYou are implementing an Azure solution that uses Azure Cosmos DB and the\nlatest Azure Cosmos DB SDK. You add a change feed processor to a new\ncontainer instance.\nYou attempt to read a batch of 100 documents. The process fails when reading\none of the documents. The solution must monitor the progress of the change\nfeed processor instance on the new container as the change feed is read. You\nmust prevent the change feed processor from retrying the entire batch when\none document cannot be read.\nYou need to implement the change feed processor to read the documents.\nWhich features should you use? To answer, drag the appropriate features to the\ncored requirements. Each feature may be used once, more than once, or not at\nall. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each cored selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: Change feed estimator -\nYou can use the change feed estimator to monitor the progress of your change feed\nprocessor instances as they read the change feed or use the life cycle notifications to\ndetect underlying failures.\nBox 2: Dead-letter queue -\nTo prevent your change feed processor from getting \"stuck\" continuously retrying the\nsame batch of changes, you should add logic in your delegate code to write documents,\nupon exception, to a dead-letter queue. This design ensures that you can keep track of\nunprocessed changes while still being able to continue to process future changes. The\ndead-letter queue might be another Cosmos container. The exact data store does not\nmatter, simply that the unprocessed changes are persisted.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/change-feed-processor\nCommunity Discussion\nThe answer is correct.\nGot this on 16/03/23. Went with proposed solution. Make sure to prepare for case study. I\ngot city and lights case study. No Kubernetes, Search, Logic Apps questions for me.\nI got this same question. Provided answers are correct. (Note: I failed the exam 20/9/23. I\nonly scored 644 and I felt bad. I think because many questions here in Examtopics are\nnot accurate. I suggest following the most voted answers and don't just not rely on\nExamtopics answers. At the beginning of the exam, you will be asked which programming\nlanguages you want to use. C#/Python. I chose C#. Also, I just want to add that some\nquestions here are really in the actual exams, but the choices are written and formatted\n\n\ndifferently. Please be aware of that. Goodluck. I feel bad for failing it, but I want to retake\nnext month. I will try Python. T_T\nOn exam 3-Nov-2023. Went with proposed anwer - 932/1000. 1) Change feed estimator 2)\nDead letter queue\nthis is a bad site",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "103",
    "question": "HOTSPOT -\nYou are developing an application that uses a premium block blob storage\naccount. The application will process a large volume of transactions daily. You\nenable\nBlob storage versioning.\nYou are optimizing costs by automating Azure Blob Storage access tiers. You\napply the following policy rules to the storage account. (Line numbers are\nincluded for reference only.)\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nWould be true if daysAfterModificationGreaterThan was used, but here\ndaysAfterCreationGreaterThan\nBox 2: No -\nWould need to use the daysAfterLastAccessTimeGreaterThan predicate.\nBox 3: Yes -\nBox 4: Yes -\nWith the lifecycle management policy, you can:\nTransition blobs from cool to hot immediately when they are accessed, to optimize for\nperformance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview\nCommunity Discussion\nI guess, third statement (The policy rule tiers..) result is Yes. Container name\n\"transactions\" is in prefixMatch. https://docs.microsoft.com/en-us/azure/storage/blobs/\nlifecycle-management-overview#archive-data-after-ingest Solution is: - No - No - Yes -\nNo\n\n\nI guess, third statement (The policy rule tiers..) result is Yes. Container name\n\"transactions\" is in prefixMatch. https://docs.microsoft.com/en-us/azure/storage/blobs/\nlifecycle-management-overview#archive-data-after-ingest Solution is: - No - No - Yes -\nNo\nWith this image, all answers are NO: - Container named transaction is not in code - is no\npresent line \"enableAutoTierToHotFromCool\": true\nNo - Not modified, created. No - Not accessed, created. Yes - Rules are matching the\nstatement. The prefix \"transactions\" can be applicable for containers as well. \"container\"\n/ \"container/blob\" or \"blob\" can be used under this context. Source: https://\nlearn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-\noverview#archive-data-after-ingest No - \"enableAutoTierToHotFromCool\": \"true\" should\nbe enabled.\nreceived 2023-04-17 went N,N,Y, score 926 last box was not there only first three",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "104",
    "question": "An organization deploys Azure Cosmos DB.\nYou need to ensure that the index is updated as items are created, updated, or\ndeleted.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Set the indexing mode to Lazy.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Set the value of the automatic property of the indexing policy to False.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Set the value of the EnableScanInQuery option to True.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set the indexing mode to Consistent.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Azure Cosmos DB supports two indexing modes:\nConsistent: The index is updated synchronously as you create, update or delete items.\nThis means that the consistency of your read queries will be the consistency configured\nfor the account.\nNone: Indexing is disabled on the container.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/index-policy"
  },
  {
    "number": "105",
    "question": "You are developing a .Net web application that stores data in Azure Cosmos DB.\nThe application must use the Core API and allow millions of reads and writes.\nThe Azure Cosmos DB account has been created with multiple write regions\nenabled. The application has been deployed to the East US2 and Central US\nregions.\nYou need to update the application to support multi-region writes.\nWhat are two possible ways to achieve this goal? Each correct answer presents\npart of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Update the ConnectionPolicy class for the Cosmos client and populate the\nPreferredLocations property based on the geo-proximity of the application.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Update Azure Cosmos DB to use the Strong consistency level. Add indexed properties to the\ncontainer to indicate region.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update the ConnectionPolicy class for the Cosmos client and set the\nUseMultipleWriteLocations property to true.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create and deploy a custom conflict resolution policy. \nE) Update Azure Cosmos DB to use the Session consistency level. Send the SessionToken\nproperty value from the FeedResponse object of the write action to the end-user by using a\ncookie.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "and add the regions (option A) Then you have\nto apply the Conflict resolution policies.This can be LLW(default, not mentioned) or\ncustom (option D). Hence : there is only ONE way to to support multi-region writes (both\napply C AND A) and there are subsequently TWO ways to apply the Conflict resolution\npolicies (@ SQL) to solve write, update and delete conflicts of which one is mentioned in\nthe question (D). To support multi-region writes I would answer A and C , but they have\nto be set both, not one or the other. See https://learn.microsoft.com/en-us/azure/\ncosmos-db/sql/how-to-multi-master?tabs=api-async and https://learn.microsoft.com/\nen-us/azure/cosmos-db/conflict-resolution-policies\nSelected Answer: AC\nA + C = Clearly stated in documentation that both are required: https://\nlearn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-multi-master?tabs=api-\nasync. And according to the question \"Each correct answer presents part of the solution\".\n\"Part\" Not D -> \"Custom conflict resolution policy is available only for API for NoSQL\naccounts and can be set only at creation time.\" The question states the account was\nalready created. Source: https://learn.microsoft.com/en-us/azure/cosmos-db/conflict-\nresolution-policies\nFrom the documentation you provided: Within the ConnectionPolicy, set\nUseMultipleWriteLocations to true and pass the name of the region where the\napplication is deployed to ApplicationRegion. This will populate the PreferredLocations\nproperty based on the geo-proximity from location passed in. If a new region is later\nadded to the account, the application does not have to be updated or redeployed, it will\nautomatically detect the closer region and will auto-home on to it should a regional\nevent occur. I take that paragraph to mean that it automatically updates the\nPage 245 of 1410\n246 Microsoft - AZ-204 Practice Questions - SecExams.com\nPreferredLocations property, when you set the UseMultipleWriteLocations property with\ntrue and pass the region name. That means you actually only need to update\nUseMultipleWriteLocations, thus C. And as you mentioned, apply the Conflict resolution\npolicies (D).\nFrom the documentation you provided: Within the ConnectionPolicy, set\nUseMultipleWriteLocations to true and pass the name of the region where the\napplication is deployed to ApplicationRegion. This will populate the PreferredLocations\nproperty based on the geo-proximity from location passed in. If a new region is later\nadded to the account, the application does not have to be updated or redeployed, it will\nautomatically detect the closer region and will auto-home on to it should a regional\nevent occur. I take that paragraph to mean that it automatically updates the\nPreferredLocations property, when you set the UseMultipleWriteLocations property with\ntrue and pass the region name. That means you actually only need to update\nUseMultipleWriteLocations, thus C. And as you mentioned, apply the Conflict resolution\npolicies (D).\nSelected Answer: AC\nOption C: Update the ConnectionPolicy class for the Cosmos client and set the\nUseMultipleWriteLocations property to true. This will enable the application to use\nmultiple write regions when storing data in Azure Cosmos DB. Option A: Update the\nConnectionPolicy class for the Cosmos client and populate the PreferredLocations\nproperty based on the geo-proximity of the application. This will allow the application to\nuse the closest write region to the application's location, improving performance and\nreducing latency. Other options, such as updating the consistency level or creating a\ncustom conflict resolution policy, are not directly related to enabling multi-region writes\nand are not necessary for this goal.\nPage 246 of 1410\n247 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "D\nC: The UseMultipleWriteLocations of the ConnectionPolicy class gets or sets the flag to\nenable writes on any locations (regions) for geo-replicated database accounts in the\nAzure Cosmos DB service.\nNote: Once an account has been created with multiple write regions enabled, you must\nmake two changes in your application to the ConnectionPolicy for the\nCosmos client to enable the multi-region writes in Azure Cosmos DB. Within the\nConnectionPolicy, set UseMultipleWriteLocations to true and pass the name of the region\nwhere the application is deployed to ApplicationRegion. This will populate the\nPreferredLocations property based on the geo-proximity from location passed in. If a new\nregion is later added to the account, the application does not have to be updated or\nredeployed, it will automatically detect the closer region and will auto-home on to it\nshould a regional event occur.\nAzure core API application \" ConnectionPolicy class\" cosmos db multiple write regions\nenabled"
  },
  {
    "number": "106",
    "question": "HOTSPOT -\nYou are developing a solution to store documents in Azure Blob storage.\nCustomers upload documents to multiple containers. Documents consist of PDF,\nCSV,\nMicrosoft Office format and plain text files.\nThe solution must process millions of documents across hundreds of\ncontainers. The solution must meet the following requirements:\n✑ Documents must be categorized by a customer identifier as they are\nuploaded to the storage account.\n✑ Allow filtering by the customer identifier.\n✑ Allow searching of information contained within a document\n✑ Minimize costs.\nYou create and configure a standard general-purpose v2 storage account to\nsupport the solution.\nYou need to implement the solution.\nWhat should you implement? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Azure Blob index tags -\nAs datasets get larger, finding a specific object in a sea of data can be difficult. Blob\nindex tags provide data management and discovery capabilities by using key- value\nindex tag attributes. You can categorize and find objects within a single container or\nacross all containers in your storage account. As data requirements change, objects can\nbe dynamically categorized by updating their index tags. Objects can remain in-place\nwith their current container organization.\nBox 2: Azure Cognitive Search -\nOnly index tags are automatically indexed and made searchable by the native Blob\nStorage service. Metadata can't be natively indexed or searched. You must use a separate\nservice such as Azure Search.\nAzure Cognitive Search is the only cloud search service with built-in AI capabilities that\nenrich all types of information to help you identify and explore relevant content at scale.\nUse cognitive skills for vision, language, and speech, or use custom machine learning\nmodels to uncover insights from all types of content.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-manage-find-blobs\nhttps://azure.microsoft.com/en-us/services/search/\nCommunity Discussion\n\n\nright Azure Blob Index tags: https://docs.microsoft.com/en-us/azure/storage/blobs/\nstorage-blob-index-how-to?tabs=azure-portal Azure Cognitive Search: Search inside\ndocuments\nDid my exam on 15th November 2022. This question was on it.\nFarmers and Distributors is that a new case study?\nFarmers and Distributors is that a new case study?\nOn exam 9 Nov 2023, went with given answer, socre 865. Case Study: Farmers and\nDistributors",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "107",
    "question": "HOTSPOT -\nYou are developing a web application by using the Azure SDK. The web\napplication accesses data in a zone-redundant BlockBlobStorage storage\naccount.\nThe application must determine whether the data has changed since the\napplication last read the data. Update operations must use the latest data\nchanges when writing data to the storage account.\nYou need to implement the update operations.\nWhich values should you use? To answer, select the appropriate option in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Last Modified -\nThe Last-Modified response HTTP header contains a date and time when the origin\nserver believes the resource was last modified. It is used as a validator to determine if\nthe resource is the same as the previously stored one. Less accurate than an ETag\nheader, it is a fallback mechanism.\nBox 2: If-Modified-Since -\nConditional Header If-Modified-Since:\nA DateTime value. Specify this header to perform the operation only if the resource has\nbeen modified since the specified time.\nIncorrect:\nNot ETag/If-Match -\nConditional Header If-Match:\nAn ETag value. Specify this header to perform the operation only if the resource's ETag\nmatches the value specified. For versions 2011-08-18 and newer, the\nETag can be specified in quotes.\nReference:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Last-Modified https://\ndocs.microsoft.com/en-us/rest/api/storageservices/specifying-conditional-headers-for-\nblob-service-operations\n\n\nCommunity Discussion\nI think it should be: - ETag - server returns this tag for a resource to ensure we operate\non the same version of the resource in subsequent API calls - If-Match - update is\nprocessed by the server only if the ETag provided matches the latest resource version\nETag The reason for that is we want to make sure we update the latest version of a\nresource: \"Update operations must use the latest data changes when writing\" So, when\nusing Last-Modified with If-Modified-Since, the operation executes only when another\nclient modifies the resource between our READ and WRITE operations. If we wanted to\nuse Last-Modified instead, we would need If-Unmodified-Since instead.\nI referred to the documentation also and I cannot convince my mind I care enough to\neven understand this because there is no chance I will ever have to know these options\nin details unless I have to use it. Who reads and understands all these information when\nthis is available as documentation and with Microsoft documentation it is all about trial\nand error and hoping for the best\nYou should trust the discussions and other users experiencies. The default answers in\nthe questions might be right or wrong.\nYou should trust the discussions and other users experiencies. The default answers in\nthe questions might be right or wrong.\nI agree with you. (ETag + If-Match) https://developer.mozilla.org/en-US/docs/Web/HTTP/\nHeaders/If-Match",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "108",
    "question": "HOTSPOT -\nAn organization deploys a blob storage account. Users take multiple snapshots\nof the blob storage account over time.\nYou need to delete all snapshots of the blob storage account. You must not\ndelete the blob storage account itself.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: DeleteSnapshotsOption -\nSample code in powershell:\n//dont forget to add the include snapshots :)\nawait batchClient.DeleteBlobsAsync(listofURIforBlobs,\nAzure.Storage.Blobs.Models.DeleteSnapshotsOption.IncludeSnapshots);\nSample code in .Net:\n// Create a batch with three deletes\nBlobBatchClient batchClient = service.GetBlobBatchClient();\nBlobBatch batch = batchClient.CreateBatch();\nbatch.DeleteBlob(foo.Uri, DeleteSnapshotsOption.IncludeSnapshots);\nbatch.DeleteBlob(bar.Uri, DeleteSnapshotsOption.OnlySnapshots);\nbatch.DeleteBlob(baz.Uri);\n// Submit the batch\n\n\nbatchClient.SubmitBatch(batch);\nBox 2: OnlySnapshots -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/overview/azure/storage.blobs.batch-\nreadme https://stackoverflow.com/questions/39471212/programmatically-delete-azure-\nblob-storage-objects-in-bulks\nCommunity Discussion\nit is DeleteSnapshotsOption.OnlySnapshots but the explanation in the answer is bogus\nas usual. see https://learn.microsoft.com/en-us/dotnet/api/\nazure.storage.blobs.models.deletesnapshotsoption?view=azure-dotnet\nIt was there in 13 Feb 2023 exam\nCorrect!!\nGot this in exam today (5 April 2023)\nDeleteSnapshotsOption.OnlySnapshots",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "109",
    "question": "HOTSPOT -\nAn organization deploys a blob storage account. Users take multiple snapshots\nof the blob storage account over time.\nYou need to delete all snapshots of the blob storage account. You must not\ndelete the blob storage account itself.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: delete_snapshots -\n# Delete only the snapshot (blob itself is retained)\nblob_client.delete_blob(delete_snapshots=\"only\")\nBox 2: only -\nReference:\nhttps://github.com/Azure/azure-sdk-for-python/blob/main/sdk/storage/azure-storage-\nblob/samples/blob_samples_common.py\n\n\nCommunity Discussion\nSo, according to Microsoft a developer has to remember how the Microsoft guy chose to\nwrite in the .Net library and also how the python person thought and the unfortunate\ninconsistencies they built into these APIs?\nThe answer looks correct and the link proves it: Link to exact line with comment: https://\ngithub.com/Azure/azure-sdk-for-python/blob/main/sdk/storage/azure-storage-blob/\nsamples/blob_samples_common.py#L65\nWeird comments. When there is only 1 or 2 comments, everyone is saying secexams is\ncorrect...\nor you can retake the exam?\nor you can retake the exam?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "110",
    "question": "You are developing a Java application that uses Cassandra to store key and\nvalue data. You plan to use a new Azure Cosmos DB resource and the Cassandra\nAPI in the application. You create an Azure Active Directory (Azure AD) group\nnamed Cosmos DB Creators to enable provisioning of Azure Cosmos accounts,\ndatabases, and containers.\nThe Azure AD group must not be able to access the keys that are required to\naccess the data.\nYou need to restrict access to the Azure AD group.\nWhich role-based access control should you use?",
    "options": [
      {
        "letter": "A",
        "text": "DocumentDB Accounts Contributor",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Cosmos Backup Operator",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Cosmos DB Operator",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Cosmos DB Account Reader\nPage 256 of 1410\n257 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure Cosmos DB now provides a new RBAC role, Cosmos DB Operator. This new role lets\nyou provision Azure Cosmos accounts, databases, and containers, but can't access the\nkeys that are required to access the data. This role is intended for use in scenarios where\nthe ability to grant access to Azure Active Directory service principals to manage\ndeployment operations for Cosmos DB is needed, including the account, database, and\ncontainers.\nReference:\nhttps://azure.microsoft.com/en-us/updates/azure-cosmos-db-operator-role-for-role-\nbased-access-control-rbac-is-now-available/"
  },
  {
    "number": "111",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will\nauthenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website:\nadmin, normal, and reader. A user's Azure AD group membership must be used\nto determine the permission level.\nYou need to configure authorization.\nSolution: Configure the Azure Web App for the website to allow only\nauthenticated requests and require Azure AD log on.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead in the Azure AD application's manifest, set value of the groupMembershipClaims\noption to All.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-\naad-groups/"
  },
  {
    "number": "112",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will\nauthenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website:\nadmin, normal, and reader. A user's Azure AD group membership must be used\nto determine the permission level.\nYou need to configure authorization.\nSolution:\n✑ Create a new Azure AD application. In the application's manifest, set value of\nthe groupMembershipClaims option to All.\n✑ In the website, use the value of the groups claim from the JWT for the user to\ndetermine permissions.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 259 of 1410\n260 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "To configure Manifest to include Group Claims in Auth Token\n1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory,\nand go to App registrations to find your application:\n2. Click on your application (or search for it if you have a lot of apps) and edit the\nManifest by clicking on it.\n3. Locate the ג€groupMembershipClaimsג€ setting. Set its value to either\n:To help you decide which .€גAll€ג or €גSecurityGroup€ג\ngroups claim will contain the identifiers of all security groups of - €גSecurityGroup€ג  ✑\nwhich the user is a member.\ngroups claim will contain the identifiers of all security groups and all - €גAll€ג  ✑\ndistribution lists of which the user is a member\nNow your application will include group claims in your manifest and you can use this\nfact in your code.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-\naad-groups/"
  },
  {
    "number": "113",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will\nauthenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website:\nadmin, normal, and reader. A user's Azure AD group membership must be used\nto determine the permission level.\nYou need to configure authorization.\nSolution:\n✑ Create a new Azure AD application. In the application's manifest, define\napplication roles that match the required permission levels for the application.\n✑ Assign the appropriate Azure AD group to each role. In the website, use the\nvalue of the roles claim from the JWT for the user to determine permissions.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "To configure Manifest to include Group Claims in Auth Token\n1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory,\nand go to App registrations to find your application:\n2. Click on your application (or search for it if you have a lot of apps) and edit the\nManifest by clicking on it.\n3. Locate the ג€groupMembershipClaimsג€ setting. Set its value to either\n:To help you decide which .€גAll€ג or €גSecurityGroup€ג\ngroups claim will contain the identifiers of all security groups of - €גSecurityGroup€ג  ✑\nwhich the user is a member.\ngroups claim will contain the identifiers of all security groups and all - €גAll€ג  ✑\ndistribution lists of which the user is a member"
  },
  {
    "number": "114",
    "question": "DRAG DROP -\nYou are developing an application to securely transfer data between on-\npremises file systems and Azure Blob storage. The application stores keys,\nsecrets, and certificates in Azure Key Vault. The application uses the Azure Key\nVault APIs.\nThe application must allow recovery of an accidental deletion of the key vault\nor key vault objects. Key vault objects must be retained for 90 days after\ndeletion.\nYou need to protect the key vault and key vault objects.\nWhich Azure Key Vault feature should you use? To answer, drag the appropriate\nfeatures to the correct actions. Each feature may be used once, more than once,\nor not at all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: Soft delete -\nWhen soft-delete is enabled, resources marked as deleted resources are retained for a\nspecified period (90 days by default). The service further provides a mechanism for\nrecovering the deleted object, essentially undoing the deletion.\nBox 2: Purge protection -\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge\nprotection can only be enabled once soft-delete is enabled.\n\n\nWhen purge protection is on, a vault or an object in the deleted state cannot be purged\nuntil the retention period has passed. Soft-deleted vaults and objects can still be\nrecovered, ensuring that the retention policy will be followed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/soft-delete-overview\nCommunity Discussion\nThe answer is correct.\nBox 1: Soft delete When soft-delete is enabled, resources marked as deleted resources\nare retained for a specified period (90 days by default). The service further provides a\nmechanism for recovering the deleted object, essentially undoing the deletion. This can\nbe achieved with the help of the soft-delete feature of the key vault. Box 2: Purge\nprotection Purge protection is an optional Key Vault behavior and is not enabled by\ndefault. Purge protection can only be enabled once soft-delete is enabled. When purge\nprotection is on, a vault or an object in the deleted state cannot be purged until the\nretention period has passed. Soft-deleted vaults and objects can still be recovered,\nensuring that the retention policy will be followed. This can be achieved with the help of\nthe purge protection feature of the key vault. Reference: https://docs.microsoft.com/en-\nus/azure/key-vault/general/soft-delete-overview\nThank mlantonis, learn from you a lot ( AZ-104, AZ-204)\nThank mlantonis, learn from you a lot ( AZ-104, AZ-204)\not it on 03/2022, I chose Soft delete => Purge protection",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "115",
    "question": "You provide an Azure API Management managed web service to clients. The\nback-end web service implements HTTP Strict Transport Security (HSTS).\nEvery request to the backend service must include a valid HTTP authorization\nheader.\nYou need to configure the Azure API Management instance with an\nauthentication policy.\nWhich two policies can you use? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Basic Authentication",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Digest Authentication",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Certificate Authentication",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "OAuth Client Credential Grant",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D"
  },
  {
    "number": "116",
    "question": "DRAG DROP -\nYou are developing an ASP.NET Core website that can be used to manage\nphotographs which are stored in Azure Blob Storage containers.\nUsers of the website authenticate by using their Azure Active Directory (Azure\nAD) credentials.\nYou implement role-based access control (RBAC) role permissions on the\ncontainers that store photographs. You assign users to RBAC roles.\nYou need to configure the website's Azure AD Application so that user's\npermissions can be used with the Azure Blob containers.\nHow should you configure the application? To answer, drag the appropriate\nsetting to the correct location. Each setting can be used once, more than once,\nor not at all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: user_impersonation -\nBox 2: delegated -\nExample:\n1. Select the API permissions section\n2. Click the Add a permission button and then:\nEnsure that the My APIs tab is selected\n3. In the list of APIs, select the API TodoListService-aspnetcore.\n4. In the Delegated permissions section, ensure that the right permissions are checked:\nuser_impersonation.\n5. Select the Add permissions button.\nBox 3: delegated -\nExample -\n1. Select the API permissions section\n2. Click the Add a permission button and then,\nEnsure that the Microsoft APIs tab is selected\n3. In the Commonly used Microsoft APIs section, click on Microsoft Graph\n4. In the Delegated permissions section, ensure that the right permissions are checked:\nUser.Read. Use the search box if necessary.\n5. Select the Add permissions button\nReference:\nhttps://docs.microsoft.com/en-us/samples/azure-samples/active-directory-dotnet-\nwebapp-webapi-openidconnect-aspnetcore/calling-a-web-api-in-an-aspnet-core- web-\napplication-using-azure-ad/\nCommunity Discussion\nThe given answer is correct.\n\n\nBox 1: user_impersonation The built-in user_impersonation scope indicates that the\ntoken is being requested on behalf of the user. Azure Storage exposes a single delegation\nscope named user_impersonation that permits applications to take any action allowed\nby the user. Box 2: delegated Box 3: delegated Reference: https://stackoverflow.com/\nquestions/31404128/azure-ad-app-application-permissions-vs-delegated-permissions\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/v2-permissions-and-\nconsent https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-\napp?tabs=dotnet https://docs.microsoft.com/en-us/rest/api/storageservices/authorize-\nwith-azure-active-directory\ncorrect, in 2023Mar24, score: 904/1000\ncorrect, in 2023Mar24, score: 904/1000\nGot this in 09/22 , went with the most voted answers, score 927.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "117",
    "question": "HOTSPOT -\nYou are developing an ASP.NET Core app that includes feature flags which are\nmanaged by Azure App Configuration. You create an Azure App Configuration\nstore named AppFeatureFlagStore that contains a feature flag named Export.\nYou need to update the app to meet the following requirements:\n✑ Use the Export feature in the app without requiring a restart of the app.\n✑ Validate users before users are allowed access to secure resources.\n✑ Permit users to access secure resources.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: UseAuthentication -\nNeed to validate users before users are allowed access to secure resources.\nUseAuthentication adds the AuthenticationMiddleware to the specified\nIApplicationBuilder, which enables authentication capabilities.\nBox 2: UseAuthorization -\nNeed to permit users to access secure resources.\nUseAuthorization adds the AuthorizationMiddleware to the specified IApplicationBuilder,\nwhich enables authorization capabilities.\nBox 3: UseStaticFiles -\nNeed to use the Export feature in the app without requiring a restart of the app.\nUseStaticFiles enables static file serving for the current request path\nReference:\n\n\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.aspnetcore.builder.iapplicationbuilder?view=aspnetcore-5.0\nCommunity Discussion\nUseAuthentication, Use Authorization, UseAzureAppConfiguration. The last option allows\ndynamic configuration to be reloaded without an app restart. https://\ndocs.microsoft.com/en-us/azure/azure-app-configuration/enable-dynamic-\nconfiguration-aspnet-core?tabs=core5x\nBox 1: UseAuthentication Need to validate users before users are allowed access to\nsecure resources. UseAuthentication adds the AuthenticationMiddleware to the specified\nIApplicationBuilder, which enables authentication capabilities. To verify users, we need to\nuse the Authentication middleware. Box 2: UseAuthorization Need to permit users to\naccess secure resources. UseAuthorization adds the AuthorizationMiddleware to the\nspecified IApplicationBuilder, which enables authorization capabilities. To authorize\nusers to access resources , we need to use the Authorization middleware. Box 3:\nUseAzureAppConfiguration Adding the UseAzureAppConfiguration middleware to allow\nthe configuration settings registered for refresh to be updated while the ASP.NET Core\nweb app continues to receive requests. For using feature flags, you need to make use of\nthe Azure App Configuration service. To ensure the configuration settings are refreshed\nwithout the need to restart the web app, you can use the middleware of\nUseAzureAppConfiguration();\nOrder matters\nAccording to the site provided by you it should be 1. UseAzureAppConfiguration\n2.UseAuthentication 3. Use Authorization\nAccording to the site provided by you it should be 1. UseAzureAppConfiguration\n2.UseAuthentication 3. Use Authorization",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "118",
    "question": "You have an application that includes an Azure Web app and several Azure\nFunction apps. Application secrets including connection strings and certificates\nare stored in Azure Key Vault.\nSecrets must not be stored in the application or application runtime\nenvironment. Changes to Azure Active Directory (Azure AD) must be minimized.\nYou need to design the approach to loading application secrets.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a single user-assigned Managed Identity with permission to access Key Vault and\nconfigure each App Service to use that Managed Identity.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a single Azure AD Service Principal with permission to access Key Vault and use a\nclient secret from within the App Services to access Key Vault.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a system assigned Managed Identity in each App Service with permission to access\nKey Vault.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create an Azure AD Service Principal with Permissions to access Key Vault for each App\nService and use a certificate from within the App Services to access Key Vault.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Use Key Vault references for App Service and Azure Functions.\nKey Vault references currently only support system-assigned managed identities. User-\nassigned identities cannot be used.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references"
  },
  {
    "number": "119",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The\nwebsite is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the\ncontents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution:\n1. Create an Azure Key Vault key named skey.\n2. Encrypt the intake forms using the public key portion of skey.\n3. Store the encrypted data in Azure Blob storage.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 275 of 1410\n276 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "120",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The\nwebsite is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the\ncontents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution:\n1. Create an Azure Cosmos DB database with Storage Service Encryption\nenabled.\n2. Store the intake forms in the Azure Cosmos DB database.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead use an Azure Key vault and public key encryption. Store the encrypted from in\nAzure Storage Blob storage."
  },
  {
    "number": "121",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The\nwebsite is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the\ncontents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution: Store the intake forms as Azure Key Vault secrets.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "122",
    "question": "HOTSPOT -\nYou plan to deploy a new application to a Linux virtual machine (VM) that is\nhosted in Azure.\nThe entire VM must be secured at rest by using industry-standard encryption\ntechnology to address organizational security and compliance requirements.\nYou need to configure Azure Disk Encryption for the VM.\nHow should you complete the Azure CLI commands? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\n\n\n\n\nExplanation\nCorrect Answer:\n\n\nBox 1: keyvault -\n\n\nCreate an Azure Key Vault with az keyvault create and enable the Key Vault for use with\ndisk encryption. Specify a unique Key Vault name for keyvault_name as follows:\nkeyvault_name=myvaultname$RANDOM az keyvault create \\\n--name $keyvault_name \\\n--resource-group $resourcegroup \\\n--location eastus \\\n--enabled-for-disk-encryption True\nBox 2: keyvault key -\nThe Azure platform needs to be granted access to request the cryptographic keys when\nthe VM boots to decrypt the virtual disks. Create a cryptographic key in your Key Vault\nwith az keyvault key create. The following example creates a key named myKey: az\nkeyvault key create \\\n--vault-name $keyvault_name \\\n--name myKey \\\n--protection software\nBox 3: vm -\nCreate a VM with az vm create. Only certain marketplace images support disk encryption.\nThe following example creates a VM named myVM using an Ubuntu\n16.04 LTS image:\naz vm create \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--image Canonical:UbuntuServer:16.04-LTS:latest \\\n--admin-username azureuser \\\n--generate-ssh-keys \\\nBox 4: vm encryption -\nEncrypt your VM with az vm encryption enable:\naz vm encryption enable \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--disk-encryption-keyvault $keyvault_name \\\n--key-encryption-key myKey \\\n--volume-type all\nNote: seems to an error in the question. Should have enable instead of create.\nBox 5: all -\nEncrypt both data and operating system.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/disk-encryption-cli-\nquickstart\n\n\nCommunity Discussion\nThe answer is correct. Other possiblities doesn't make sense. Although it seems strange\nto me to create the VM when the question says that we already have one.\nOk, now I see that the question doesn't say that the VM is already created. Sorry for that\n:)\nOk, now I see that the question doesn't say that the VM is already created. Sorry for that\n:)\ncorrect\nWas on my exam today (03-01-2023)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "123",
    "question": "Your company is developing an Azure API hosted in Azure.\nYou need to implement authentication for the Azure API to access other Azure\nresources. You have the following requirements:\n✑ All API calls must be authenticated.\n✑ Callers to the API must not send credentials to the API.\nWhich authentication mechanism should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Basic",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Anonymous",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Managed identity",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Client certificate",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure Active Directory Managed Service Identity (MSI) gives your code an automatically\nmanaged identity for authenticating to Azure services, so that you can keep credentials\nout of your code."
  },
  {
    "number": "124",
    "question": "DRAG DROP -\nYou are developing an application. You have an Azure user account that has\naccess to two subscriptions.\nYou need to retrieve a storage account key secret from Azure Key Vault.\nIn which order should you arrange the PowerShell commands to develop the\nsolution? To answer, move all commands from the list of commands to the\nanswer area and arrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Get-AzSubscription -\nIf you have multiple subscriptions, you might have to specify the one that was used to\ncreate your key vault. Enter the following to see the subscriptions for your account:\nGet-AzSubscription -\nStep 2: Set-AzContext -SubscriptionId\nTo specify the subscription that's associated with the key vault you'll be logging, enter:\nSet-AzContext -SubscriptionId <subscriptionID>\nStep 3: Get-AzStorageAccountKey -\nYou must get that storage account key.\nStep 4: $secretvalue = ConvertTo-SecureString <storageAccountKey> -AsPlainText -Force\nSet-AzKeyVaultSecret -VaultName <vaultName> -Name <secretName> -SecretValue\n$secretvalue\nAfter retrieving your secret (in this case, your storage account key), you must convert that\nkey to a secure string, and then create a secret with that value in your key vault.\nStep 5: Get-AzKeyVaultSecret -\nNext, get the URI for the secret you created. You'll need this URI in a later step to call the\nkey vault and retrieve your secret. Run the following PowerShell command and make\nnote of the ID value, which is the secret's URI:\nGet-AzKeyVaultSecret ג€\"VaultName <vaultName>\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/Azure/key-vault/key-vault-key-rotation-log-\nmonitoring\n\n\nCommunity Discussion\nAnswer looks correct\nQuestion does not explain where the variables come from... Assuming I set the variables\nmyself (as a \"side effect\" of the command), the solution below does make sense: 1. Get-\nAzSubscription 2. Set-AzContext –SubscriptionId $subscriptionID 3. Get-AzKeyVaultSecret\n–VaultName $vaultName 4. Get-AzStorageAccountKey –ResourceGroupName $resGroup –\nName $storAcct 5. $secretvalue = ConvertTo-SecureString $storAcctkey –AsPlainText –\nForce Set-AzKeyVaultSecret –VaultName $vaultName –Name $secretName –SecretValue\n$secretvalue\nI believe the assignment wants to select a proper subscription, then to retrieve the\nstorage account key, then to store this key into the KeyVault and finally to check the\nsecret was inserted properly. So: 1. Get-AzSubscription ($subscriptionID =\n(...).SubscriptionName) 2. Set-AzContext 3. Get-AzStorageAccountKey ($storAcctkey = \"(...)\n[0].Value\") 4. ConvertTo-SecureString, Set-AzKeyVaultSecret 5. Get-AzKeyVaultSecret (list\nthe secrets stored in the KeyVault - check only)\nDoes the question miss some part? Why it has to save the key before retrieval? No where\nin the question mentioned that ... First two Get-AzSubscription Set-AzContext –\nSubscriptionId $subscriptionID are absolutely correct, since there are two subscriptions,\nyou have to point to the correct one ... but I am confused with 3, 4, 5 steps, what are\nthose?\nAgree with some people saying the question and picture does not make sense. The\nquestion only says how to retrieve the secret from Key Vault. Based on the options, we\nalso have to retrieve the value and insert it to Key Vault first. The question/picture lacks\ninformation based on what we have.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "125",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in\nAzure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Use an X.509 certificate to authenticate the VM with Azure Resource\nManager.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead run the Invoke-RestMethod cmdlet to make a request to the local managed\nidentity for Azure resources endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-\nresources/tutorial-windows-vm-access-arm"
  },
  {
    "number": "126",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in\nAzure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Use the Reader role-based access control (RBAC) role to authenticate\nthe VM with Azure Resource Manager.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "127",
    "question": "HOTSPOT -\nYou are building a website that is used to review restaurants. The website will\nuse an Azure CDN to improve performance and add functionality to requests.\nYou build and deploy a mobile app for Apple iPhones. Whenever a user accesses\nthe website from an iPhone, the user must be redirected to the app store.\nYou need to implement an Azure CDN rule that ensures that iPhone users are\nredirected to the app store.\nHow should you complete the Azure Resource Manager template? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: iOS -\nAzure AD Conditional Access supports the following device platforms:\n✑ Android\n✑ iOS\n✑ Windows Phone\n✑ Windows\nmacOS\nBox 2: DeliveryRuleIsDeviceConditionParameters\nThe DeliveryRuleIsDeviceCondition defines the IsDevice condition for the delivery rule.\nparameters defines the parameters for the condition.\nBox 3: HTTP_USER_AGENT -\nIncorrect Answers:\n✑ The Pragma HTTP/1.0 general header is an implementation-specific header that may\nhave various effects along the request-response chain. It is used for backwards\ncompatibility with HTTP/1.0 caches.\n✑ \"X-Powered-By\" is a common non-standard HTTP response header (most headers\nprefixed with an 'X-' are non-standard).\nBox 4: DeliveryRuleRequestHeaderConditionParameters\nDeliveryRuleRequestHeaderCondition defines the RequestHeader condition for the\ndelivery rule. parameters defines the parameters for the condition.\nBox 5: iOS -\nThe Require approved client app requirement only supports the iOS and Android for\n\n\ndevice platform condition.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/concept-\nconditional-access-conditions https://docs.microsoft.com/en-us/azure/active-directory/\nconditional-access/concept-conditional-access-grant\nCommunity Discussion\nFirst box: Mobile https://docs.microsoft.com/en-us/python/api/azure-mgmt-cdn/\nazure.mgmt.cdn.models.isdevicematchconditionparametersmatchvaluesitem Third box:\niPhone. We are not looking for other Apple devices running iOS. It's the iPhone we're\nlooking for.\n1 DeliveryRuleDeviceConditionParameters - we are first checking for a device condition,\nhence we need to use the condition of DeliveryRuleDeviceConditionParameters 2.Mobile\n- The devices can be either Desktop or Mobile. These are the two accepted values. Here\nsince we need to route requests based on mobile devices, we need to choose the value\nof Mobile. 3.DeliveryRequestHeaderConditionParameters. we need to understand the type\nof operating system running on the device. We can get this information from the request\nheaders. Hence, we need to use the parameter of\nDeliveryRequestHeaderConditionParameters. 4.HTTP_USER_AGENT - we can check the\nHTTP_USER_AGENT property in the request header. In the user agent property of the\nrequest header, you will normally get information about the environment where the\nrequest is originating from. An example is given below where I am showing the request\nheader from my own machine when I browse to a site. 5.iOS - we need to check the\noperating system which will be iOS.\nhttps://deviceatlas.com/blog/list-of-user-agent-strings#iphone For 5 it has to be iPhone\n(this is not something Microsoft we are matching, but the details pulled from the device).\nSo I get: 1. DeliveryRuleDeviceConditionParameters 2. Mobile (as below: https://\ndocs.microsoft.com/en-us/python/api/azure-mgmt-cdn/\nazure.mgmt.cdn.models.isdevicematchconditionparametersmatchvaluesitem?view=azure-\npython) 3.DeliveryRequestHeaderConditionParameters 4. HTTP_USER_AGENT 5. iPhone\nhttps://deviceatlas.com/blog/list-of-user-agent-strings#iphone For 5 it has to be iPhone\n(this is not something Microsoft we are matching, but the details pulled from the device).\nSo I get: 1. DeliveryRuleDeviceConditionParameters 2. Mobile (as below: https://\ndocs.microsoft.com/en-us/python/api/azure-mgmt-cdn/\nazure.mgmt.cdn.models.isdevicematchconditionparametersmatchvaluesitem?view=azure-\npython) 3.DeliveryRequestHeaderConditionParameters 4. HTTP_USER_AGENT 5. iPhone\n\n\nI agree on the first four points, but the fifth point is incorrect. The correct answer is\niPhone. Here you can see lots of iOS User Agent examples: https://\ndevelopers.whatismybrowser.com/useragents/explore/operating_system_name/ios/\n\"iPhone\" appears all the time and \"Mac OS X\" is used instead of \"iOS\" Example: Mozilla/\n5.0 (iPhone; CPU iPhone OS 9_3_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko)\nVersion/9.0 Mobile/13E238 Safari/601.1",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "128",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will\nauthenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website:\nadmin, normal, and reader. A user's Azure AD group membership must be used\nto determine the permission level.\nYou need to configure authorization.\nSolution:\n✑ Configure and use Integrated Windows Authentication in the website.\n✑ In the website, query Microsoft Graph API to load the groups to which the\nuser is a member.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Microsoft Graph is a RESTful web API that enables you to access Microsoft Cloud service\nresources.\nInstead in the Azure AD application's manifest, set value of the groupMembershipClaims\noption to All. In the website, use the value of the groups claim from the"
  },
  {
    "number": "129",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in\nAzure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Run the Invoke-RestMethod cmdlet to make a request to the local\nmanaged identity for Azure resources endpoint.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Get an access token using the VM's system-assigned managed identity and use it to call\nAzure Resource Manager\nYou will need to use PowerShell in this portion.\n1. In the portal, navigate to Virtual Machines and go to your Windows virtual machine and\nin the Overview, click Connect.\n2. Enter in your Username and Password for which you added when you created the\nWindows VM.\n3. Now that you have created a Remote Desktop Connection with the virtual machine,\nopen PowerShell in the remote session.\n4. Using the Invoke-WebRequest cmdlet, make a request to the local managed identity\nfor Azure resources endpoint to get an access token for Azure Resource\nManager.\nExample:\n$response = Invoke-WebRequest -Uri 'http://169.254.169.254/metadata/identity/oauth2/\ntoken?api-version=2018-02-01&resource=https:// management.azure.com/' -Method GET -\nHeaders @{Metadata=\"true\"}\nReference:"
  },
  {
    "number": "130",
    "question": "HOTSPOT -\nYou are building a website to access project data related to teams within your\norganization. The website does not allow anonymous access. Authentication is\nperformed using an Azure Active Directory (Azure AD) app named internal.\nThe website has the following authentication requirements:\n✑ Azure AD users must be able to login to the website.\n✑ Personalization of the website must be based on membership in Active\nDirectory groups.\nYou need to configure the application's manifest to meet the authentication\nrequirements.\nHow should you configure the manifest? To answer, select the appropriate\nconfiguration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: groupMembershipClaims -\nScenario: Personalization of the website must be based on membership in Active\nDirectory groups.\nGroup claims can also be configured in the Optional Claims section of the Application\nManifest.\nEnable group membership claims by changing the groupMembershipClaim\nThe valid values are:\n\"All\"\n\"SecurityGroup\"\n\"DistributionList\"\n\"DirectoryRole\"\nBox 2: oauth2Permissions -\nScenario: Azure AD users must be able to login to the website. oauth2Permissions\nspecifies the collection of OAuth 2.0 permission scopes that the web API (resource) app\nexposes to client apps. These permission scopes may be granted to client apps during\nconsent.\nIncorrect Answers:\noauth2AllowImplicitFlow. oauth2AllowImplicitFlow specifies whether this web app can\nrequest OAuth2.0 implicit flow access tokens. The default is false. This flag is used for\nbrowser-based apps, like Javascript single-page apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/hybrid/how-to-connect-fed-\ngroup-claims\n\n\nCommunity Discussion\nSecond answer is incorrect. Well, oauth2Permissions can only accept collections value\nlike an array not a boolean. It should be oauth2AllowImplicitFlow.\nBox 1: groupMembershipClaims Personalization of the website must be based on\nmembership in Active Directory groups. Group claims can also be configured in the\nOptional Claims section of the Application Manifest. Enable group membership claims by\nchanging the groupMembershipClaim The valid values are: - \"All\" - \"SecurityGroup\" -\n\"DistributionList\" - \"DirectoryRole\" Here we need to mention that we want to get the\ngroups for the users. Hence we need to mention to set the groupMembershipClaims\nproperty to All. Box 2: oauth2AllowImplicitFlow Azure AD users must be able to login to\nthe website. auth2Permissions can only accept collections value like an array, not a\nboolean. oauth2AllowImplicitFlow accepts boolean value. Here from the list of options\ngiven, if we want the application to fetch the required tokens , we would need to allow\nImplicit Flow.\nI Agree with you: 1) groupMembershipClaims 2) oauth2AllowImplicitFlow\nI Agree with you: 1) groupMembershipClaims 2) oauth2AllowImplicitFlow\nAgree. oauth2permissions: https://docs.microsoft.com/en-us/azure/active-directory/\ndevelop/reference-app-manifest#oauth2permissions-attribute oauth2allowimplicitflow:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-\nmanifest#oauth2allowimplicitflow-attribute",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "131",
    "question": "You develop an app that allows users to upload photos and videos to Azure\nstorage. The app uses a storage REST API call to upload the media to a blob\nstorage account named Account1. You have blob storage containers named\nContainer1 and Container2.\nUploading of videos occurs on an irregular basis.\nYou need to copy specific blobs from Container1 to Container2 when a new\nvideo is uploaded.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Copy blobs to Container2 by using the Put Blob operation of the Blob Service REST API",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an Event Grid topic that uses the Start-AzureStorageBlobCopy cmdlet (Correct\nPage 304 of 1410\n305 Microsoft - AZ-204 Practice Questions - SecExams.com\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use AzCopy with the Snapshot switch to copy blobs to Container2",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Download the blob to a virtual machine and then upload the blob to Container2",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The Start-AzureStorageBlobCopy cmdlet starts to copy a blob.\nExample 1: Copy a named blob -\nC:\\PS>Start-AzureStorageBlobCopy -SrcBlob \"ContosoPlanning2015\" -DestContainer\n\"ContosoArchives\" -SrcContainer \"ContosoUploads\"\nThis command starts the copy operation of the blob named ContosoPlanning2015 from\nthe container named ContosoUploads to the container named\nContosoArchives.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/azure.storage/start-\nazurestorageblobcopy?view=azurermps-6.13.0"
  },
  {
    "number": "132",
    "question": "You are developing an ASP.NET Core website that uses Azure FrontDoor. The\nwebsite is used to build custom weather data sets for researchers. Data sets are\ndownloaded by users as Comma Separated Value (CSV) files. The data is\nrefreshed every 10 hours.\nSpecific files must be purged from the FrontDoor cache based upon Response\nHeader values.\nYou need to purge individual assets from the Front Door cache.\nWhich type of cache purge should you use?",
    "options": [
      {
        "letter": "A",
        "text": "single path",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "wildcard",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "root domain",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "These formats are supported in the lists of paths to purge:\n✑ Single path purge: Purge individual assets by specifying the full path of the asset\n(without the protocol and domain), with the file extension, for example, /\n[1]\n✑ Wildcard purge: Asterisk (*) may be used as a wildcard. Purge all folders, subfolders,\nand files under an endpoint with /* in the path or purge all subfolders and files under a\nspecific folder by specifying the folder followed by /*, for example, /pictures/*.\n✑ Root domain purge: Purge the root of the endpoint with \"/\" in the path.\nReference:\nhttps://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching"
  },
  {
    "number": "133",
    "question": "Your company is developing an Azure API.\nYou need to implement authentication for the Azure API. You have the following\nrequirements:\nAll API calls must be secure.\n✑ Callers to the API must not send credentials to the API.\nWhich authentication mechanism should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Basic",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Anonymous",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Managed identity",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Client certificate",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "tokens. https://docs.microsoft.com/en-us/azure/\nactive-directory/managed-identities-azure-resources/overview\nit says : You need to implement authentication for the Azure API. So the Azure API is the\none that needs to have an authenticated access to the backend services. That means\nManaged Identity.\nThose are NOT managed identity, those are claim based authentication.\nThose are NOT managed identity, those are claim based authentication.\nPage 308 of 1410\n309 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "134",
    "question": "You are a developer for a SaaS company that offers many web services.\nAll web services for the company must meet the following requirements:\n✑ Use API Management to access the services\n✑ Use OpenID Connect for authentication\n✑ Prevent anonymous usage\nA recent security audit found that several web services can be called without\nany authentication.\nWhich API Management policy should you implement?",
    "options": [
      {
        "letter": "A",
        "text": "jsonp",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "authentication-certificate",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "check-header",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "validate-jwt",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Add the validate-jwt policy to validate the OAuth token for every incoming request.\nIncorrect Answers:\nA: The jsonp policy adds JSON with padding (JSONP) support to an operation or an API to\nallow cross-domain calls from JavaScript browser-based clients.\nJSONP is a method used in JavaScript programs to request data from a server in a\ndifferent domain. JSONP bypasses the limitation enforced by most web browsers where\naccess to web pages must be in the same domain.\nJSONP - Adds JSON with padding (JSONP) support to an operation or an API to allow\ncross-domain calls from JavaScript browser-based clients.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-\nprotect-backend-with-aad"
  },
  {
    "number": "135",
    "question": "DRAG DROP -\nContoso, Ltd. provides an API to customers by using Azure API Management\n(APIM). The API authorizes users with a JWT token.\nYou must implement response caching for the APIM gateway. The caching\nmechanism must detect the user ID of the client that accesses data for a given\nlocation and cache the response for that user ID.\nYou need to add the following policies to the policies file:\n✑ a set-variable policy to store the detected user identity\n✑ a cache-lookup-value policy\n✑ a cache-store-value policy\n✑ a find-and-replace policy to update the response body with the user profile\ninformation\nTo which policy section should you add the policies? To answer, drag the\nappropriate sections to the correct policies. Each section may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: Inbound.\nA set-variable policy to store the detected user identity.\nExample:\n<policies>\n<inbound>\n<!-- How you determine user identity is application dependent -->\n<set-variable\nname=\"enduserid\"\nvalue=\"@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\").Split(' ')\n[1].AsJwt()?.Subject)\" />\nBox 2: Inbound -\nA cache-lookup-value policy -\nExample:\n<inbound>\n<base />\n<cache-lookup vary-by-developer=\"true | false\" vary-by-developer-groups=\"true | false\"\ndownstream-caching-type=\"none | private | public\" must- revalidate=\"true | false\">\n<vary-by-query-parameter>parameter name</vary-by-query-parameter> <!-- optional,\ncan repeated several times -->\n</cache-lookup>\n</inbound>\n\n\nBox 3: Outbound -\nA cache-store-value policy.\nExample:\n<outbound>\n<base />\n<cache-store duration=\"3600\" />\n</outbound>\nBox 4: Outbound -\nA find-and-replace policy to update the response body with the user profile information.\nExample:\n<outbound>\n<!-- Update response body with user profile-->\n<find-and-replace\nfrom='\"$userprofile$\"'\nto=\"@((string)context.Variables[\"userprofile\"])\" />\n<base />\n</outbound>\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-\npolicies https://docs.microsoft.com/en-us/azure/api-management/api-management-\nsample-cache-by-key\nCommunity Discussion\nInbound Inbound Inbound Outbound\nThe answer provided is correct.\nBest score ever, compliments.\nlooks correct, in Nov182023, scored 962\ncorrect! -> https://docs.microsoft.com/en-us/azure/api-management/api-management-\nsample-cache-by-key#fragment-caching",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "136",
    "question": "DRAG DROP -\nYou are developing an Azure solution.\nYou need to develop code to access a secret stored in Azure Key Vault.\nHow should you complete the code segment? To answer, drag the appropriate\ncode segments to the correct location. Each code segment may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: SecretClient -\nBox 2: DefaultAzureCredential -\nIn below example, the name of your key vault is expanded to the key vault URI, in the\nformat \"https://<your-key-vault-name>.vault.azure.net\". This example is using\n'DefaultAzureCredential()' class from Azure Identity Library, which allows to use the same\ncode across different environments with different options to provide identity. string\nkeyVaultName = Environment.GetEnvironmentVariable(\"KEY_VAULT_NAME\"); var kvUri =\n\"https://\" + keyVaultName + \".vault.azure.net\"; var client = new SecretClient(new Uri(kvUri),\nnew DefaultAzureCredential());\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-net\nCommunity Discussion\nAnswer is correct. string keyVaultName =\nEnvironment.GetEnvironmentVariable(\"KEY_VAULT_NAME\"); var kvUri = \"https://\" +\nkeyVaultName + \".vault.azure.net\"; var client = new SecretClient(new Uri(kvUri), new\nDefaultAzureCredential()); Box 1: SecretClient Box 2: DefaultAzureCredential Reference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-net\nGot this one 02/2022. Went with most voted\nDefaultAzureCredential DefaultAzureCredential is appropriate for most scenarios where\nthe application is intended to run in the Azure Cloud. This is because the\nDefaultAzureCredential determines the appropriate credential type based of the\nenvironment it is executing in. It supports authenticating both as a service principal or\nmanaged identity, and can be configured so that it will work both in a local development\nenvironment or when deployed to the cloud. The DefaultAzureCredential will first\nattempt to authenticate using credentials provided in the environment. In a development\nenvironment you can authenticate as a service principal with the DefaultAzureCredential\nby providing configuration in environment variables as described in the next section. If\nthe environment configuration is not present or incomplete, the DefaultAzureCredential\nwill then determine if a managed identity is available in the current environment.\nAuthenticating as a managed identity requires no configuration, but does require\nplatform support. See the managed identity documentation for more details on this.\nCorrect\ngot it on my exam 30-12-2022 score: 818",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "137",
    "question": "You are developing an Azure App Service REST API.\nThe API must be called by an Azure App Service web app. The API must retrieve\nand update user profile information stored in Azure Active Directory (Azure AD).\nYou need to configure the API to make the updates.\nWhich two tools should you use? Each correct answer presents part of the\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Microsoft Graph API",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Microsoft Authentication Library (MSAL)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure API Management",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Microsoft Azure Security Center\nE) Microsoft Azure Key Vault SDK",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C\nA: You can use the Azure AD REST APIs in Microsoft Graph to create unique workflows\nbetween Azure AD resources and third-party services.\nEnterprise developers use Microsoft Graph to integrate Azure AD identity management\nand other services to automate administrative workflows, such as employee onboarding\n(and termination), profile maintenance, license deployment, and more.\nC: API Management (APIM) is a way to create consistent and modern API gateways for\nexisting back-end services.\nAPI Management helps organizations publish APIs to external, partner, and internal\ndevelopers to unlock the potential of their data and services.\nReference:\nhttps://docs.microsoft.com/en-us/graph/azuread-identity-access-management-concept-\noverview"
  },
  {
    "number": "138",
    "question": "You develop a REST API. You implement a user delegation SAS token to\ncommunicate with Azure Blob storage.\nThe token is compromised.\nYou need to revoke the token.\nWhat are two possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Revoke the delegation key.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Delete the stored access policy.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Regenerate the account key.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Remove the role assignment for the security principle.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B\nA: Revoke a user delegation SAS -\nTo revoke a user delegation SAS from the Azure CLI, call the az storage account revoke-\ndelegation-keys command. This command revokes all of the user delegation keys\nassociated with the specified storage account. Any shared access signatures associated\nwith those keys are invalidated.\nB: To revoke a stored access policy, you can either delete it, or rename it by changing the"
  },
  {
    "number": "139",
    "question": "DRAG DROP -\nYou are developing an Azure-hosted application that must use an on-premises\nhardware security module (HSM) key.\nThe key must be transferred to your existing Azure Key Vault by using the Bring\nYour Own Key (BYOK) process.\nYou need to securely transfer the key to Azure Key Vault.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nTo perform a key transfer, a user performs following steps:\n✑ Generate KEK.\n✑ Retrieve the public key of the KEK.\n✑ Using HSM vendor provided BYOK tool - Import the KEK into the target HSM and\nexports the Target Key protected by the KEK.\n✑ Import the protected Target Key to Azure Key Vault.\nStep 1: Generate a Key Exchange Key (KEK).\nStep 2: Retrieve the Key Exchange Key (KEK) public key.\n\n\nStep 3: Generate a key transfer blob file by using the HSM vendor-provided tool.\nGenerate key transfer blob using HSM vendor provided BYOK tool\nStep 4: Run the az keyvault key import command\nUpload key transfer blob to import HSM-key.\nCustomer will transfer the Key Transfer Blob (\".byok\" file) to an online workstation and\nthen run a az keyvault key import command to import this blob as a new\nHSM-backed key into Key Vault.\nTo import an RSA key use this command:\naz keyvault key import\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/keys/byok-specification\nCommunity Discussion\ncorrect: https://docs.microsoft.com/en-us/azure/key-vault/keys/byok-\nspecification#user-steps\nGot this on the exam. :) top kek\nthis is still showing up in feb 2022 exams, the kek definitely made this question\nunforgettable for me top kek\nthis is still showing up in feb 2022 exams, the kek definitely made this question\nunforgettable for me top kek\nis it enough to pass the exam to read up to 22 pages",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "140",
    "question": "You develop and deploy an Azure Logic app that calls an Azure Function app.\nThe Azure Function app includes an OpenAPI (Swagger) definition and uses an\nAzure Blob storage account. All resources are secured by using Azure Active\nDirectory (Azure AD).\nThe Azure Logic app must securely access the Azure Blob storage account. Azure\nAD resources must remain if the Azure Logic app is deleted.\nYou need to secure the Azure Logic app.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a user-assigned managed identity and assign role-based access controls. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an Azure AD custom role and assign the role to the Azure Blob storage account.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure Key Vault and issue a client certificate.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a system-assigned managed identity and issue a client certificate.\nE) Create an Azure AD custom role and assign role-based access controls.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "without having to sign in and provide credentials or secrets, your\nlogic app can use a managed identity (formerly known as Managed Service Identity or\nMSI). Azure manages this identity for you and helps secure your credentials because you\ndon't have to provide or rotate secrets.\nIf you set up your logic app to use the system-assigned identity or a manually created,\nuser-assigned identity, the function in your logic app can also use that same identity for\nauthentication.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-\nmutual-certificates-for-clients",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "To give a managed identity access to an Azure resource, you need to add a role to the\ntarget resource for that identity.\nNote: To easily authenticate access to other resources that are protected by Azure Active\nDirectory (Azure AD) without having to sign in and provide credentials or secrets, your\nlogic app can use a managed identity (formerly known as Managed Service Identity or\nMSI). Azure manages this identity for you and helps secure your credentials because you\ndon't have to provide or rotate secrets.\nIf you set up your logic app to use the system-assigned identity or a manually created,\nuser-assigned identity, the function in your logic app can also use that same identity for\nauthentication.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-\nmutual-certificates-for-clients"
  },
  {
    "number": "141",
    "question": "HOTSPOT -\nYou are developing an application that uses a premium block blob storage\naccount. You are optimizing costs by automating Azure Blob Storage access\ntiers.\nYou apply the following policy rules to the storage account. You must determine\nthe implications of applying the rules to the data. (Line numbers are included\nfor reference only.)\n\n\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Yes -\nBox 2: Yes -\n\n\nBox 3: Yes -\nBox 4: Yes -\nCommunity Discussion\nCorrect answer: 1. Yes 2. Yes 3. Yes 4. No\nAnswer is correct 1. Yes 2. Yes 3. Yes 4. Yes\nwrong - 4 is No. If a block has recently been modified then it will not be deleted despite\nbeing older than 730 days.\nwrong - 4 is No. If a block has recently been modified then it will not be deleted despite\nbeing older than 730 days.\nAgree, 4 should be no. Instead of \"All block blobs older than 730 days will be deleted\", it\nshould be: \"all block blobs 730 days after last modified\". All info can be found at: https://\ndocs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview?\ntabs=azure-portal#move-aging-data-to-a-cooler-tier",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "142",
    "question": "You are developing a solution that will use a multi-partitioned Azure Cosmos DB\ndatabase. You plan to use the latest Azure Cosmos DB SDK for development.\nThe solution must meet the following requirements:\n✑ Send insert and update operations to an Azure Blob storage account.\n✑ Process changes to all partitions immediately.\n✑ Allow parallelization of change processing.\nYou need to process the Azure Cosmos DB operations.\nWhat are two possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create an Azure App Service API and implement the change feed estimator of the SDK.\nScale the API by using multiple Azure App Service instances.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create a background job in an Azure Kubernetes Service and implement the change feed\nfeature of the SDK.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure Function to use a trigger for Azure Cosmos DB. Configure the trigger to\nconnect to the container.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create an Azure Function that uses a FeedIterator object that processes the change feed by\nusing the pull model on the container. Use a FeedRange object to parallelize the processing of\nthe change feed across multiple functions.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C\nAzure Functions is the simplest option if you are just getting started using the change\nfeed. Due to its simplicity, it is also the recommended option for most change feed use\ncases. When you create an Azure Functions trigger for Azure Cosmos DB, you select the\ncontainer to connect, and the Azure Function gets triggered whenever there is a change\nin the container. Because Azure Functions uses the change feed processor behind the\nscenes, it automatically parallelizes change processing across your container's partitions.\nNote: You can work with change feed using the following options:\n✑ Using change feed with Azure Functions\n✑ Using change feed with change feed processor\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/read-change-feed"
  },
  {
    "number": "143",
    "question": "HOTSPOT -\nYou have an Azure Web app that uses Cosmos DB as a data store. You create a\nCosmosDB container by running the following PowerShell script:\n$resourceGroupName = \"testResourceGroup\"\n$accountName = \"testCosmosAccount\"\n$databaseName = \"testDatabase\"\n$containerName = \"testContainer\"\n$partitionKeyPath = \"/EmployeeId\"\n$autoscaleMaxThroughput = 5000\nNew-AzCosmosDBSqlContainer -\n-ResourceGroupName $resourceGroupName\n-AccountName $accountName\n-DatabaseName $databaseName\n-Name $containerName\n-PartitionKeyKind Hash\n-PartitionKeyPath $partitionKeyPath\n-AutoscaleMaxThroughput $autoscaleMaxThroughput\nYou create the following queries that target the container:\nSELECT * FROM c WHERE c.EmployeeId > '12345'\nSELECT * FROM c WHERE c.UserID = '12345'\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nYou set the highest, or maximum RU/s Tmax you don't want the system to exceed. The\nsystem automatically scales the throughput T such that 0.1* Tmax <= T <=\nTmax.\nIn this example we have autoscaleMaxThroughput = 5000, so the minimum throughput\nfor the container is 500 R/Us.\nBox 2: No -\nFirst query: SELECT * FROM c WHERE c.EmployeeId > '12345'\nHere's a query that has a range filter on the partition key and won't be scoped to a single\nphysical partition. In order to be an in-partition query, the query must have an equality\nfilter that includes the partition key:\nSELECT * FROM c WHERE c.DeviceId > 'XMS-0001'\nBox 3: Yes -\nExample of In-partition query:\nConsider the below query with an equality filter on DeviceId. If we run this query on a\ncontainer partitioned on DeviceId, this query will filter to a single physical partition.\nSELECT * FROM c WHERE c.DeviceId = 'XMS-0001'\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-choose-offer https://\ndocs.microsoft.com/en-us/azure/cosmos-db/how-to-query-container\nCommunity Discussion\ncorrect: 1° no => line 6 2° no => https://docs.microsoft.com/en-us/azure/cosmos-db/\nhow-to-choose-offer#overview-of-provisioned-throughput-types https://\ndocs.microsoft.com/en-us/azure/cosmos-db/how-to-query-container#in-partition-query\n\"Here's a query that has a range filter on the partition key and won't be scoped to a\nsingle physical partition. In order to be an in-partition query, the query must have an\nequality filter that includes the partition key: SELECT * FROM c WHERE c.DeviceId >\n\n\n'XMS-0001'\" 3° : yes => partition key is EmployeeId. https://docs.microsoft.com/en-us/\nazure/cosmos-db/how-to-query-container#cross-partition-query\n1. No : Because as question specifies max RUs = 5000, so minimum RUs = 5000/10 = 500\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-choose-offer#overview-of-\nprovisioned-throughput-types 2. No : In-partition query needs equality filter that is\nmissing here 3. Yes: Though we have equality filter here, 'UserId' is not a partition key\nhere. https://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-query-container\nThe documentation on the second point could be better. They put the most important\npart at the end and that is that even if the query uses the partition key it MUST use an\nequality filter to be considered in-partition.\nThe documentation on the second point could be better. They put the most important\npart at the end and that is that even if the query uses the partition key it MUST use an\nequality filter to be considered in-partition.\nGot this in exam today (5 April 2023)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "144",
    "question": "HOTSPOT -\nYou are developing a web application that makes calls to the Microsoft Graph\nAPI. You register the application in the Azure portal and upload a valid X509\ncertificate.\nYou create an appsettings.json file containing the certificate name, client\nidentifier for the application, and the tenant identifier of the Azure Active\nDirectory (Azure\nAD). You create a method named ReadCertificate to return the X509 certificate\nby name.\nYou need to implement code that acquires a token by using the certificate.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: ConfidentialClientApplicationBuilder\nHere's the code to instantiate the confidential client application with a client secret: app\n= ConfidentialClientApplicationBuilder.Create(config.ClientId)\n.WithClientSecret(config.ClientSecret)\n.WithAuthority(new Uri(config.Authority))\n.Build();\nBox 2: scopes -\nAfter you've constructed a confidential client application, you can acquire a token for the\napp by calling AcquireTokenForClient, passing the scope, and optionally forcing a refresh\nof the token.\nSample code: result = await app.AcquireTokenForClient(scopes)\n.ExecuteAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-daemon-\napp-configuration https://docs.microsoft.com/en-us/azure/active-directory/develop/\nscenario-daemon-acquire-token\nCommunity Discussion\ncorrect https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-\ndaemon-app-configuration?tabs=dotnet#instantiate-the-confidential-client-application-\nwith-a-client-certificate https://docs.microsoft.com/en-us/azure/active-directory/\ndevelop/scenario-daemon-acquire-token?tabs=dotnet#acquiretokenforclient-api\nProvided answer is correct.\nGot this one 01/2022. Went with most voted (to avoid writing answers again)\ngot this question on 06-29-2023\n\n\nDid my exam on 15th November 2022. This question was on it.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "145",
    "question": "HOTSPOT -\nYou develop a containerized application. You plan to deploy the application to a\nnew Azure Container instance by using a third-party continuous integration and\ncontinuous delivery (CI/CD) utility.\nThe deployment must be unattended and include all application assets. The\nthird-party utility must only be able to push and pull images from the registry.\nThe authentication must be managed by Azure Active Directory (Azure AD). The\nsolution must use the principle of least privilege.\nYou need to ensure that the third-party utility can access the registry.\nWhich authentication options should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Service principal -\nApplications and container orchestrators can perform unattended, or \"headless,\"\nauthentication by using an Azure Active Directory (Azure AD) service principal.\nIncorrect Answers:\n✑ Individual AD identity does not support unattended push/pull\n✑ Repository-scoped access token is not integrated with AD identity\n✑ Managed identity for Azure resources is used to authenticate to an Azure container\nregistry from another Azure resource.\nBox 2: AcrPush -\nAcrPush provides pull/push permissions only and meets the principle of least privilege.\nIncorrect Answers:\nAcrPull only allows pull permissions it does not allow push permissions.\n✑ Owner and Contributor allow pull/push permissions but does not meet the principle\nof least privilege.\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-\nauthentication?tabs=azure-cli https://docs.microsoft.com/en-us/azure/container-\nregistry/container-registry-roles?tabs=azure-cli\nCommunity Discussion\nGot this 4/29. I passed test, but scored poorly in third party integration questions. I went\nwith Managed Identity and Contributor, and I think both are WRONG. I had not heard of\nACR Push/Pull, so went with Contributor, but upon learning that ACR Push satisfies least\nprivilege, that is probably the best answer. Table of privileges: https://\n\n\ndocs.microsoft.com/en-us/azure/container-registry/container-registry-roles?tabs=azure-\ncli I suggest ACR Push is the correct permission level.\nCorrect. It is only service principal. acr-token: cannot be because the AD authentication\nrequirement. The roles are RBAC but the identity is not AD managed Managed identity:\nNot an option because it is third party AcrPush includes AcrPull. I did not know that\nnuance and the choice made by Microsoft to do this\nGot this on 16/03/23. Chosen Service Principal and AcrPush. Make sure to prepare for\ncase study. I got city and lights case study. No Kubernetes, Search, Logic Apps questions\nfor me.\nIndividual identity - no, you don't want to use somebody's username, when he left,\nservice will stop working (saw this tens of times). Managed identity - yes that could work,\nbut you're not sure if CI/CD is running on Azure resource. Repository scoped access token\n- the next question would be regarding Scope Map, but they are apparently asking about\nRBAC role. So the correct answer is Service Principal with AcrPush role, which will meet\nthe least priviledge requirement.\non exam 02/23",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "146",
    "question": "You deploy an Azure App Service web app. You create an app registration for the\napp in Azure Active Directory (Azure AD) and Twitter.\nThe app must authenticate users and must use SSL for all communications. The\napp must use Twitter as the identity provider.\nYou need to validate the Azure AD request in the app code.\nWhat should you validate?",
    "options": [
      {
        "letter": "A",
        "text": "ID token header",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "ID token signature",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "HTTP response code",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Tenant ID",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "147",
    "question": "HOTSPOT\n-\nYou are developing an Azure Static Web app that contains training materials for\na tool company. Each tool’s training material is contained in a static web page\nthat is linked from the tool’s publicly available description page.\nA user must be authenticated using Azure AD prior to viewing training.\nYou need to ensure that the user can view training material pages after\nauthentication.\nHow should you complete the configuration file? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI think this should look like this (.referrer will be replaced with the URL of original\nrequest): \"responseOverrides\": { \"401\": { \"statusCode\": \"302\", \"redirect\": \"/.auth/login/aad?\npost_login_redirect_uri=.referrer\" } }\nI think this should look like this (.referrer will be replaced with the URL of original\nrequest): \"responseOverrides\": { \"401\": { \"statusCode\": \"302\", \"redirect\": \"/.auth/login/aad?\npost_login_redirect_uri=.referrer\" } }\n\n\nI think this should look like this (.referrer will be replaced with the URL of original\nrequest): \"responseOverrides\": { \"401\": { \"statusCode\": \"302\", \"redirect\": \"/.auth/login/aad?\npost_login_redirect_uri=.referrer\" } }\nI think this should look like this (.referrer will be replaced with the URL of original\nrequest): \"responseOverrides\": { \"401\": { \"statusCode\": \"302\", \"redirect\": \"/.auth/login/aad?\npost_login_redirect_uri=.referrer\" } }\nGot it in the exam 7/03/23. Went with the given answer",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "148",
    "question": "HOTSPOT\n-\nYou are authoring a set of nested Azure Resource Manager templates to deploy\nAzure resources. You author an Azure Resource Manager template named\nmainTemplate.json that contains the following linked templates:\nlinkedTemplate1.json, linkedTemplate2.json.\nYou add parameters to a parameters template file named\nmainTemplate.parameters,json. You save all templates on a local device in the\nC:\\templates\\ folder.\nYou have the following requirements:\n• Store the templates in Azure for later deployment.\n• Enable versioning of the templates.\n• Manage access to the templates by using Azure RBAC.\n• Ensure that users have read-only access to the templates.\n• Allow users to deploy the templates.\nYou need to store the templates in Azure.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\ncorrect https://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/\ntemplate-specs-create-linked?tabs=azure-cli\nThis whole text is wrong. Please, ignore this answer\nThis whole text is wrong. Please, ignore this answer\nThis was on the exam (08/20/2023). Went with proposed. Scored 925\nBecause the code is not about creating a storage account. Pay attention to the part: --\nname TemplateStore",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "149",
    "question": "HOTSPOT\n-\nYou are developing a service where customers can report news events from a\nbrowser using Azure Web PubSub. The service is implemented as an Azure\nFunction App that uses the JSON WebSocket subprotocol to receive news\nevents.\nYou need to implement the bindings for the Azure Function App.\nHow should you configure the binding? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nreceived 2023-17-04 went with given answer, score 926\nGiven answer is correct. https://learn.microsoft.com/en-us/azure/azure-web-pubsub/\nreference-functions-bindings?tabs=javascript\nwere all Qs from sec exams?\nwere all Qs from sec exams?\n\n\nwhat does eventType \"user\" do ?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "150",
    "question": "HOTSPOT\n-\nYou are building a software-as-a-service (SaaS) application that analyzes DNA\ndata that will run on Azure virtual machines (VMs) in an availability zone. The\ndata is stored on managed disks attached to the VM. The performance of the\nanalysis is determined by the speed of the disk attached to the VM.\nYou have the following requirements:\n• The application must be able to quickly revert to the previous day’s data if a\nsystemic error is detected.\n• The application must minimize downtime in the case of an Azure datacenter\noutage.\nYou need to provision the managed disk for the VM to maximize performance\nwhile meeting the requirements.\nWhich type of Azure Managed Disk should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nWrong answer: Managed disk doesn't support GRS Ans: Premium SSD and ZRS They are\nasking for high performance workloads which is supported by Premium tier https://\nlearn.microsoft.com/en-us/azure/virtual-machines/disks-types Also they are asking for\nzone redundancy (if datacenter goes down, NOT region outage). Also managed disk\ndoesn't support GRS https://learn.microsoft.com/en-us/azure/virtual-machines/disks-\nredundancy\nAzure managed disks offer two storage redundancy options, zone-redundant storage\n(ZRS), and locally-redundant storage. Ans: Premium SSD and ZRS https://\nlearn.microsoft.com/en-us/azure/virtual-machines/disks-redundancy\nI greed. Premium SSD, ZRS are correct answers\nI greed. Premium SSD, ZRS are correct answers\nI believe the VMs are not in the AZ-204 anymore",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "151",
    "question": "HOTSPOT\n-\nYou are developing an application that includes two Docker containers.\nThe application must meet the following requirements:\n• The containers must not run as root.\n• The containers must be deployed to Azure Container Instances by using a\nYAML file.\n• The containers must share a lifecycle, resources, local network, and storage\nvolume.\n• The storage volume must persist through container crashes.\n• The storage volume must be deployed on stop or restart of the containers.\nYou need to configure Azure Container Instances for the application.\nWhich configuration values should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAns: Container group, EmptyDir Container group is the only logical answer that can have\nshared lifecycle https://learn.microsoft.com/en-us/azure/container-instances/container-\ninstances-container-groups?source=recommendations#what-is-a-container-group Azure\nfiles need root permission Secret is for secrets and read-only EmtyDir can persist\nthrough crash and redeployed on stop and restart https://learn.microsoft.com/en-us/\nazure/container-instances/container-instances-volume-emptydir#emptydir-volume\nCloned Git Repo also does the job but it needs more details like Git URL and stuff which\nare not mentioned to be available in the question\ncorrect, in 2023Mar24, score:904/1000\ncorrect, in 2023Mar24, score:904/1000\nChat GPT : Container Group, Azure fileshare\n\n\nData in an emptyDir volume is persisted through container crashes. Containers that are\nrestarted, however, are not guaranteed to persist the data in an emptyDir volume. If you\nstop a container group, the emptyDir volume is not persisted.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "152",
    "question": "You develop Azure Durable Functions to manage vehicle loans.\nThe loan process includes multiple actions that must be run in a specified\norder. One of the actions includes a customer credit check process, which may\nrequire multiple days to process.\nYou need to implement Azure Durable Functions for the loan process.\nWhich Azure Durable Functions type should you use?",
    "options": [
      {
        "letter": "A",
        "text": "orchestrator",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "client",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "entity",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "activity",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "153",
    "question": "HOTSPOT\n-\nYou are developing an Azure Function app.\nAll functions in the app meet the following requirements:\n• Run until either a successful run or until 10 run attempts occur.\n• Ensure that there are at least 20 seconds between attempts for up to 15\nminutes.\nYou need to configure the host.json file.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer seems correct. Did some research about exponential backoff: The \"exponential\nbackoff\" retry strategy is a technique for retrying failed operations in a manner that\navoids overloading the system being accessed. It works by increasing the amount of time\nthat is waited between each retry attempt, using an exponential function to calculate the\nwait time. For example, with a coefficient of 2.0 and an initial retry interval of 1 second,\nthe wait times between retries might be 1 second, 2 seconds, 4 seconds, 8 seconds, and\nso on. This allows the system being accessed to recover from any failures or load spikes\nbefore the next retry attempt is made, reducing the likelihood of further failures.\nReceived this in my exam today (22/02/2023). Selected retry, exponentialBackoff, and\nmaxRetry. Score 927.\n\n\nHad this question today: 2023-07-26\nGot this in exam today (5 April 2023)\nWhere is then the coefficient argument in the code? or is this an implicit value?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "154",
    "question": "You develop Azure Web Apps for a commercial diving company. Regulations\nrequire that all divers fill out a health questionnaire every 15 days after each\ndiving job starts.\nYou need to configure the Azure Web Apps so that the instance count scales up\nwhen divers are filling out the questionnaire and scales down after they are\ncomplete.\nYou need to configure autoscaling.\nWhat are two possible auto scaling configurations to achieve this goal? Each\ncorrect answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Recurrence profile",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "CPU usage-based autoscaling",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Fixed date profile",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Predictive autoscaling",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D"
  },
  {
    "number": "155",
    "question": "HOTSPOT\n-\nYou are developing an application that monitors data added to an Azure Blob\nstorage account.\nYou need to process each change made to the storage account.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI've tried the code in VS. Here's some thoughts: 1. box: - GetChanges() - wrong - var c in\nthe foreach would be BlobChangeFeedEvent which doesn't contain Values property used\nin ProcessChanges(c.Values) line below - GetChangesAsync - wrong - code won't compile\nbecause it would require await foreach loop instead - GetChanges(x).AsPages() - correct -\nit's the only option to make this code even compile - GetChanges(x).GetEnumerator() -\nwrong - you cannot use IEnumerator type as foreach source 2. box: - x =\nc.ContinuationToken - right - variable x was used as continuationToken parameter in\nchangeFeedClient.GetChanges(x).AsPages() above - c.GetRawResponse().ReasonPhrase -\nwrong - that does not make sense to use this value as continuation token - x =\nc.Values.Min - wrong - continuation token is a number not date - x = c.Values.Max - wrong\n- as above So to sum up 1. changeFeedClient.GetChanges(x).AsPages() 2. x =\nc.ContinuationToken; You can find more about Continuation Token here: https://\njessehouwing.net/azure-devops-accessing-apis-with-large-volumes-of-data/\nyou are right... 1. changeFeedClient.GetChanges(x).AsPages() -> returns an\nIEnumerable<Page<BlobChangeFeedEvent>> ... when you loop through these pages\n\"Page<BlobChangeFeedEvent>\" you will get the options \"page.ContinuationToken\" and\npage.Values which are used in this example 100% Correct... found the code here https://\ngithub.com/Azure/azure-sdk-for-net/tree/main/sdk/storage/\nAzure.Storage.Blobs.ChangeFeed#resume-with-continuationtoken\n\n\nyou are right... 1. changeFeedClient.GetChanges(x).AsPages() -> returns an\nIEnumerable<Page<BlobChangeFeedEvent>> ... when you loop through these pages\n\"Page<BlobChangeFeedEvent>\" you will get the options \"page.ContinuationToken\" and\npage.Values which are used in this example 100% Correct... found the code here https://\ngithub.com/Azure/azure-sdk-for-net/tree/main/sdk/storage/\nAzure.Storage.Blobs.ChangeFeed#resume-with-continuationtoken\nvar x = default(string); so, x is string ContinuationToken is not string\nvar x = default(string); so, x is string ContinuationToken is not string",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "156",
    "question": "HOTSPOT\n-\nYou develop an application that sells AI generated images based on user input.\nYou recently started a marketing campaign that displays unique ads every\nsecond day.\nSales data is stored in Azure Cosmos DB with the date of each sale being stored\nin a property named ‘whenFinished’.\nThe marketing department requires a view that shows the number of sales for\neach unique ad.\nYou need to implement the query for the view.\nHow should you complete the query? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nCorrect! Can't be DateTimePart as it takes two args only, see https://learn.microsoft.com/\nen-us/azure/cosmos-db/nosql/query/date-time-functions\nThis question was in today's exam on 10-June-2023\nOn exam 9 Nov 2023, went with given answer, socre 865. Case Study: Farmers and\nDistributors\n\n\nGiven answers Count, DateTimeBin are correct answers as per MS docs DateTimebin takes\n3 and 4 parameters but DateTimepart takes only 2 args https://learn.microsoft.com/en-\nus/azure/cosmos-db/nosql/query/date-time-functions\nYou are right good sir I stand corrected.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "157",
    "question": "HOTSPOT\n-\nYou implement an Azure solution to include Azure Cosmos DB, the latest Azure\nCosmos DB SDK, and the Core (SQL) API. You also implement a change feed\nprocessor on a new container instance by using the Azure Functions trigger for\nAzure Cosmos DB.\nA large batch of documents continues to fail when reading one of the\ndocuments in the batch. The same batch of documents is continuously retried\nby the triggered function and a new batch of documents must be read.\nYou need to implement the change feed processor to read the documents.\nWhich feature should you implement? To answer, select the appropriate\nfeatures in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nGot this on 16/03/23. Went with proposed solution. Make sure to prepare for case study. I\ngot city and lights case study. No Kubernetes, Search, Logic Apps questions for me.\nI agree with the answer! 1. Change feed estimators monitor change feed processors'\nprogress 2. Dead-letter queues handle errors and are able to monitor failed attempts,\nrequire failed attempts and even trigger a follow-up action (remediation or response)\nAgreed. This was on the exam (08/20/2023). Scored 925 Change feed estimator Dead-\nletter queue\nAgreed. This was on the exam (08/20/2023). Scored 925 Change feed estimator Dead-\nletter queue\nshould be vice versa",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "158",
    "question": "You are developing an application to store business-critical data in Azure Blob\nstorage.\nThe application must meet the following requirements:\n• Data must not be modified or deleted for a user-specified interval.\n• Data must be protected from overwrites and deletes.\n• Data must be written once and allowed to be read many times.\nYou need to protect the data in the Azure Blob storage account.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Configure a time-based retention policy for the storage account.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create an account shared-access signature (SAS).",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Enable the blob change feed for the storage account.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable version-level immutability support for the storage account.\nE) Enable point-in-time restore for containers in the storage account.\nF) Create a service shared-access signature (SAS).",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "F"
  },
  {
    "number": "159",
    "question": "You are updating an application that stores data on Azure and uses Azure\nCosmos DB for storage. The application stores data in multiple documents\nassociated with a single username.\nThe application requires the ability to update multiple documents for a\nusername in a single ACID operation.\nYou need to configure Azure Cosmos DB.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create a collection sharded on username to store documents.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure Azure Cosmos DB to use the Gremlin API.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an unsharded collection to store documents.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Configure Azure Cosmos DB to use the MongoDB API.",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D"
  },
  {
    "number": "160",
    "question": "You develop Azure solutions.\nYou must connect to a No-SQL globally-distributed database by using the .NET\nAPI.\nYou need to create an object to configure and execute requests in the database.\nWhich code segment should you use?",
    "options": [
      {
        "letter": "A",
        "text": "database_name = 'MyDatabase'\ndatabase = client.create_database_if_not_exists(id=database_name)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "client = CosmosClient(endpoint, key)",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "container_name = 'MyContainer'\ncontainer = database.create_container_if_not_exists(\nid=container_name, partition_key=PartitionKey(path=\"/lastName\"), offer_throughput=400 )",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "161",
    "question": "HOTSPOT\n-\nYou are developing an online game that allows players to vote for their favorite\nphoto that illustrates a word. The game is built by using Azure Functions and\nuses durable entities to track the vote count.\nThe voting window is 30 seconds. You must minimize latency.\nYou need to implement the Azure Function for voting.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAnswer is correct: https://learn.microsoft.com/en-us/azure/azure-functions/durable/\ndurable-functions-dotnet-entities\nGot this today. Went with answer here. Score 927\nGot this today. Went with answer here. Score 927\nIDurableEntityClient is correct to works with entities. Signal is used for 1-way Call would\nbe used for 2-way\nThis seems to be a new question. It's not here last month. Will exam next week",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "162",
    "question": "You develop a web application that provides access to legal documents that are\nstored on Azure Blob Storage with version-level immutability policies.\nDocuments are protected with both time-based policies and legal hold policies.\nAll time-based retention policies have the AllowProtectedAppendWrites\nproperty enabled.\nYou have a requirement to prevent the user from attempting to perform\noperations that would fail only when a legal hold is in effect and when all other\npolicies are expired.\nYou need to meet the requirement.\nWhich two operations should you prevent? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "adding data to documents",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "deleting documents",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "creating documents",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "overwriting existing documents",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D"
  },
  {
    "number": "163",
    "question": "HOTSPOT\n-\nYou provisioned an Azure Cosmos DB for NoSQL account named account1 with\nthe default consistency level.\nYou plan to configure the consistency level on a per request basis. The level\nneeds to be set for consistent prefix for read and write operations to account1.\nYou need to identify the resulting consistency level for read and write\noperations.\nWhich levels should you configure? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nStill more useful than yours :)\nStill more useful than yours :)\nStill more useful than yours :)\nStill more useful than yours :)\nJust passed the exam 26/04/2023. All the questions were from Exam topics. Got 970\nmarks!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "164",
    "question": "DRAG DROP\n-\nYou are developing an application to store millions of images in Azure blob\nstorage. The images are uploaded to an Azure blob storage container named\ncompanyimages contained in an Azure blob storage account named\ncompanymedia. The stored images are uploaded with multiple blob index tags\nacross multiple blobs in the container.\nYou must find all blobs whose tags match a search expression in the container.\nThe search expression must evaluate an index tag named status with a value of\nfinal.\nYou need to construct the GET method request URI.\nHow should you complete the URI? To answer, drag the appropriate parameters\nto the correct request URI targets. Each parameter may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is correct!\nJust for information: I just had this question on my AZ204 exam - 16-jun-2023. I barely\nmade it (with only 767 points) so I can't inform anyony if this answer is correct or not,\njust stating that this is an actual exam question.\nGot this question on the exam at 2023/05/31\nCorrect\nDo you have contributor access? what other websites did you study from?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "165",
    "question": "HOTSPOT\n-\nYou have an App Service plan named asp1 based on the Free pricing tier.\nYou plan to use asp1 to implement an Azure Function app with a queue trigger.\nYour solution must minimize cost.\nYou need to identify the configuration options that will meet the requirements.\nWhich value should you configure? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\n1. Always on If you run on an App Service plan, you should enable the Always on setting\nso that your function app runs correctly. On an App Service plan, the functions runtime\ngoes idle after a few minutes of inactivity, so only HTTP triggers will \"wake up\" your\nfunctions. The Always on setting is available only on an App Service plan. On a\nConsumption plan, the platform activates function apps automatically. Even with Always\nOn enabled, the execution timeout for individual functions is controlled by the\nfunctionTimeout setting in the host.json project file. 2. Basic The dedicated App Service\nplans supported for function app hosting include Basic, Standard, Premium, and Isolated\nSKUs. Free and Shared tier App Service plans aren't supported by Azure Functions. Basic\nis the correct answer since it is cheaper than Standard. https://learn.microsoft.com/en-\nus/azure/azure-functions/dedicated-plan\nAlways On and Continuous Deployment are not supported on the Free tier plans. Answer:\nManaged Identity & Basic\n1. Always on 2. Basic\nAnswer is correct. https://learn.microsoft.com/en-us/azure/azure-functions/dedicated-\nplan\n\n\nIt was on my exam today (2023-09-26) I went with the secexams answer - score 850",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "166",
    "question": "DRAG DROP\n-\nYou are developing several microservices to run on Azure Container Apps.\nThe microservices must allow HTTPS access by using a custom domain.\nYou need to configure the custom domain in Azure Container Apps.\nIn which order should you perform the actions? To answer, move all actions\nfrom the list of actions to the answer area and arrange them in the correct\norder.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\n1. Enable ingress: Enable ingress for the Azure Container Apps. This allows external traffic\nto reach the microservices. 2. Add the custom domain name: Add the custom domain\nname that you want to use for HTTPS access to your Azure Container Apps. 3. Add DNS\nrecords to the domain provider: After adding the custom domain name in Azure\nContainer Apps, you need to add the required DNS records (such as CNAME or A records)\nto your domain provider's DNS settings. This step is essential for directing traffic from the\ncustom domain to your Azure Container Apps. 4. Validate the custom domain name (after\nadding the dns records to the provider): After adding the DNS records to the domain\nprovider, you need to validate the custom domain name in Azure Container Apps. This\nstep ensures that the DNS records are correctly configured and the domain is pointing to\nyour Azure Container Apps. 5. Bind the certificate (only enabled after finishing the\nvalidation): Once the custom domain name is validated, you can bind the SSL/TLS\ncertificate to enable HTTPS access for the custom domain.\nfrom my understanding of https://learn.microsoft.com/en-us/azure/container-apps/\ncustom-domains-certificates 1) Enable Ingress == step 2 2) Add the custom domain name\n== step 4 3) Bind certificate == steps 5 to 11 4) Add DNS records to the domain provider ==\n12 to 14 5) Validate the custom domain name == step 15 Using similar questions as a\nbasis, I'm assuming that \"Add the custom..\" means click the button and not step 16 \"Once\nvalidation succeeds, select the Add button\" Not very clear.\nLatest correct answer based on this link https://learn.microsoft.com/en-us/azure/\ncontainer-apps/custom-domains-certificates Please look at No.2, No. 4, No.7, No. 10, No.14\n--> This is the correct order. Note: No. 15 is validating the DNS not the CDN. Correct\nanswer. 1. Enable Ingress 2. Add the custom domain name 3. Bind certificate 4. Validate\nthe custom domain name 5. Add DNS records to the domain provider\nThe correct answer is: 1. Enable ingress 2. Add the custom domain name 3. Bind the\ncertificate 4. Add DNS records to the domain provider 5. Validate the custom domain\n\n\nname Explanation: -Enable ingress: This is the first step as it allows HTTP access to your\ncontainer app. -Add the custom domain name: After enabling ingress, you can add the\ncustom domain name to your container app. -Bind the certificate: Once the custom\ndomain name is added, you can bind the certificate to it. This can be a certificate you\nupload or a free managed certificate provided by Azure. -Add DNS records to the domain\nprovider: After binding the certificate, you need to add DNS records to your domain\nprovider. This points the domain to your container app and verifies that you own it. -\nValidate the custom domain name: Finally, you validate the custom domain name. This\nensures that the DNS records have been correctly configured and that the domain\ncorrectly points to your container app.\nOccurs to me that steps 5-11 adding the cert maybe irrelevant and binding the cert is\nactually the last step, based on what you click in the UI.. so I'd revise that to: 1) Enable\nIngress 2) Add the custom domain name 3) Add DNS records to the domain provider 4)\nValidate the custom domain name 5) Bind certificate, Still not clear!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "167",
    "question": "You are developing several microservices to run on Azure Container Apps.\nExternal HTTP ingress traffic has been enabled for the microservices.\nThe microservices must be deployed to the same virtual network and write logs\nto the same Log Analytics workspace.\nYou need to deploy the microservices.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable single revision mode.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Use a separate environment for each container.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use a private container registry image and single image for all containers.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use a single environment for all containers. \nE) Enable multiple revision mode.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "168",
    "question": "HOTSPOT\n-\nYou develop two Python scripts to process data.\nThe Python scripts must be deployed to two, separate Linux containers running\nin an Azure Container Instance container group. The containers must access\nexternal data by using the Server Message Block (SMB) protocol. Containers in\nthe container group must run only once.\nYou need to configure the Azure Container Instance.\nWhich configuration value should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI believe the answer is correct. Azure file share: https://learn.microsoft.com/en-us/\nazure/storage/files/files-smb-protocol?tabs=azure-portal\nAzure File Share is the only option that supports SMB\nAnswer is correct.\nIs not Empty Directory, but Azure File Share.\nWhyyyy???????????????",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "169",
    "question": "HOTSPOT\n-\nYou are developing a static website hosted on Azure Blob Storage. You create a\nstorage account and enable static website hosting.\nThe website must support the following requirements:\n• Custom domain name\n• Custom header values for all responses\n• Custom SSL certificate\nYou need to implement the static website.\nWhat should you configure? To answer, select the appropriate values in the\nanswer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer seems correct. \"Static websites have some limitations. For example, If you want\nto configure headers, you'll have to use Azure Content Delivery Network (Azure CDN)\" and\n\" To enable HTTPS, you'll have to use Azure CDN because Azure Storage doesn't yet\nnatively support HTTPS with custom domains\" https://learn.microsoft.com/en-us/azure/\nstorage/blobs/storage-blob-static-website\nOn exam 9 Nov 2023, went with given answer, socre 865. Case Study: Farmers and\nDistributors\n\"Farmers and Distribuor\" is a new one (not present here on SecExams).\n\"Farmers and Distribuor\" is a new one (not present here on SecExams).\n\n\nThe answers are correct. The most appropriate answer is Azure CDN, as it can fulfill all\nthree requirements: custom domain name, custom header values, and a custom SSL\ncertificate.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "170",
    "question": "You are developing an inventory tracking solution. The solution includes an\nAzure Function app containing multiple functions triggered by Azure Cosmos DB.\nYou plan to deploy the solution to multiple Azure regions.\nThe solution must meet the following requirements:\n• Item results from Azure Cosmos DS must return the most recent committed\nversion of an item.\n• Items written to Azure Cosmos DB must provide ordering guarantees.\nYou need to configure the consistency level for the Azure Cosmos DB\ndeployments.\nWhich consistency level should you use?",
    "options": [
      {
        "letter": "A",
        "text": "consistent prefix",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "eventual",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "bounded staleness",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "strong \nE) session",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "171",
    "question": "HOTSPOT\n-\nYou are developing an application that runs in several customer Azure\nKubernetes Service clusters. Within each cluster, a pod runs that collects\nperformance data to be analyzed later. A large amount of data is collected so\nsaving latency must be minimized.\nThe performance data must be stored so that pod restarts do not impact the\nstored data. Write latency should be minimized.\nYou need to configure blob storage.\nHow should you complete the YAML configuration? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nThanks for mentioning, so this question is definitely not out of scope! In fact, I don't even\nbelieve \"scope\" is in Microsoft's dictionary\nThanks for mentioning, so this question is definitely not out of scope! In fact, I don't even\nbelieve \"scope\" is in Microsoft's dictionary\ngot in 28/09/23 , totally out of scope\nSadly got this question 08/17/23 in the exam\n\n\nOut of scope. Suggested answer seems correct. Ref: https://learn.microsoft.com/en-us/\nazure/aks/azure-csi-disk-storage-provision https://kubernetes.io/docs/concepts/\nstorage/storage-classes/#azure-disk Ref azure-file https://learn.microsoft.com/en-us/\nazure/aks/azure-csi-files-storage-provision#create-a-storage-class (Create a storage\nclass) https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file Ref\nPodStorage / Persistent Volume / PersistentVolumeClaim https://kubernetes.io/docs/\ntasks/configure-pod-container/configure-persistent-volume-storage/ Ref portworx-\nvolume https://docs.portworx.com/portworx-enterprise/operations/operate-kubernetes/\nstorage-operations/kubernetes-storage-101/volumes Ref Reclaim Policy https://\nkubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "172",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment\n-\nCorporate website\n-\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\nRetail Store Locations\n-\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements\n-\nThe application components must meet the following requirements:\nCorporate website\n-\n\n\n• Secure the website by using SSL.\n• Minimize costs for data storage and hosting.\n• Implement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\n• Distribute the website content globally for local use.\n• Implement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\n• The website must have 99.95 percent uptime.\nRetail store locations\n-\n• Azure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\n• Audit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services\n-\n• Store service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\n• Store delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services\n-\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to\ninclude read-only access to the data.\n\n\nSecurity\n-\n• All Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\n• Authentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues\n-\nRetail Store Locations\n-\n• You must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\n• Azure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling.\nYou need to implement the delivery service telemetry data.\nHow should you configure the solution? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nAPI: \"Core (SQL)\" Partition Key: \"Item id\" See: https://learn.microsoft.com/en-us/azure/\ncosmos-db/partitioning-overview#use-item-id-as-the-partition-key\nOn exam 3-Nov-2023. Went with most-voted answer - 932/1000: 1) Core SQL 2) Item Id\nNote: all 11 Qs from VanArsdel case study were on Exam.\nWhy item.id is not used for Partition Key?\nThis should be Core(SQL) and Partition Key (item.id) i belive ?\nWhy can I see only 1 question here is it because I dont have contributors access?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "173",
    "question": "HOTSPOT\n-\nYou are developing several microservices to run on Azure Container Apps.\nExternal HTTP ingress traffic has been enabled for the microservices.\nA deployed microservice must be updated to allow users to test new features.\nYou have the following requirements:\n• Enable and maintain a single URL for the updated microservice to provide to\ntest users.\n• Update the microservice that corresponds to the current microservice version.\nYou need to configure Azure Container Apps.\nWhich features should you configure? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n1. Revision Label Labels are useful for testing new revisions. For example, when you want\nto give access to a set of test users, you can give them the label's URL. Then when you\nwant to move your users to a different revision, you can move the label to that revision.\n2. Revision Mode: The revision mode controls whether only a single revision or multiple\nrevisions of your container app can be simultaneously active. You can set your app's\nrevision mode from your container app's Revision management page in the Azure portal,\nusing Azure CLI commands, or in the ARM template. https://learn.microsoft.com/en-us/\nazure/container-apps/revisions\nCorrect. Revision labels are most useful when the app is in multiple revision mode.\nRevision mode controls whether only a single revision or multiple revisions of your\ncontainer app can be simultaneously active.\nQuestion in my exam 22sept 2023\n1. Revision Label: For container apps with external HTTP traffic, labels are a portable\nmeans to direct traffic to specific revisions. A label provides a unique URL that you can\nuse to route traffic to the revision that the label is assigned. 2. Revision Mode The\nrevision mode controls whether only a single revision or multiple revisions of your\n\n\ncontainer app can be simultaneously active. https://learn.microsoft.com/en-us/azure/\ncontainer-apps/revisions\nme too, today",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "174",
    "question": "You create and publish a new Azure App Service web app.\nUser authentication and authorization must use Azure Active Directory (Azure\nAD).\nYou need to configure authentication and authorization.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Add an identity provider.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Map an existing custom DNS name.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create and configure a new app setting.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add a private certificate.\nE) Create and configure a managed identity.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "175",
    "question": "DRAG DROP\n-\nYou have an Azure Cosmos DB for NoSQL account.\nYou plan to develop two apps named App1 and App2 that will use the change\nfeed functionality to track changes to containers. App1 will use the pull model\nand App2 will use the push model.\nYou need to choose the method to track the most recently processed change in\nApp1 and App2.\nWhich component should you use? To answer, drag the appropriate components\nto the correct apps. Each component may be used once, more than once, or not\nat all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nSeems correct. App2 Push Model - Lease container \"When reading from the Azure Cosmos\nDB change feed, we usually recommend using a push model because you won't need to\nworry about: ... Storing state for the last processed change. If you are reading from the\nchange feed processor, state is automatically stored in a lease container. Ref: https://\nlearn.microsoft.com/en-us/azure/cosmos-db/nosql/read-change-feed#reading-change-\nfeed-with-a-push-model\"\nCorrect. App1 (Pull Model): This model involves explicitly querying the change feed and\nmanaging the state of what has been read. Best Component: Continuation Token. The\npull model typically relies on continuation tokens to keep track of where the last read\noperation ended and to resume from that point. App2 (Push Model): In this model, the\nchange feed processor pushes changes to the application, and the application logic\nprocesses these changes. Best Component: Lease Container. The push model, especially\nwhen using the change feed processor, leverages a lease container to maintain state and\nensure reliable processing.\nCorrect! The change feed pull model allows you to consume the change feed at your own\npace. Changes must be requested by the client and there's no automatic polling for\nchanges. If you want to permanently \"bookmark\" the last processed change (similar to\nthe push model's lease container), you'll need to save a continuation token. https://\nlearn.microsoft.com/en-us/azure/cosmos-db/nosql/read-change-feed\nThe answer is correct. Link: https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/\nchange-feed-pull-model?tabs=dotnet#compare-to-the-change-feed-processor\nCorrect",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "176",
    "question": "HOTSPOT\n-\nYou plan to develop an Azure Functions app with an HTTP trigger.\nThe app must support the following requirements:\n• Event-driven scaling\n• Ability to use custom Linux images for function execution\nYou need to identify the app’s hosting plan and the maximum amount of time\nthat the app function can take to respond to incoming requests.\nWhich configuration setting values should you use? To answer, select the\nappropriate values in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect: The Premium plan supports event-driven scaling and allows to use custom Linux\nimages. The default timeout for Azure Functions on the Consumption and Premium plans\nis 5 minutes (300 seconds), and 230 is a good fit.\nIgnore me. I'm stupid\nHave to correct my answer: Consumption plan does not allow for CUSTOM Linux images.\nSo the correct answer would have to be: Premium unlimited again source: https://\nlearn.microsoft.com/en-us/azure/azure-functions/functions-scale\nHave to correct my answer: Consumption plan does not allow for CUSTOM Linux images.\nSo the correct answer would have to be: Premium unlimited again source: https://\nlearn.microsoft.com/en-us/azure/azure-functions/functions-scale\nIgnore me. I'm stupid",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "177",
    "question": "HOTSPOT\n-\nYou develop a Python application for image rendering. The application uses\nGPU resources to optimize rendering processes.\nYou have the following requirements:\n• The application must be deployed to a Linux container.\n• The container must be stopped when the image rendering is complete.\n• The solution must minimize cost.\nYou need to deploy the application to Azure.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nACI - Because the GPU usage. Kubernetes can manage ACIs but is not a 'compute target'\nand it will increment the cost. \"The container instances in the group can access one or\nmore NVIDIA Tesla GPUs while running container workloads such as CUDA and deep\nlearning applications.\" https://learn.microsoft.com/en-us/azure/container-instances/\ncontainer-instances-gpu Restart Policy - To stop the container after the execution (In fact\nis avoiding to restart it after a succeeded execution) \"Set an appropriate restart policy for\nthe container instance, depending on whether the command-line specifies a long-\nrunning task or a run-once task. For example, a restart policy of Never or OnFailure is\nrecommended for a run-once task.\" https://learn.microsoft.com/en-us/azure/container-\ninstances/container-instances-start-command#command-line-guidelines\ngiven that AKS is not in the exam curriculum , only ACA and ACI are left. out of which,\nonly ACI supports GPU (though still in preview). As for the container termination-> restart\npolicy set to never should be true\nTo deploy a Python application for image rendering with the specified requirements on\nAzure, you can use Azure Container Instances (ACI). ACI provides an easy and cost-\neffective way to run containers in the cloud. Here are the options you can choose:\nCompute Value: Choose the ACI option for running your Linux container. Container\nTermination: Since you want the container to be stopped when the image rendering is\ncomplete to minimize cost, you can use the \"Single\" container group mode in Azure\nContainer Instances. This mode is suitable for scenarios where you want to run a single\n\n\ncontainer task and stop it when it's done. This command creates a single container\ninstance (--restart-policy Never) and stops it when the image rendering is complete,\nminimizing cost. Adjust the resource limits (--cpu and --memory) based on your\napplication's requirements.\nAs suggested ACI is preferred for one time or demand script and jobs execution whereas\nas ACA is preferred for deploying container continually running full lifecycle apps, I am\nleaning towards ACI.\nI agree with ACI Since its a new question the preview thing makes sense as well. source:\nhttps://learn.microsoft.com/en-gb/azure/container-instances/container-instances-\noverview",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "178",
    "question": "HOTSPOT\n-\nYou plan to develop an Azure Functions app with an Azure Blob Storage trigger.\nThe app will be used infrequently, with a limited duration of individual\nexecutions.\nThe app must meet the following requirements:\n• Event-driven scaling\n• Support for deployment slots\n• Minimize costs\nYou need to identify the hosting plan and the maximum duration when\nexecuting the app.\nWhich configuration setting values should you use? To answer, select the\nappropriate values in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nCorrect. \"used infrequently\" -> Consumption And 10mins is a good choice.\nRight Answer: Premium - 230 Event-Driven requirement: Is only souported by\nConsumption and Premium https://learn.microsoft.com/en-us/azure/azure-functions/\nfunctions-scale#scale Slot requirement: Is only souported by Dedicated and Premium\n\"Function apps running under the Apps Service plan may have multiple slots, while\nunder the Consumption plan only one slot is allowed.\" https://learn.microsoft.com/en-\nus/azure/azure-functions/functions-deployment-slots Minimize costs: Is not relevant\nbecause there is just one service plan that meets the previous requirements\nConsumption plan also supports 2 slots..https://learn.microsoft.com/en-us/azure/azure-\nfunctions/functions-scale#service-limits Answer; Consumption - 10 min\nConsumption plan also supports 2 slots..https://learn.microsoft.com/en-us/azure/azure-\nfunctions/functions-scale#service-limits Answer; Consumption - 10 min\n\"Support for deployment slotS\" not slot (more than one)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "179",
    "question": "You have a Linux container-based console application that uploads image files\nfrom customer sites all over the world. A back-end system that runs on Azure\nvirtual machines processes the images by using the Azure Blobs API.\nYou are not permitted to make changes to the application.\nSome customer sites only have phone-based internet connections.\nYou need to configure the console application to access the images.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure BlobFuse",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure Disks",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Storage Network File System (NFS) 3.0 support",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Files",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "180",
    "question": "DRAG DROP\n-\nYou are developing several microservices named serviceA, serviceB, and\nserviceC. You deploy the microservices to a new Azure Container Apps\nenvironment.\nYou have the following requirements:\n• The microservices must persist data to storage.\n• serviceA must persist data only visible to the current container and the\nstorage must be restricted to the amount of disk space available in the\ncontainer.\n• serviceB must persist data for the lifetime of the replica and allow multiple\ncontainers in the replica to mount the same storage location.\n• serviceC must persist data beyond the lifetime of the replica while allowing\nmultiple containers to access the storage and enable per object permissions.\nYou need to configure storage for each microservice.\nWhich storage type should you use? To answer, drag the appropriate storage\ntypes to the correct microservices. Each storage type may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nACA does not mount to blob storage(https://learn.microsoft.com/en-us/azure/container-\napps/storage-mounts), AKS i belive it does. so original answer is right\nACA does not mount to blob storage(https://learn.microsoft.com/en-us/azure/container-\napps/storage-mounts), AKS i belive it does. so original answer is right\nSorry. I'm mean: ServiceA: Container file system ServiceB: Ephemeral volumes (it lives as\nlong as replica lives) ServiceC: Azure Blob Storage (it lives outside of replica. So it's\nbeyond the lifetime. And we want to have blob instead of file)\nACA does not mount to blob storage(https://learn.microsoft.com/en-us/azure/container-\napps/storage-mounts), AKS i belive it does. so original answer is right\nACA does not mount to blob storage(https://learn.microsoft.com/en-us/azure/container-\napps/storage-mounts), AKS i belive it does. so original answer is right",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "181",
    "question": "You are developing an ASP.NET Core app hosted in Azure App Service.\nThe app requires custom claims to be returned from Microsoft Entra ID for user\nauthorization. The claims must be removed when the app registration is\nremoved.\nYou need to include the custom claims in the user access token.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Require the https://graph.microsoft.com/.default scope during authentication.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure the app to use the OAuth 2.0 authorization code flow.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Implement custom middleware to retrieve role information from Azure AD.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add the groups to the groupMembershipClaims attribute in the app manifest.\nE) Add the roles to the appRoles attribute in the app manifest.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "supports adding custom roles to an application's\nmanifest, which can then be assigned to users or groups. When a user is authenticated,\nthese roles are included in the token as claims. This approach allows for fine-grained\naccess control within your application based on these role assignments. Moreover, when\nthe application registration is deleted, these roles and corresponding claims\nautomatically cease to exist, fulfilling the requirement that the claims must be removed\nwhen the app registration is removed.\nE not D because of this condition \"The claims must be removed when the app\nregistration is removed.\"",
        "is_correct": false
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "182",
    "question": "You are developing a microservice to run on Azure Container Apps for a\ncompany. External HTTP ingress traffic has been enabled.\nThe company requires that updates to the microservice must not cause\ndowntime.\nYou need to deploy an update to the microservices.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable single revision mode.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Use multiple environments for each container.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use a private container registry and single image for all containers.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use a single environment for all containers.\nE) Enable multiple revision mode.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "183",
    "question": "HOTSPOT\n-\nA company uses Azure Container Apps. A container app named App1 resides in a\nresource group named RG1.\nThe company requires testing of updates to App1.\nYou enable multiple revision modes on App1.\nYou need to ensure traffic is routed to each revision of App1.\nHow should you complete the code segment?\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect. containerapp, ingress Source: https://learn.microsoft.com/en-us/azure/\ncontainer-apps/traffic-splitting?pivots=azure-cli\nCORRECT",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "184",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou deploy an Azure Container Apps app and disable ingress on the container\napp.\nUsers report that they are unable to access the container app. You investigate\nand observe that the app has scaled to 0 instances.\nYou need to resolve the issue with the container app.\nSolution: Enable ingress, create an HTTP scale rule, and apply the rule to the\ncontainer app.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "185",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou deploy an Azure Container Apps app and disable ingress on the container\napp.\nUsers report that they are unable to access the container app. You investigate\nand observe that the app has scaled to 0 instances.\nYou need to resolve the issue with the container app.\nSolution: Enable ingress, create a custom scale rule, and apply the rule to the\ncontainer app.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "186",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou deploy an Azure Container Apps app and disable ingress on the container\napp.\nUsers report that they are unable to access the container app. You investigate\nand observe that the app has scaled to 0 instances.\nYou need to resolve the issue with the container app.\nSolution: Enable ingress and configure the minimum replicas to 1 for the\ncontainer app.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "187",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to configure App Service to support the corporate website migration.\nWhich configuration should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nBasic App Service Plan does not have multiple Deployment slot. Answer should be\nStandard and Deployment Slot.\nStandard and Deployment Slot.\nAbsolutely wrong, because \"applications must autoscale\", basic plan doesn't allowe this,\nthe answer is Standard: https://learn.microsoft.com/en-us/azure/app-service/manage-\nautomatic-scaling?tabs=azure-portal\nApp Service Plan: Standard. It meets the need for auto-scaling and cost efficiency while\nproviding the necessary features such as SSL and custom domains. Code Change\nValidation Feature: Deployment Slot. Enables the company to validate all code changes in\na pre-production environment, ensuring stability and performance before changes are\nmade live.\ncorrect: https://learn.microsoft.com/en-us/azure/app-service/deploy-staging-slots?\nsource=recommendations&tabs=portal",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "188",
    "question": "DRAG DROP\n-\nYou are developing a web service that will run on Azure virtual machines that\nuse Azure Storage. You configure all virtual machines to use managed identities.\nYou have the following requirements:\n• Secret-based authentication mechanisms are not permitted for accessing an\nAzure Storage account.\n• Must use only Azure Instance Metadata Service endpoints.\nYou need to write code to retrieve an access token to access Azure Storage. To\nanswer, drag the appropriate code segments to the correct locations. Each code\nsegment may be used once or not at all. You may need to drag the split bar\nbetween panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nNot relevant question and answer!\nI asked Google AI tool the following question: How do I retrieve an access token using\nAzure Instance Metadata Service endpoints ? I think the given answer here is correct!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "189",
    "question": "HOTSPOT\n-\nYou are developing an Azure Function app.\nThe Azure Function app must enable a WebHook to read an image from Azure\nBlob Storage and create a new Azure Cosmos DB document.\nYou need to implement the Azure Function app.\nWhich configuration should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nGiven Answer is Correct: A- HTTP Trigger for webhook B- Input binding for blob storage to\nread image C- Output binding to output a document to Cosmos DB https://\nlearn.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-\ninput?tabs=python-v2%2Cisolated-process%2Cnodejs-v4&pivots=programming-language-\ncsharp https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-\ncosmosdb-v2-output?tabs=python-v2%2Cisolated-process%2Cnodejs-\nv4%2Cextensionv4&pivots=programming-language-csharp\nGiven answer is false: Blob storage, Blob storage, Azure Cosmos DB\nprovided answer is correct: https://learn.microsoft.com/en-us/azure/azure-functions/\nfunctions-bindings-http-webhook-trigger?tabs=python-v2%2Cisolated-\nprocess%2Cnodejs-v4%2Cfunctionsv2&pivots=programming-language-csharp\nAnswer is correct because you can route storage events to webhook https://\nlearn.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-quickstart?\ntoc=%2Fazure%2Fevent-grid%2Ftoc.json",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "190",
    "question": "You create an Azure Cosmos DB for NoSQL database.\nYou plan to use the Azure Cosmos DB .NET SDK v3 API for NoSQL to upload the\nfollowing files:\nYou receive the following error message when uploading the files: “413 Entity\ntoo large”.\nYou need to determine which files you can upload to the Azure Cosmos DB for\nNoSQL database.\nWhich files can you upload?",
    "options": [
      {
        "letter": "A",
        "text": "File1, File2, File3, File4, and File5",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "File1 and File2 only",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "File1, File2, and File3 only",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "File1, File2, File3, and File4 only\nE) File1 only",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "191",
    "question": "A development team is creating a new REST API. The API will store data in Azure\nBlob storage. You plan to deploy the API to Azure App Service.\nDevelopers must access the Azure Blob storage account to develop the API for\nthe next two months. The Azure Blob storage account must not be accessible by\nthe developers after the two-month time period.\nYou need to grant developers access to the Azure Blob storage account.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Generate a shared access signature (SAS) for the Azure Blob storage account and provide\nthe SAS to all developers.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create and apply a new lifecycle management policy to include a last accessed date value.\nApply the policy to the Azure Blob storage account.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Provide all developers with the access key for the Azure Blob storage account. Update the API\nto include the Coordinated Universal Time (UTC) timestamp for the request header.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Grant all developers access to the Azure Blob storage account by assigning role-based access\ncontrol (RBAC) roles.\nPage 455 of 1410\n456 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview"
  },
  {
    "number": "192",
    "question": "DRAG DROP -\nYou develop a web application.\nYou need to register the application with an active Azure Active Directory (Azure\nAD) tenant.\nWhich three actions should you perform in sequence? To answer, move all\nactions from the list of actions to the answer area and arrange them in the\ncorrect order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nRegister a new application using the Azure portal\n1. Sign in to the Azure portal using either a work or school account or a personal\nMicrosoft account.\n2. If your account gives you access to more than one tenant, select your account in the\nupper right corner. Set your portal session to the Azure AD tenant that you want.\n3. Search for and select Azure Active Directory. Under Manage, select App registrations.\n4. Select New registration. (Step 1)\n5. In Register an application, enter a meaningful application name to display to users.\n6. Specify who can use the application. Select the Azure AD instance. (Step 2)\n7. Under Redirect URI (optional), select the type of app you're building: Web or Public\nclient (mobile & desktop). Then enter the redirect URI, or reply URL, for your application.\n(Step 3)\n8. When finished, select Register.\nCommunity Discussion\n1 Azure AD instance 2 In App Registration, select new registration 3 Create a new\napplication and provide the name Disscussion: https://www.secexams.com/discussions/\nmicrosoft/view/22224-exam-az-204-topic-3-question-3-discussion/\nI agree with you that it should be in following order: 1. Select AD instance - select AD\ntenant in portal 2. In App Registration, select new registration - use switched tenant 3.\nCreate a new application and provide the name\n\n\nI would say Komat is right, I would select the same steps.\nI would say Komat is right, I would select the same steps.\nKomat is right, reference is here: https://docs.microsoft.com/en-us/azure/active-\ndirectory/develop/quickstart-register-app",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "193",
    "question": "You have a new Azure subscription. You are developing an internal website for\nemployees to view sensitive data. The website uses Azure Active Directory\n(Azure\nAD) for authentication.\nYou need to implement multifactor authentication for the website.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Configure the website to use Azure AD B2C.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "In Azure AD, create a new conditional access policy.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Upgrade to Azure AD Premium.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "In Azure AD, enable application proxy.\nE) In Azure AD conditional access, enable the baseline policy.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nB: MFA Enabled by conditional access policy. It is the most flexible means to enable two-\nstep verification for your users. Enabling using conditional access policy only works for\nAzure MFA in the cloud and is a premium feature of Azure AD.\nC: Multi-Factor Authentication comes as part of the following offerings:\n✑ Azure Active Directory Premium licenses - Full featured use of Azure Multi-Factor\nAuthentication Service (Cloud) or Azure Multi-Factor Authentication Server\n(On-premises).\n✑ Multi-Factor Authentication for Office 365\n✑ Azure Active Directory Global Administrators\nReference:"
  },
  {
    "number": "194",
    "question": "DRAG DROP -\nAn organization plans to deploy Azure storage services.\nYou need to configure shared access signature (SAS) for granting access to Azure\nStorage.\nWhich SAS types should you use? To answer, drag the appropriate SAS types to\nthe correct requirements. Each SAS type may be used once, more than once, or\nnot at all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview\n\n\nCommunity Discussion\nCorrect\nIf you have reached this far, question no 160+ then you have taken paid subscription\n(Contributor role)\nI got this question on 6th August 2023. chose highly voted. passed with 904. I got Case\nstudy: city and Lights. All questions are from SecExams.\nI can see it without contributor, for now :)\nI can see it without contributor, for now :)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "195",
    "question": "HOTSPOT -\nYou are developing an ASP.NET Core app that includes feature flags which are\nmanaged by Azure App Configuration. You create an Azure App Configuration\nstore named AppFeatureflagStore as shown in the exhibit:\nYou must be able to use the feature in the app by using the following markup:\nYou need to update the app to use the feature flag.\nWhich values should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: FeatureGate -\nYou can use the FeatureGate attribute to control whether a whole controller class or a\nspecific action is enabled.\nBox 2: AddAzureAppConfiguration -\nThe extension method AddAzureAppConfiguration is used to add the Azure App\nConfiguration Provider.\nBox 3: https://appfeatureflagstore.azconfig.io\nYou need to request the access token with resource=https://<yourstorename>.azconfig.io\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/use-feature-flags-\ndotnet-core https://csharp.christiannagel.com/2020/05/19/azureappconfiguration/\nhttps://stackoverflow.com/questions/61899063/how-to-use-azure-app-configuration-\nrest-api\nCommunity Discussion\n\n\nAnd Microsoft claims this is not .Net specific.\nGot it in exam 20/10/2022\nThis was on the exam (July 2023). Went with proposed. Scored 917\nGot it in exam 28/08/23. Went with proposed answer. Scored 912\n2027 LOL",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "196",
    "question": "HOTSPOT -\nYou have a single page application (SPA) web application that manages\ninformation based on data returned by Microsoft Graph from another\ncompany's Azure\nActive Directory (Azure AD) instance.\nUsers must be able to authenticate and access Microsoft Graph by using their\nown company's Azure AD instance.\nYou need to configure the application manifest for the app registration.\nHow should you complete the manifest? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: true -\nThe oauth2AllowImplicitFlow attribute Specifies whether this web app can request\nOAuth2.0 implicit flow access tokens. The default is false. This flag is used for browser-\nbased apps, like JavaScript single-page apps.\nIn implicit flow, the app receives tokens directly from the Azure Active Directory (Azure\nAD) authorize endpoint, without any server-to-server exchange. All authentication logic\nand session handling is done entirely in the JavaScript client with either a page redirect\nor a pop-up box.\nBox 2: requiredResourceAccess -\nWith dynamic consent, requiredResourceAccess drives the admin consent experience and\nthe user consent experience for users who are using static consent.\nHowever, this parameter doesn't drive the user consent experience for the general case.\nresourceAppId is the unique identifier for the resource that the app requires access to.\nThis value should be equal to the appId declared on the target resource app.\nresourceAccess is an array that lists the OAuth2.0 permission scopes and app roles that\nthe app requires from the specified resource. Contains the id and type values of the\nspecified resources.\nExample:\n\"requiredResourceAccess\": [\n\n\n{\n\"resourceAppId\": \"00000002-0000-0000-c000-000000000000\",\n\"resourceAccess\": [\n{\n\"id\": \"311a71cc-e848-46a1-bdf8-97ff7156d8e6\",\n\"type\": \"Scope\"\n}\n]\n}\n],\nIncorrect Answers:\n✑ The legacy attribute availableToOtherTenants is no longer supported.\n✑ The addIns attribute defines custom behavior that a consuming service can use to call\nan app in specific contexts. For example, applications that can render file streams may\nset the addIns property for its \"FileHandler\" functionality. This parameter will let services\nlike Microsoft 365 call the application in the context of a document the user is working\non.\nExample:\n\"addIns\": [\n{\n\"id\": \"968A844F-7A47-430C-9163-07AE7C31D407\",\n\"type\":\" FileHandler\",\n\"properties\": [\n{\n\"key\": \"version\",\n\"value\": \"2\"\n}\n]\n}\n],\nBox 3: AzureADMyOrg -\nThe signInAudience attribute specifies what Microsoft accounts are supported for the\ncurrent application. Supported values are:\n✑ AzureADMyOrg - Users with a Microsoft work or school account in my organization's\nAzure AD tenant (for example, single tenant)\n✑ AzureADMultipleOrgs - Users with a Microsoft work or school account in any\norganization's Azure AD tenant (for example, multi-tenant)\n✑ AzureADandPersonalMicrosoftAccount - Users with a personal Microsoft account, or a\nwork or school account in any organization's Azure AD tenant\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-\nmanifest https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-\nimplicit-grant-flow\n\n\nCommunity Discussion\nI think the last one should be AzureADMultipleOrgs\ngot on my exam 6/22/2023\nI think Gmishra88 is correct. Question states that - \"Users must be able to authenticate\nand access Microsoft Graph by using their own company's Azure AD instance.\" So it\nshould be AzureADMyOrg\nsame, question states that AD data is from external organisation\nI think Gmishra88 is correct. Question states that - \"Users must be able to authenticate\nand access Microsoft Graph by using their own company's Azure AD instance.\" So it\nshould be AzureADMyOrg",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "197",
    "question": "You manage a data processing application that receives requests from an Azure\nStorage queue.\nYou need to manage access to the queue. You have the following requirements:\n✑ Provide other applications access to the Azure queue.\n✑ Ensure that you can revoke access to the queue without having to regenerate\nthe storage account keys.\n✑ Specify access at the queue level and not at the storage account level.\nWhich type of shared access signature (SAS) should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Service SAS with a stored access policy",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Account SAS",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "User Delegation SAS",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Service SAS with ad hoc SAS",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "198",
    "question": "HOTSPOT -\nYou are developing an application to store and retrieve data in Azure Blob\nstorage. The application will be hosted in an on-premises virtual machine (VM).\nThe\nVM is connected to Azure by using a Site-to-Site VPN gateway connection. The\napplication is secured by using Azure Active Directory (Azure AD) credentials.\nThe application must be granted access to the Azure Blob storage account with\na start time, expiry time, and read permissions. The Azure Blob storage account\naccess must use the Azure AD credentials of the application to secure data\naccess. Data access must be able to be revoked if the client application security\nis breached.\nYou need to secure the application access to Azure Blob storage.\nWhich security features should you use? To answer select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Shared access signature (SAS) token\nWhen your application design requires shared access signatures for access to Blob\nstorage, use Azure AD credentials to create a user delegation SAS when possible for\nsuperior security.\nBox 2: Stored access policy -\nStored access policies give you the option to revoke permissions for a service SAS\nwithout having to regenerate the storage account keys.\nA shared access signature can take one of the following two forms:\n✑ Service SAS with stored access policy. A stored access policy is defined on a resource\ncontainer, which can be a blob container, table, queue, or file share.\nThe stored access policy can be used to manage constraints for one or more service\nshared access signatures. When you associate a service SAS with a stored access policy,\nthe SAS inherits the constraints ג€\" the start time, expiry time, and permissions ג€\"\ndefined for the stored access policy.\n✑ Ad hoc SAS.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview\n\n\nCommunity Discussion\nstorage account access keys : similar to a root password for your storage account, not for\ndistrbution system assigned managed identity : only for use within of services in azure\nSAS token : right answer Stored Access Policy : right answer user assigned managed\nidentity : only for use within of services in azure, this must connect to a server outside\nCORS protectection : Site-to-Site VPN gateway, there is no other domain involved\nThe answer is correct since none of the other possible combinations make sense. Even\nthough service level SAS doesnt use Azure AD credentials.\n\"Data access must be able to be revoked if the client application security is breached.\" -\nisn't that a clue to use access policy? Typical SAS is not revokable itself.\n\"Data access must be able to be revoked if the client application security is breached.\" -\nisn't that a clue to use access policy? Typical SAS is not revokable itself.\nSAS with user-delegation works with AD roles",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "199",
    "question": "You are building a web application that uses the Microsoft identity platform for\nuser authentication.\nYou are implementing user identification for the web application.\nYou need to retrieve a claim to uniquely identify a user.\nWhich claim type should you use?",
    "options": [
      {
        "letter": "A",
        "text": "aud",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "nonce",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "oid",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "idp",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "oid -The object identifier for the user in Azure AD. This value is the immutable and non-\nreusable identifier of the user. Use this value, not email, as a unique identifier for users;"
  },
  {
    "number": "200",
    "question": "You are developing an Azure Function that calls external APIs by providing an\naccess token for the API. The access token is stored in a secret named token in\nan\nAzure Key Vault named mykeyvault.\nYou need to ensure the Azure Function can access to the token. Which value\nshould you store in the Azure Function App configuration?",
    "options": [
      {
        "letter": "A",
        "text": "KeyVault:mykeyvault;Secret:token",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "App:Settings:Secret:mykeyvault:token",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "AZUREKVCONNSTR_ https://mykeyveult.vault.ezure.net/secrets/token/",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "@Microsoft.KeyVault(SecretUri=https://mykeyvault.vault.azure.net/secrets/token/)\nPage 475 of 1410\n476 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Add Key Vault secrets reference in the Function App configuration.\nSyntax: @Microsoft.KeyVault(SecretUri={copied identifier for the username secret})\nReference:\nhttps://daniel-krzyczkowski.github.io/Integrate-Key-Vault-Secrets-With-Azure-Functions/"
  },
  {
    "number": "201",
    "question": "A company maintains multiple web and mobile applications. Each application\nuses custom in-house identity providers as well as social identity providers.\nYou need to implement single sign-on (SSO) for all the applications.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use Azure Active Directory B2C (Azure AD B2C) with custom policies.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Use Azure Active Directory B2B (Azure AD B2B) and enable external collaboration. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use Azure Active Directory B2C (Azure AD B2C) with user flows.\nPage 476 of 1410\n477 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use Azure Active Directory B2B (Azure AD B2B).",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "with user flows.\" Azure AD B2C is a\ncloud-based identity management service that enables you to customize and control\nhow users sign up, sign in, and manage their profiles when interacting with your\nPage 477 of 1410\n478 Microsoft - AZ-204 Practice Questions - SecExams.com\napplications. With Azure AD B2C, you can create user flows that define the steps in the\nauthentication and authorization process for your applications, and you can use these\nuser flows to enable SSO across your applications. User flows allow you to customize the\nauthentication experience for your users, including specifying the identity providers that\nyou want to use, such as custom in-house identity providers or social identity providers.\nYou can also customize the user interface, branding, and policies that apply to each user\nflow. References: https://learn.microsoft.com/en-us/azure/active-directory-b2c/user-\nflow-overview\nSelected Answer: A\nA - correct answer\nSelected Answer: A\nA for sure\nA https://docs.microsoft.com/en-us/azure/active-directory-b2c/custom-policy-reference-\nsso",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "You can add Google as an identity provider for B2B guest users.\nFederation with SAML/WS-Fed identity providers for guest users.\nMake sure your organization's external collaboration settings are configured such that\nyou're allowed to invite guests.\nNote 1: As a user who is assigned any of the limited administrator directory roles, you can\nuse the Azure portal to invite B2B collaboration users. You can invite guest users to the\ndirectory, to a group, or to an application. After you invite a user through any of these\nmethods, the invited user's account is added to Azure\nActive Directory (Azure AD), with a user type of Guest.\nNote 2: Direct federation in Azure Active Directory is now referred to as SAML/WS-Fed\nidentity provider (IdP) federation.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/external-identities/google-\nfederation https://docs.microsoft.com/en-us/azure/active-directory/external-identities/\nadd-users-administrator"
  },
  {
    "number": "202",
    "question": "You develop a Python application for image rendering that uses GPU resources\nto optimize rendering processes. You deploy the application to an Azure\nContainer Instances (ACI) Linux container.\nThe application requires a secret value to be passed when the container is\nstarted. The value must only be accessed from within the container.\nYou need to pass the secret value.\nWhat are two possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create an environment variable Set the secureValue property to the secret value. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add the secret value to the container image. Use a managed identity.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add the secret value to the application code Set the container startup command.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add the secret value to an Azure Blob storage account. Generate a SAS token.\nE) Mount a secret volume containing the secret value in a secrets file. \nPage 478 of 1410\n479 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "E\nA: Secure environment variables -\nAnother method (another than a secret volume) for providing sensitive information to\ncontainers (including Windows containers) is through the use of secure environment\nvariables.\nE: Use a secret volume to supply sensitive information to the containers in a container\ngroup. The secret volume stores your secrets in files within the volume, accessible by the\ncontainers in the container group. By storing secrets in a secret volume, you can avoid\nadding sensitive data like SSH keys or database credentials to your application code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-\nvolume-secret"
  },
  {
    "number": "203",
    "question": "You are developing a user portal for a company.\nYou need to create a report for the portal that lists information about\nemployees who are subject matter experts for a specific topic. You must ensure\nthat administrators have full control and consent over the data.\nWhich technology should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Microsoft Graph data connect",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Microsoft Graph API",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Microsoft Graph connectors",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Data Connect grants a more granular control and consent model: you can manage data,\nsee who is accessing it, and request specific properties of an entity. This enhances the\nMicrosoft Graph model, which grants or denies applications access to entire entities.\nMicrosoft Graph Data Connect augments Microsoft Graph's transactional model with an\nintelligent way to access rich data at scale. The data covers how workers communicate,\ncollaborate, and manage their time across all the applications and services in Microsoft\n365.\nIncorrect:\nNot B: The Microsoft Graph API is a RESTful web API that enables you to access Microsoft\nCloud service resources. After you register your app and get authentication tokens for a\nuser or service, you can make requests to the Microsoft Graph API.\nA simplistic definition of a Graph API is an API that models the data in terms of nodes\nand edges (objects and relationships) and allows the client to interact with multiple\nnodes in a single request.\nNot C: Microsoft Graph connectors, your organization can index third-party data so that it\nappears in Microsoft Search results.\nWith Microsoft Graph connectors, your organization can index third-party data so that it\nappears in Microsoft Search results.\nReference:\nhttps://docs.microsoft.com/en-us/graph/data-connect-concept-overview"
  },
  {
    "number": "204",
    "question": "HOTSPOT -\nYou are a developer building a web site using a web app. The web site stores\nconfiguration data in Azure App Configuration.\nAccess to Azure App Configuration has been configured to use the identity of the\nweb app for authentication. Security requirements specify that no other\nauthentication systems must be used.\nYou need to load configuration data from Azure App Configuration.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: AddAzureAppConfiguration -\nLoad data from App Configuration, code example:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\n{\nEtc.\nBox 2: ManagedIdentityCredential\n\n\nUse managed identities to access App Configuration\nIf you want to use a user-assigned managed identity, be sure to specify the clientId when\ncreating the ManagedIdentityCredential. config.AddAzureAppConfiguration(options =>\n{\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new\nManagedIdentityCredential(\"<your_clientId>\"))\n});\nFull code sample:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new\nManagedIdentityCredential()));\n})\n.UseStartup<Startup>());\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/howto-integrate-\nazure-managed-service-identity?tabs=core5x&pivots=framework- dotnet\nCommunity Discussion\nAnswer is right. \"Security requirements specify that no other authentication systems\nmust be used\" So you cannot use DefaultAzureCredential, risk of other authentication\nsystems being used. otherwise you could have used DefaultAzureCredential for ease\nGot this in 09/22 , went with the answers given, score 927.\nAns is correct AddAzureAppConfiguration ManagedIdentityCredential\nhttps://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-integrate-\nazure-managed-service-identity?tabs=core6x&pivots=framework-dotnet#use-a-managed-\nidentity\nsecond answer is wrong ->https://learn.microsoft.com/en-us/answers/questions/\n987373/azure-app-configuration-label",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "205",
    "question": "HOTSPOT -\nYou are building an application that stores sensitive customer data in Azure\nBlob storage. The data must be encrypted with a key that is unique for each\ncustomer.\nIf the encryption key has been corrupted it must not be used for encryption.\nYou need to ensure that the blob is encrypted.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: CustomerProvidedKey(key)\nThe data must be encrypted with a key that is unique for each customer.\nSample code:\nasync static Task UploadBlobWithClientKey(Uri blobUri,\nStream data,\nbyte[] key,\nstring keySha256)\n{\n// Create a new customer-provided key.\n// Key must be AES-256.\nvar cpk = new CustomerProvidedKey(key);\nBox 2: Encryption -\nCustomerProvidedKey.EncryptionKey Property\nSample code continued:\n// Check the key's encryption hash.\nif (cpk.EncryptionKeyHash != keySha256)\n{\nthrow new InvalidOperationException(\"The encryption key is corrupted.\");\n}\nBox 3: CustomerProvidedKey -\nSample code continued;\n\n\n// Specify the customer-provided key on the options for the client.\nBlobClientOptions options = new BlobClientOptions()\n{\nCustomerProvidedKey = cpk -\n};\n// Create the client object with options specified.\nBlobClient blobClient = new BlobClient(\nblobUri,\nnew DefaultAzureCredential(),\noptions);\nIncorrect:\n* Version - Gets the BlobClientOptions.ServiceVersion of the service API used when\nmaking requests.\nTransport - The HttpPipelineTransport to be used for this client.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-customer-\nprovided-key\nCommunity Discussion\nThis looks correct! https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-\ncustomer-provided-key\nShouldn't it be EncryptionKeyHash based on the link provided?\nHad this question in today's exam: 2023-07-26 Also the exam did have the value as\nEncryptedKeyHash\nAZ204 should be renamed as Azure .NET/c# Developer\nThe second box probably should be EncryptionKey, CustomerProvidedKey doesn't even\nhas Encryption property, https://docs.microsoft.com/en-us/dotnet/api/\nazure.storage.blobs.models.customerprovidedkey?view=azure-dotnet",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "206",
    "question": "DRAG DROP -\nYou develop a web app that uses the tier D1 app service plan by using the Web\nApps feature of Microsoft Azure App Service.\nSpikes in traffic have caused increases in page load times.\nYou need to ensure that the web app automatically scales when CPU load is\nabout 85 percent and minimize costs.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit\nfor any of the correct orders you select.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Configure the web app to the Standard App Service Tier\nThe Standard tier supports auto-scaling, and we should minimize the cost.\nStep 2: Enable autoscaling on the web app\nFirst enable autoscale -\nStep 3: Add a scale rule -\nStep 4: Add a Scale condition -\nReference:\nhttps://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-\nautoscale-get-started\nCommunity Discussion\nThe Provided Answer is Correct\nthe correct answer is :- Configure the web app to the Standard App Service Tier Enable\nautoscaling on the web app Add a Scale condition Add a scale rule Please refer - https://\ndocs.microsoft.com/en-us/learn/modules/app-service-autoscale-rules/6-autoscale-a-\nweb-app\n\n\nthe correct answer is :- Configure the web app to the Standard App Service Tier Enable\nautoscaling on the web app Add a Scale condition Add a scale rule Please refer - https://\ndocs.microsoft.com/en-us/learn/modules/app-service-autoscale-rules/6-autoscale-a-\nweb-app\nConfigure the web app to the Standard App Service Tier Enable autoscaling on the web\napp Add a Scale condition Add a scale rule\nNo. This is for Monitor auto scale. Question is for Web App auto scale, and for that steps\nwould be as follow: Configure the web app to the Standard App Service Tier Enable\nautoscaling on the web app Add a Scale condition Add a scale rule Reference: https://\ndocs.microsoft.com/en-us/learn/modules/app-service-autoscale-rules/6-autoscale-a-\nweb-app",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "207",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution. Determine\nwhether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure\nApp Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n✑ Share session state across all ASP.NET web applications.\n✑ Support controlled, concurrent access to the same session state data for\nmultiple readers and a single writer.\n✑ Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Enable Application Request Routing (ARR).\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead deploy and configure Azure Cache for Redis. Update the web applications."
  },
  {
    "number": "208",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution. Determine\nwhether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure\nApp Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n✑ Share session state across all ASP.NET web applications.\n✑ Support controlled, concurrent access to the same session state data for\nmultiple readers and a single writer.\n✑ Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Deploy and configure an Azure Database for PostgreSQL.\nUpdate the web applications.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead deploy and configure Azure Cache for Redis. Update the web applications.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching#managing-\nconcurrency-in-a-cache"
  },
  {
    "number": "209",
    "question": "HOTSPOT -\nA company is developing a gaming platform. Users can join teams to play online\nand see leaderboards that include player statistics. The solution includes an\nentity named Team.\nYou plan to implement an Azure Redis Cache instance to improve the efficiency\nof data operations for entities that rarely change.\nYou need to invalidate the cache when team data is changed.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: IDatabase cache = connection.GetDatabase();\nConnection refers to a previously configured ConnectionMultiplexer.\nBox 2: cache.StringSet(\"teams\",\")\nTo specify the expiration of an item in the cache, use the TimeSpan parameter of\nStringSet. cache.StringSet(\"key1\", \"value1\", TimeSpan.FromMinutes(90));\nReference:\nhttps://azure.microsoft.com/sv-se/blog/lap-around-azure-redis-cache-preview/ https://\ndocs.microsoft.com/en-us/cli/azure/webapp/config/container\nCommunity Discussion\nSame question on AZ203 Answer is: IDatabase cache = Connection.GetDatabase();\ncache.KeyDelete(\"teams\") https://www.secexams.com/discussions/microsoft/view/12706-\nexam-az-203-topic-5-question-9-discussion/\nAnswer is incorrect (for the 2nd part). cache.KeyDelete() is the correct method for\nremoving a key from the redis cache.\nhttps://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-web-app-cache-\naside-leaderboard#update-the-teamscontroller-to-read-from-the-cache-or-the-database\n\n\nhttps://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-web-app-cache-\naside-leaderboard#update-the-teamscontroller-to-read-from-the-cache-or-the-database\nhttps://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-web-app-cache-\naside-leaderboard#update-the-teamscontroller-to-read-from-the-cache-or-the-database",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "210",
    "question": "DRAG DROP -\nA company has multiple warehouses. Each warehouse contains IoT temperature\ndevices which deliver temperature data to an Azure Service Bus queue.\nYou need to send email alerts to facility supervisors immediately if the\ntemperature at a warehouse goes above or below specified threshold\ntemperatures.\nWhich five actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Create a blank Logic app.\nCreate and configure a Logic App.\nStep 2: Add a logical app trigger that fires when one or more messages arrive in the\nqueue.\nConfigure the logic app trigger.\nUnder Triggers, select When one or more messages arrive in a queue (auto-complete).\nStep 3: Add an action that reads IoT temperature data from the Service Bus queue\nStep 4: Add a condition that compares the temperature against the upper and lower\nthresholds.\nStep 5: Add an action that sends an email to specified personnel if the temperature is\noutside of those thresholds\nReference:\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitoring-notifications-with-\nazure-logic-apps\nCommunity Discussion\nThe 2nd step sholud be \"logic app trigger\". You need a trigger to fire the logic app Ref:\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitoring-notifications-with-\nazure-logic-apps\nOrder: 1. Create blank logic app 2. Add logic app trigger when one or more messages\narrive in the queue 3. Action to read IoT Temp data 4. Compare upper and lower temp 5.\nSend emails\n\n\nLogic apps are supposed to be out of scope as of March 2021. Can anybody confirm they\nhave gotten this question recently?\nFirst , we need to create a blank logic app. Then we need to add a trigger to the logic app\nwhich will fire when one or more messages arrives in the queue. Then we need an action\nto read the IoT temperature data. Then we need to have a condition that compares the\ntemperature against the upper and lower thresholds. And finally, we have to have an\naction that sends an email to the specified personnel.\nLogic Apps are out of Scope from AZ-204 now !",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "211",
    "question": "DRAG DROP -\nYou develop an ASP.NET Core MVC application. You configure the application to\ntrack webpages and custom events.\nYou need to identify trends in application usage.\nWhich Azure Application Insights Usage Analysis features should you use? To\nanswer, drag the appropriate features to the correct requirements. Each feature\nmay be used once, more than once, or not at all. You may need to drag the split\nbar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Users -\nBox 2: Impact -\nOne way to think of Impact is as the ultimate tool for settling arguments with someone\non your team about how slowness in some aspect of your site is affecting whether users\nstick around. While users may tolerate a certain amount of slowness, Impact gives you\ninsight into how best to balance optimization and performance to maximize user\nconversion.\nBox 3: Retention -\nThe retention feature in Azure Application Insights helps you analyze how many users\nreturn to your app, and how often they perform particular tasks or achieve goals. For\nexample, if you run a game site, you could compare the numbers of users who return to\nthe site after losing a game with the number who return after winning. This knowledge\ncan help you improve both your user experience and your business strategy.\nBox 4: User flows -\nThe User Flows tool visualizes how users navigate between the pages and features of\nyour site. It's great for answering questions like:\n✑ How do users navigate away from a page on your site?\n✑ What do users click on a page on your site?\n✑ Where are the places that users churn most from your site?\n✑ Are there places where users repeat the same action over and over?\nIncorrect Answers:\nFunnel: If your application involves multiple stages, you need to know if most customers\nare progressing through the entire process, or if they are ending the process at some\npoint. The progression through a series of steps in a web application is known as a\nfunnel. You can use Azure Application Insights Funnels to gain insights into your users,\nand monitor step-by-step conversion rates.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-impact\nCommunity Discussion\n\n\n1st one Funnels\n1.Funnels 2.Impact 3.Retention 4.User flow\nOn my exam 2022-12-26. Chose： Funnels Impact Retention User Flows\nOn my exam 2022-12-26. Chose： Funnels Impact Retention User Flows\nThe end-goal of a 'Funnel' is a product purchase. 'Users' is just about how users use your\napp.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "212",
    "question": "You develop a gateway solution for a public facing news API. The news API back\nend is implemented as a RESTful service and uses an OpenAPI specification.\nYou need to ensure that you can access the news API by using an Azure API\nManagement service instance.\nWhich Azure PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "Import-AzureRmApiManagementApi -Context $ApiMgmtContext -SpecificationFormat\n\"Swagger\" -SpecificationPath $SwaggerPath -Path $Path",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "New-AzureRmApiManagementBackend -Context $ApiMgmtContext -Url $Url -Protocol http",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "New-AzureRmApiManagement -ResourceGroupName $ResourceGroup -Name $Name\nLocation $Location -Organization $Org -AdminEmail $AdminEmail\"€ג",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "New-AzureRmApiManagementBackendProxy -Url $ApiUrl",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "New-AzureRmApiManagementBackendProxy creates a new Backend Proxy Object which\ncan be piped when creating a new Backend entity.\nExample: Create a Backend Proxy In-Memory Object\nPS C:\\>$secpassword = ConvertTo-SecureString \"PlainTextPassword\" -AsPlainText -Force\nPS C:\\>$proxyCreds = New-Object System.Management.Automation.PSCredential (\"foo\",\n$secpassword)\nPS C:\\>$credential = New-AzureRmApiManagementBackendProxy -Url \"http://\n12.168.1.1:8080\" -ProxyCredential $proxyCreds\nPS C:\\>$apimContext = New-AzureRmApiManagementContext -ResourceGroupName \"Api-"
  },
  {
    "number": "213",
    "question": "You are creating a hazard notification system that has a single signaling server\nwhich triggers audio and visual alarms to start and stop.\nYou implement Azure Service Bus to publish alarms. Each alarm controller uses\nAzure Service Bus to receive alarm signals as part of a transaction. Alarm events\nmust be recorded for audit purposes. Each transaction record must include\ninformation about the alarm type that was activated.\nYou need to implement a reply trail auditing solution.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Assign the value of the hazard message SessionID property to the ReplyToSessionId\nproperty.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Assign the value of the hazard message MessageId property to the DevileryCount property.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Assign the value of the hazard message SessionID property to the SequenceNumber\nproperty.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Assign the value of the hazard message MessageId property to the CorrelationId property.\n\nE) Assign the value of the hazard message SequenceNumber property to the DeliveryCount\nproperty.\nF) Assign the value of the hazard message MessageId property to the SequenceNumber\nproperty.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D\nD: CorrelationId: Enables an application to specify a context for the message for the\npurposes of correlation; for example, reflecting the MessageId of a message that is being\nreplied to.\nA: ReplyToSessionId: This value augments the ReplyTo information and specifies which\nSessionId should be set for the reply when sent to the reply entity.\nIncorrect Answers:\nB, E: DeliveryCount -\nNumber of deliveries that have been attempted for this message. The count is\nincremented when a message lock expires, or the message is explicitly abandoned by the\nreceiver. This property is read-only."
  },
  {
    "number": "214",
    "question": "You are developing an Azure function that connects to an Azure SQL Database\ninstance. The function is triggered by an Azure Storage queue.\nYou receive reports of numerous System.InvalidOperationExceptions with the\nfollowing message:\n`Timeout expired. The timeout period elapsed prior to obtaining a connection\nfrom the pool. This may have occurred because all pooled connections were in\nuse and max pool size was reached.`\nYou need to prevent the exception.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "In the host.json file, decrease the value of the batchSize option",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Convert the trigger to Azure Event Hub",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Convert the Azure Function to the Premium plan",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "In the function.json file, change the value of the type option to queueScaling",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "With the Premium plan the max outbound connections per instance is unbounded\ncompared to the 600 active (1200 total) in a Consumption plan.\nNote: The number of available connections is limited partly because a function app runs\nin a sandbox environment. One of the restrictions that the sandbox imposes on your\ncode is a limit on the number of outbound connections, which is currently 600 active\n(1,200 total) connections per instance. When you reach this limit, the functions runtime\nwrites the following message to the logs: Host thresholds exceeded: Connections.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/manage-connections https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-scale#service-limits"
  },
  {
    "number": "215",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution. Determine\nwhether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure\nApp Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n✑ Share session state across all ASP.NET web applications.\n✑ Support controlled, concurrent access to the same session state data for\nmultiple readers and a single writer.\n✑ Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Deploy and configure Azure Cache for Redis. Update the web\napplications.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The session state provider for Azure Cache for Redis enables you to share session\ninformation between different instances of an ASP.NET web application.\nThe same connection can be used by multiple concurrent threads.\nRedis supports both read and write operations.\nThe output cache provider for Azure Cache for Redis enables you to save the HTTP\nresponses generated by an ASP.NET web application.\nNote: Using the Azure portal, you can also configure the eviction policy of the cache, and\ncontrol access to the cache by adding users to the roles provided. These roles, which\ndefine the operations that members can perform, include Owner, Contributor, and\nReader. For example, members of the Owner role have complete control over the cache\n(including security) and its contents, members of the Contributor role can read and write\ninformation in the cache, and members of the\nReader role can only retrieve data from the cache.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching"
  },
  {
    "number": "216",
    "question": "HOTSPOT -\nYou are debugging an application that is running on Azure Kubernetes cluster\nnamed cluster1. The cluster uses Azure Monitor for containers to monitor the\ncluster.\nThe application has sticky sessions enabled on the ingress controller.\nSome customers report a large number of errors in the application over the last\n24 hours.\nYou need to determine on which virtual machines (VMs) the errors are\noccurring.\nHow should you complete the Azure Monitor query? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: ago(1d)\nBox 2: distinct containerID -\nBox 3: where ContainerID in (ContainerIDs)\nBox 4: summarize Count by Computer\nSummarize: aggregate groups of rows\nUse summarize to identify groups of records, according to one or more columns, and\napply aggregations to them. The most common use of summarize is count, which returns\nthe number of results in each group.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-queries\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-optimization\n\n\nCommunity Discussion\nthe answer is correct\nYou'll notice it really isn't about kubernetes per say, but how to query your logs from Log\nAnalytics\nYou'll notice it really isn't about kubernetes per say, but how to query your logs from Log\nAnalytics\nIsn't kubernetes out of scope for AZ-204?\nThis was on the exam (July 2023). Went with proposed. Scored 917",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "217",
    "question": "HOTSPOT -\nYou plan to deploy a web app to App Service on Linux. You create an App\nService plan. You create and push a custom Docker image that contains the web\napp to Azure Container Registry.\nYou need to access the console logs generated from inside the container in\nreal-time.\nHow should you complete the Azure CLI command? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: config -\nTo Configure logging for a web app use the command:\naz webapp log config\nBox 2: --docker-container-logging\nSyntax include:\naz webapp log config [--docker-container-logging {filesystem, off}]\nBox 3: webapp -\nTo download a web app's log history as a zip file use the command: az webapp log\ndownload\nBox 4: download -\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/webapp/log\nCommunity Discussion\nIt never said dowload, it says \"access real time\" Will go with TAIL: https://\ndocs.microsoft.com/en-us/cli/azure/webapp/log?view=azure-cli-\nlatest#az_webapp_log_tail\nCorrect Answers are config docker-container-logging webapp tail\nIt says the same for download.\nIt says the same for download.\n\n\nIt says the same for download.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "218",
    "question": "You develop and deploy an ASP.NET web app to Azure App Service. You use\nApplication Insights telemetry to monitor the app.\nYou must test the app to ensure that the app is available and responsive from\nvarious points around the world and at regular intervals. If the app is not\nresponding, you must send an alert to support staff.\nYou need to configure a test for the web app.\nWhich two test types can you use? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "integration",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "multi-step web",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "URL ping",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "unit\nE) load",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nThere are three types of availability tests:\n✑ URL ping test: a simple test that you can create in the Azure portal.\n✑ Multi-step web test: A recording of a sequence of web requests, which can be played\nback to test more complex scenarios. Multi-step web tests are created in\nVisual Studio Enterprise and uploaded to the portal for execution.\n✑ Custom Track Availability Tests: If you decide to create a custom application to run\navailability tests, the TrackAvailability() method can be used to send the results to\nApplication Insights.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-\navailability"
  },
  {
    "number": "219",
    "question": "DRAG DROP -\nA web service provides customer summary information for e-commerce\npartners. The web service is implemented as an Azure Function app with an\nHTTP trigger.\nAccess to the API is provided by an Azure API Management instance. The API\nManagement instance is configured in consumption plan mode. All API calls are\nauthenticated by using OAuth.\nAPI calls must be cached. Customers must not be able to view cached data for\nother customers.\nYou need to configure API Management policies for caching.\nHow should you complete the policy statement?\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: internal -\ncaching-type\n\n\nChoose between the following values of the attribute:\n✑ internal to use the built-in API Management cache,\n✑ external to use the external cache as Azure Cache for Redis prefer-external to use\nexternal cache if configured or internal cache otherwise.\nBox 2: private -\ndownstream-caching-type\nThis attribute must be set to one of the following values.\n✑ none - downstream caching is not allowed.\n✑ private - downstream private caching is allowed.\n✑ public - private and shared downstream caching is allowed.\nBox 3: Authorization -\n<vary-by-header>Authorization</vary-by-header>\n<!-- should be present when allow-private-response-caching is \"true\"-->\nNote: Start caching responses per value of specified header, such as Accept, Accept-\nCharset, Accept-Encoding, Accept-Language, Authorization, Expect, From,\nHost, If-Match -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-\npolicies\nCommunity Discussion\nInternal Cache isn't available for Consumption plan. So shouldn't it be External (using\nRedis)?\nConsumption Pricing Tier of API Management does not support Built-in Cache. Therefore\ncaching-type should be set to \"external\". Reference: https://docs.microsoft.com/en-us/\nazure/api-management/api-management-features\nYes, you are right. In the feature-based comparison of the tiers, it says that built-in cache\nis not available. https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-features And here it says that \"internal\" would be using the built-in cache.\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-\npolicies\nYes, you are right. In the feature-based comparison of the tiers, it says that built-in cache\nis not available. https://docs.microsoft.com/en-us/azure/api-management/api-\n\n\nmanagement-features And here it says that \"internal\" would be using the built-in cache.\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-\npolicies\nGot this one 02/2022. Went with: External Private Authorization",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "220",
    "question": "You are developing applications for a company. You plan to host the\napplications on Azure App Services.\nThe company has the following requirements:\n✑ Every five minutes verify that the websites are responsive.\n✑ Verify that the websites respond within a specified time threshold.\nDependent requests such as images and JavaScript files must load properly.\n✑ Generate alerts if a website is experiencing issues.\n✑ If a website fails to load, the system must attempt to reload the site three\nmore times.\nYou need to implement this process with the least amount of effort.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a Selenium web test and configure it to run from your workstation as a scheduled\ntask.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Set up a URL ping test to query the home page.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure function to query the home page.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a multi-step web test to query the home page. \nE) Create a Custom Track Availability Test to query the home page.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "You can monitor a recorded sequence of URLs and interactions with a website via multi-\nstep web tests.\nIncorrect Answers:\nA: Selenium is an umbrella project for a range of tools and libraries that enable and\nsupport the automation of web browsers.\nIt provides extensions to emulate user interaction with browsers, a distribution server for\nscaling browser allocation, and the infrastructure for implementations of the W3C\nWebDriver specification that lets you write interchangeable code for all major web"
  },
  {
    "number": "221",
    "question": "You develop and add several functions to an Azure Function app that uses the\nlatest runtime host. The functions contain several REST API endpoints secured\nby using SSL. The Azure Function app runs in a Consumption plan.\nYou must send an alert when any of the function endpoints are unavailable or\nresponding too slowly.\nYou need to monitor the availability and responsiveness of the functions.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a URL ping test.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a timer triggered function that calls TrackAvailability() and send the results to\nApplication Insights. \nPage 520 of 1410\n521 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a timer triggered function that calls GetMetric(\"Request Size\") and send the results to\nApplication Insights.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add a new diagnostic setting to the Azure Function app. Enable the FunctionAppLogs and\nSend to Log Analytics options.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "You can create an Azure Function with TrackAvailability() that will run periodically\naccording to the configuration given in TimerTrigger function with your own business\nlogic. The results of this test will be sent to your Application Insights resource, where you\nwill be able to query for and alert on the availability results data.\nThis allows you to create customized tests similar to what you can do via Availability\nMonitoring in the portal. Customized tests will allow you to write more complex\navailability tests than is possible using the portal UI, monitor an app inside of your Azure\nVNET, change the endpoint address, or create an availability test even if this feature is\nnot available in your region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-azure-functions"
  },
  {
    "number": "222",
    "question": "DRAG DROP -\nYou are developing an application to retrieve user profile information. The\napplication will use the Microsoft Graph SDK.\nThe app must retrieve user profile information by using a Microsoft Graph API\ncall.\nYou need to call the Microsoft Graph API from the application.\nIn which order should you perform the actions? To answer, move all actions\nfrom the list of actions to the answer area and arrange them in the correct\norder.\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Register the application with the Microsoft identity platform.\nTo authenticate with the Microsoft identity platform endpoint, you must first register\nyour app at the Azure app registration portal\nStep 2: Build a client by using the client app ID\nStep 3: Create an authentication provider\n\n\nCreate an authentication provider by passing in a client application and graph scopes.\nCode example:\nDeviceCodeProvider authProvider = new DeviceCodeProvider(publicClientApplication,\ngraphScopes);\n// Create a new instance of GraphServiceClient with the authentication provider.\nGraphServiceClient graphClient = new GraphServiceClient(authProvider);\nStep 4: Create a new instance of the GraphServiceClient\nStep 5: Invoke the request to the Microsoft Graph API\nReference:\nhttps://docs.microsoft.com/en-us/graph/auth-v2-service\nhttps://docs.microsoft.com/en-us/graph/sdks/create-client\nCommunity Discussion\nAnswer is correct\nI got this in my exam this morning. The provided answer is correct.\nHahaha, that's so classic, creating acronyms for memorizing stuff like this.\nThe answer is correct - RBCCI\nDid my exam on 15th November 2022. This question was on it.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "223",
    "question": "DRAG DROP -\nYou develop and deploy an Azure Logic App that calls an Azure Function app.\nThe Azure Function App includes an OpenAPI (Swagger) definition and uses an\nAzure Blob storage account. All resources are secured by using Azure Active\nDirectory (Azure AD).\nThe Logic App must use Azure Monitor logs to record and store information\nabout runtime data and events. The logs must be stored in the Azure Blob\nstorage account.\nYou need to set up Azure Monitor logs and collect diagnostics data for the Azure\nLogic App.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Create a Log Analytics workspace\nBefore you start, you need a Log Analytics workspace.\nStep 2: Install the Logic Apps Management solution\nTo set up logging for your logic app, you can enable Log Analytics when you create your\nlogic app, or you can install the Logic Apps Management solution in your\nLog Analytics workspace for existing logic apps.\nStep 3: Add a diagnostic setting to the Azure Logic App\nSet up Azure Monitor logs -\n1. In the Azure portal, find and select your logic app.\n2. On your logic app menu, under Monitoring, select Diagnostic settings > Add diagnostic\nsetting.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/monitor-logic-apps-log-analytics\nCommunity Discussion\nAfter the March 26 changes, Logic apps are out of scope for the AZ-204 exam\nGot this one today. So much for being out of scope. 2023-09-19\nAnswer is correct https://docs.microsoft.com/en-us/azure/logic-apps/monitor-logic-\napps-log-analytics\nI got this on 6/29/2023 and passed with 850. Answer is correct.\nGot this one today. So much for being out of scope. 2023-09-19",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "224",
    "question": "DRAG DROP -\nYou develop an application. You plan to host the application on a set of virtual\nmachines (VMs) in Azure.\nYou need to configure Azure Monitor to collect logs from the application.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Create a Log Analytics workspace.\nFirst create the workspace.\nStep 2: Add a VMInsights solution.\nBefore a Log Analytics workspace can be used with VM insights, it must have the\nVMInsights solution installed.\nStep 3: Install agents on the VM and VM scale set to be monitored.\nPrior to onboarding agents, you must create and configure a workspace. Install or update\nthe Application Insights Agent as an extension for Azure virtual machines and VM scalet\nsets.\nStep 4: Create an Application Insights resource\nSign in to the Azure portal, and create an Application Insights resource.\n\n\nOnce a workspace-based Application Insights resource has been created, configuring\nmonitoring is relatively straightforward.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-configure-\nworkspace https://docs.microsoft.com/en-us/azure/azure-monitor/app/create-\nworkspace-resource\nCommunity Discussion\ncorrect https://docs.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-\nconfigure-workspace?tabs=CLI https://docs.microsoft.com/en-us/azure/azure-monitor/\napp/create-workspace-resource\nWas on my exam today (3rd of Jan 2023)\n\n\nYou must make the workspace in the portal In Monitor you must add Application insights\nwith the workspace In Monitor you must activate VMInsights and add your VM's and\nassign a workspace to it Then install and run the agents in each VM 1. Create Log\nAnalytics workspace (in Azure Portal) 2. Create Application Insights resource (in Monitor,\nApplication Insights with workspace attached) 3.Add VMInsights solution (== activate\nVMInsights in Monitor, choose VM's, attach workspace). 4.Install agents on VM\nConfiguring VM insights The steps to configure VM insights are as follows. Follow each\nlink for detailed guidance on each step: 1.Create Log Analytics workspace. 2.Add\nVMInsights solution to workspace. 3.Install agents on virtual machine and virtual\nmachine scale set to be monitored.\nAfter lots of research ... 1. Log analytics name space 2. Application insights 3. VM solutions\n4. Install agents",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "225",
    "question": "You develop and deploy an Azure App Service web app. The app is deployed to\nmultiple regions and uses Azure Traffic Manager. Application Insights is enabled\nfor the app.\nYou need to analyze app uptime for each month.\nWhich two solutions will achieve the goal? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Azure Monitor logs",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Application Insights alerts",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Monitor metrics",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Application Insights web tests",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nReference:\nhttps://azure.microsoft.com/en-us/blog/creating-a-web-test-alert-programmatically-\nwith-application-insights/"
  },
  {
    "number": "226",
    "question": "DRAG DROP -\nYou develop and deploy an Azure App Service web app. The web app accesses\ndata in an Azure SQL database.\nYou must update the web app to store frequently used data in a new Azure\nCache for Redis Premium instance.\nYou need to implement the Azure Cache for Redis features.\nWhich feature should you implement? To answer, drag the appropriate feature\nto the correct requirements. Each feature may be used once, more than once, or\nnot at all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nReference:\nhttps://www.red-gate.com/simple-talk/development/dotnet-development/overview-of-\nazure-cache-for-redis/ https://docs.microsoft.com/en-us/azure/architecture/best-\npractices/caching\n\n\nCommunity Discussion\nThis is correct! https://docs.microsoft.com/en-us/azure/architecture/best-practices/\ncaching If you look for the right words (related > set | high-performance > channel |\nrecent > list) in documentation, it helps you validate this answer. Ignore the other\ncomments as it only leads to confusion.\nThis was on the exam (July 2023). Went with proposed. Scored 917\nGot it in exam 20/10/2022\nSet: for related items https://learn.microsoft.com/en-us/azure/architecture/best-\npractices/caching#use-tags-to-cross-correlate-cached-items List: to find the most\nrecently accessed items https://learn.microsoft.com/en-us/azure/architecture/best-\npractices/caching#find-recently-accessed-items\nCorrect https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "227",
    "question": "You are developing an ASP.NET Core Web API web service. The web service uses\nAzure Application Insights for all telemetry and dependency tracking. The web\nservice reads and writes data to a database other than Microsoft SQL Server.\nYou need to ensure that dependency tracking works for calls to the third-party\ndatabase.\nWhich two dependency telemetry properties should you use? Each correct\nanswer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Telemetry.Context.Cloud.RoleInstance",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Telemetry.Id",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Telemetry.Name",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Telemetry.Context.Operation.Id \nE) Telemetry.Context.Session.Id",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "228",
    "question": "HOTSPOT -\nYou are using Azure Front Door Service.\nYou are expecting inbound files to be compressed by using Brotli compression.\nYou discover that inbound XML files are not compressed. The files are 9\nmegabytes (MB) in size.\nYou need to determine the root cause for the issue.\nTo answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nFront Door can dynamically compress content on the edge, resulting in a smaller and\nfaster response to your clients. All files are eligible for compression.\nHowever, a file must be of a MIME type that is eligible for compression list.\nBox 2: No -\nSometimes you may wish to purge cached content from all edge nodes and force them\nall to retrieve new updated assets. This might be due to updates to your web application,\nor to quickly update assets that contain incorrect information.\nBox 3: Yes -\nThese profiles support the following compression encodings: Gzip (GNU zip), Brotli\nReference:\nhttps://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching\nCommunity Discussion\n1.Yes 2.No Refer - https://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching\n,You don’t need to purge the cache assets. Here the issue is that the file size needs to be\nless than 8MB 3.Yes\napplication/xml is a MIME type and therefore, is supported. Answer 1 should be yes.\nWhy are the admins not correcting the answers having gone thru the highly voted\ndiscussion comments!!!\n\n\nCompression isn't but the answer states \"compression TYPE\". And Brotli is supported.\nCompression isn't but the answer states \"compression TYPE\". And Brotli is supported.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "229",
    "question": "HOTSPOT -\nYou are developing an Azure App Service hosted ASP.NET Core web app to\ndeliver video-on-demand streaming media. You enable an Azure Content\nDelivery\nNetwork (CDN) Standard for the web endpoint. Customer videos are\ndownloaded from the web app by using the following example URL: http://\nwww.contoso.com/ content.mp4?quality=1.\nAll media content must expire from the cache after one hour. Customer videos\nwith varying quality must be delivered to the closest regional point of presence\n(POP) node.\nYou need to configure Azure CDN caching rules.\nWhich options should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Override -\nOverride: Ignore origin-provided cache duration; use the provided cache duration\ninstead. This will not override cache-control: no-cache.\nSet if missing: Honor origin-provided cache-directive headers, if they exist; otherwise,\nuse the provided cache duration.\nIncorrect:\nBypass cache: Do not cache and ignore origin-provided cache-directive headers.\nBox 2: 1 hour -\nAll media content must expire from the cache after one hour.\nBox 3: Cache every unique URL -\nCache every unique URL: In this mode, each request with a unique URL, including the\nquery string, is treated as a unique asset with its own cache. For example, the response\nfrom the origin server for a request for example.ashx?q=test1 is cached at the POP node\nand returned for subsequent caches with the same query string. A request for\nexample.ashx?q=test2 is cached as a separate asset with its own time-to-live setting.\nIncorrect Answers:\n\n\nBypass caching for query strings: In this mode, requests with query strings are not\ncached at the CDN POP node. The POP node retrieves the asset directly from the origin\nserver and passes it to the requestor with each request.\nIgnore query strings: Default mode. In this mode, the CDN point-of-presence (POP) node\npasses the query strings from the requestor to the origin server on the first request and\ncaches the asset. All subsequent requests for the asset that are served from the POP\nignore the query strings until the cached asset expires.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-query-string\nCommunity Discussion\nGiven Answers are correct\nBox 1: Override Override: Ignore origin-provided cache duration; use the provided cache\nduration instead. This will not override cache-control: no-cache. Set if missing: Honor\norigin-provided cache-directive headers, if they exist; otherwise, use the provided cache\nduration. Bypass cache: Do not cache and ignore origin-provided cache-directive\nheaders. Box 2: 1 hour All media content must expire from the cache after one hour.\nBox 3: Cache every unique URL Cache every unique URL: In this mode, each request with\na unique URL, including the query string, is treated as a unique asset with its own cache.\nFor example, the response from the origin server for a request for example.ashx?q=test1\nis cached at the POP node and returned for subsequent caches with the same query\nstring. A request for example.ashx?q=test2 is cached as a separate asset with its own\ntime-to-live setting. Bypass caching for query strings: In this mode, requests with query\nstrings are not cached at the CDN POP node. The POP node retrieves the asset directly\nfrom the origin server and passes it to the requestor with each request. Ignore query\nstrings: Default mode. In this mode, the CDN point-of-presence (POP) node passes the\nquery strings from the requestor to the origin server on the first request and caches the\nasset. All subsequent requests for the asset that are served from the POP ignore the\nquery strings until the cached asset expires.\nBox 3: Cache every unique URL Cache every unique URL: In this mode, each request with\na unique URL, including the query string, is treated as a unique asset with its own cache.\nFor example, the response from the origin server for a request for example.ashx?q=test1\nis cached at the POP node and returned for subsequent caches with the same query\nstring. A request for example.ashx?q=test2 is cached as a separate asset with its own\ntime-to-live setting. Bypass caching for query strings: In this mode, requests with query\nstrings are not cached at the CDN POP node. The POP node retrieves the asset directly\nfrom the origin server and passes it to the requestor with each request. Ignore query\n\n\nstrings: Default mode. In this mode, the CDN point-of-presence (POP) node passes the\nquery strings from the requestor to the origin server on the first request and caches the\nasset. All subsequent requests for the asset that are served from the POP ignore the\nquery strings until the cached asset expires.\n1. Override - because we are explicitly defining the duration to be 1 hour . We DO NEED\nCache ( hence it cannot be bypass cache) We need EXACT 1 HOUR ( so can't use Set If\nMissing which could change the specific value) 2. 1 Hour as required 3. We do need to\n'Cache every unique url' because we need to provide cache for different qualities that is\nspecified in the query parameter.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "230",
    "question": "HOTSPOT -\nYou are developing an ASP.NET Core time sheet application that runs as an\nAzure Web App. Users of the application enter their time sheet information on\nthe first day of every month.\nThe application uses a third-party web service to validate data.\nThe application encounters periodic server errors due to errors that result from\ncalling a third-party web server. Each request to the third-party server has the\nsame chance of failure.\nYou need to configure an Azure Monitor alert to detect server errors unrelated\nto the third-party service. You must minimize false-positive alerts.\nHow should you complete the Azure Resource Manager template? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: DynamicThresholdCriterion\nBox 2: Http5xx -\nServer errors are in the 5xx range.\nClient errors are in the 4xx range\nBox 3: Low -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-dynamic-\nthresholds\nCommunity Discussion\nDynamicThresholdCriterion Http5XX Low\nI think sensitivity should be low due to \"false positive\" requirements, if we set to high we\nreport on every smallest deviation, and it means there is a highly chance of false positive\nI think sensitivity should be low due to \"false positive\" requirements, if we set to high we\nreport on every smallest deviation, and it means there is a highly chance of false positive\n\n\nDynamicThresholdCriterion: Dynamic thresholds in metric alerts use advanced machine\nlearning to learn metric Http5XX, just a name , we are monitoring server errors, so 500\nrange error reponses AlertSensitvity : would set is to high. \"You must minimize false-\npositive alerts\" The thresholds will be tight and close to the metric series pattern. An\nalert rule will be triggered on the smallest deviation, resulting in more alerts.\nAgree. Definition according to MS: High: The thresholds will be tight and close to the\nmetric series pattern. An alert rule will be triggered on the smallest deviation, resulting\nin more alerts. Medium: The thresholds will be less tight and more balanced. There will\nbe fewer alerts than with high sensitivity (default). Low: The thresholds will be loose with\nmore distance from metric series pattern. An alert rule will only trigger on large\ndeviations, resulting in fewer alerts.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "231",
    "question": "You are developing a web application that uses Azure Cache for Redis. You\nanticipate that the cache will frequently fill and that you will need to evict keys.\nYou must configure Azure Cache for Redis based on the following predicted\nusage pattern: A small subset of elements will be accessed much more often\nthan the rest.\nYou need to configure the Azure Cache for Redis to optimize performance for\nthe predicted usage pattern.\nWhich two eviction policies will achieve the goal?\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "noeviction",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "allkeys-lru",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "volatile-lru",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "allkeys-random\nE) volatile-ttl\nF) volatile-random",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nB: The allkeys-lru policy evict keys by trying to remove the less recently used (LRU) keys\nfirst, in order to make space for the new data added. Use the allkeys-lru policy when you\nexpect a power-law distribution in the popularity of your requests, that is, you expect"
  },
  {
    "number": "232",
    "question": "DRAG DROP -\nAn organization has web apps hosted in Azure.\nThe organization wants to track events and telemetry data in the web apps by\nusing Application Insights.\nYou need to configure the web apps for Application Insights.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Create an Application Insights resource\nCreating an Application Insights workspace-based resource us a prerequisite.\nStep 2: Copy the connection string\nA connection string identifies the resource that you want to associate with your\ntelemetry data. It also allows you to modify the endpoints that your resource will use as\n\n\na destination for your telemetry. You'll need to copy the connection string and add it to\nyour application's code or to an environment variable.\nStep 3: Configure the Application Insights SDK in the app\nThe Application Insights SDK for ASP.NET Core can monitor your applications no matter\nwhere or how they run.\nInstall the Application Insights SDK NuGet package for ASP.NET Core.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-core\nCommunity Discussion\nThe given answer is correct. 1. Create an Application Insights resource 2. Copy the\ninstrumentation key 3. Install the SDK in your app https://learn.microsoft.com/en-us/\nazure/azure-monitor/app/create-new-resource\nWhy would you make a Machine Learning Workspace LOL\nGot it in exam 20/10/2022\n1. Create resource 2. install Application insight SDK 3. setup connection string according\nto provided link: https://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-\ncore\nbad question, connection string != instrumentation key",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "233",
    "question": "An organization hosts web apps in Azure. The organization uses Azure Monitor.\nYou discover that configuration changes were made to some of the web apps.\nYou need to identify the configuration changes.\nWhich Azure Monitor log should you review?",
    "options": [
      {
        "letter": "A",
        "text": "AppServiceAppLogs",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "AppServiceEnvironmentPlatformlogs",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "AppServiceConsoleLogs",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "AppServiceAuditLogs\nPage 546 of 1410\n547 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The log type AppServiceEnvironmentPlatformLogs handles the App Service Environment:\nscaling, configuration changes, and status logs.\nIncorrect:\nAppServiceAppLogs contains logs generated through your application.\nAppServiceAuditLogs logs generated when publishing users successfully log on via one of\nthe App Service publishing protocols.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs"
  },
  {
    "number": "234",
    "question": "You develop and deploy an Azure App Service web app to a production\nenvironment. You enable the Always On setting and the Application Insights site\nextensions.\nYou deploy a code update and receive multiple failed requests and exceptions\nin the web app.\nYou need to validate the performance and failure counts of the web app in near\nreal time.\nWhich Application Insights tool should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Profiler",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Smart Detection",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Live Metrics Stream",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Application Map\nE) Snapshot Debugger",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Live Metrics Stream -\nDeploying the latest build can be an anxious experience. If there are any problems, you\nwant to know about them right away, so that you can back out if necessary. Live Metrics\nStream gives you key metrics with a latency of about one second.\nWith Live Metrics Stream, you can:\n* Validate a fix while it's released, by watching performance and failure counts.\n* Etc."
  },
  {
    "number": "235",
    "question": "HOTSPOT -\nYou deploy an ASP.NET web app to Azure App Service.\nYou must monitor the web app by using Application Insights.\nYou need to configure Application Insights to meet the requirements.\nWhich feature should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Smart Detection -\nSmart detection automatically warns you of potential performance problems and failure\nanomalies in your web application. It performs proactive analysis of the telemetry that\nyour app sends to Application Insights. If there is a sudden rise in failure rates, or\nabnormal patterns in client or server performance, you get an alert. This feature needs\nno configuration. It operates if your application sends enough telemetry.\nBox 2: Snapshot Debugger -\nWhen an exception occurs, you can automatically collect a debug snapshot from your\nlive web application. The snapshot shows the state of source code and variables at the\nmoment the exception was thrown. The Snapshot Debugger in Azure Application Insights\nmonitors exception telemetry from your web app. It collects snapshots on your top-\nthrowing exceptions so that you have the information you need to diagnose issues in\nproduction.\nBox 3: Profiler -\nAzure Application Insights Profiler provides performance traces for applications running\nin production in Azure. Profiler:\nCaptures the data automatically at scale without negatively affecting your users.\nHelps you identify the ג€hotג€ code path spending the most time handling a particular\nweb request.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/proactive-diagnostics\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/snapshot-debugger/snapshot-\ndebugger https://docs.microsoft.com/en-us/azure/azure-monitor/profiler/profiler-\noverview\n\n\nCommunity Discussion\nAnswer is correct\n1.Smart detection 2.Snapshot 3.user profile\nOn my exam 2022-12-26. Chose：Given Answer (1、2、3)\nOn my exam 2022-12-26. Chose：Given Answer (1、2、3)\nDid my exam on 15th November 2022. This question was on it.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "236",
    "question": "You are building a web application that performs image analysis on user photos\nand returns metadata containing objects identified. The image analysis is very\ncostly in terms of time and compute resources. You are planning to use Azure\nRedis Cache so duplicate uploads do not need to be reprocessed.\nIn case of an Azure data center outage, metadata loss must be kept to a\nminimum.\nYou need to configure the Azure Redis cache instance.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Configure Azure Redis with AOF persistence.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure Azure Redis with RDB persistence.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Configure second storage account for persistence.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set backup frequency to the minimum value.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nRDB persistence - When you use RDB persistence, Azure Cache for Redis persists a"
  },
  {
    "number": "237",
    "question": "You are developing an Azure-based web application. The application goes\noffline periodically to perform offline data processing. While the application is\noffline, numerous Azure Monitor alerts fire which result in the on-call developer\nbeing paged.\nThe application must always log when the application is offline for any reason.\nYou need to ensure that the on-call developer is not paged during offline\nprocessing.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Add Azure Monitor alert processing rules to suppress notifications.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Disable Azure Monitor Service Health Alerts during offline processing.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure Monitor Metric Alert.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Build an Azure Monitor action group that suppresses the alerts.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "You can use alert processing rules to add action groups or remove (suppress) action\ngroups from your fired alerts.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-action-rules"
  },
  {
    "number": "238",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data\nfrom 2,000 stores located throughout the world. A single device can produce 2\nmegabytes (MB) of data every 24 hours. Each store location has one to five\ndevices that send data.\nYou must store the device data in Azure Blob storage. Device data must be\ncorrelated based on a device identifier. Additional stores are expected to open\nin the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Service Bus. Configure a topic to receive the device\ndata by using a correlation filter.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes \nPage 556 of 1410\n557 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "A message is raw data produced by a service to be consumed or stored elsewhere. The\nService Bus is for high-value enterprise messaging, and is used for order processing and\nfinancial transactions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services"
  },
  {
    "number": "239",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data\nfrom 2,000 stores located throughout the world. A single device can produce 2\nmegabytes (MB) of data every 24 hours. Each store location has one to five\ndevices that send data.\nYou must store the device data in Azure Blob storage. Device data must be\ncorrelated based on a device identifier. Additional stores are expected to open\nin the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Grid. Configure event filtering to evaluate the\ndevice identifier.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead use an Azure Service Bus, which is used order processing and financial\ntransactions.\nNote: An event is a lightweight notification of a condition or a state change. Event hubs is\nusually used reacting to status changes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services"
  },
  {
    "number": "240",
    "question": "DRAG DROP -\nYou manage several existing Logic Apps.\nYou need to change definitions, add new logic, and optimize these apps on a\nregular basis.\nWhat should you use? To answer, drag the appropriate tools to the correct\nfunctionalities. Each tool may be used once, more than once, or not at all. You\nmay need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: Enterprise Integration Pack\nFor business-to-business (B2B) solutions and seamless communication between\norganizations, you can build automated scalable enterprise integration workflows by\nusing the Enterprise Integration Pack (EIP) with Azure Logic Apps.\nBox 2: Code View Editor -\nEdit JSON - Azure portal -\n1. Sign in to the Azure portal.\n2. From the left menu, choose All services. In the search box, find \"logic apps\", and then\nfrom the results, select your logic app.\n3. On your logic app's menu, under Development Tools, select Logic App Code View.\n4. The Code View editor opens and shows your logic app definition in JSON format.\nBox 3: Logic Apps Designer -\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-\noverview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-\ndefinitions\nCommunity Discussion\nstart from march 2021 no logic apps in the exam!\nThe answer is correct\nCorrect answer ?\n\n\nThat's correct Answer\nHope to get this question in my exam.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "241",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?\nA.\nB.\nC.\nD.\nExplanation\nCorrect Answer:\nA\nA service bus instance has already been created (Step 2 below). Next is step 3, Create a\n\n\nService Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name\nnamespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-\ngroup $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name\n$namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-\ngroup $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-\ncli\nCommunity Discussion\nI think the Service Bus has already been created and now they ask you to complete the\nconfiguration. The next step is creating the queue. In fact, all the steps are shown: B.\nCreate group. C. Create Service Bus. A. Create Queue. <-- Correct answer. D. Get\nconnectionstring.\nIt Ask for create an Azure Service Bus instance by providing not the Service bus Queue So\ncorrect Answes should be C\nGot this question at the exam and scored 100% on Azure Storage, so I'm sure this is\ncorrect.\nGot this question at the exam and scored 100% on Azure Storage, so I'm sure this is\ncorrect.\nI'm really sorry, Service Bus falls in another section where I scored 85%, so I can't be sure\nif this is the correct answer.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "242",
    "question": "HOTSPOT -\nYou are developing an application that uses Azure Storage Queues.\nYou have the following code:\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nThe QueueDescription.LockDuration property gets or sets the duration of a peek lock;\nthat is, the amount of time that the message is locked for other receivers.\nThe maximum value for LockDuration is 5 minutes; the default value is 1 minute.\nBox 2: Yes -\nYou can peek at the message in the front of a queue without removing it from the queue\nby calling the PeekMessage method.\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-\nqueues https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.servicebus.messaging.queuedescription.lockduration\nCommunity Discussion\nGiven answer is correct\nGetMessageAsync: Gets a message from the queue using the default request options.\nThis operation marks the retrieved message as invisible in the queue for the default\nvisibility timeout period. Only marks the message is invisible but does not delete.\nAnswer: No Yes Yes\npeek: get the message, don't lock, don't delete get: get the message, lock the message\n(make it invisible for some seconds) delete: delete the message intention is: if get would\nalso delete the message from the queue, an error in the function would render the\nmessage unhandled. Thus first get, process than delete. \"at least handled once\"\n\n\npeek: get the message, don't lock, don't delete get: get the message, lock the message\n(make it invisible for some seconds) delete: delete the message intention is: if get would\nalso delete the message from the queue, an error in the function would render the\nmessage unhandled. Thus first get, process than delete. \"at least handled once\"",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "243",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?\nA.\nB.\nC.\nD.\nExplanation\nCorrect Answer:\nC\n\n\nA service bus instance has already been created (Step 2 below). Next is step 3, Create a\nService Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name\nnamespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-\ngroup $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name\n$namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-\ngroup $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-\ncli\nCommunity Discussion\nQuestion is poorly worded. I think what is asked here is that service bus instance is\nalready created and now you need to complete the configuration to start using the bus.\nIn this case, you will need to create Queue and hence correct answer is C\nThis question is equal to the",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "9",
    "question": "to see my answer why we should create the queue.\nC is correct answer.\nGot this 2023July13. Went with given answer. score 917\nAgree with the accepted answer but given question 9, this seems less like it is testing\nyour knowledge and more an attempt to trick you.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "244",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data\nwhen it receives a message from a mobile application. Messages may not be\nsent to the service consistently.\nYou have the following requirements:\n✑ Queue size must not grow larger than 80 gigabytes (GB).\n✑ Use first-in-first-out (FIFO) ordering of messages.\n✑ Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Storage Queue from the\nmobile application. Create an Azure Function App that uses an Azure Storage\nQueue trigger.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Create an Azure Function App that uses an Azure Service Bus Queue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-\nqueue-triggered-function"
  },
  {
    "number": "245",
    "question": "DRAG DROP -\nYou develop software solutions for a mobile delivery service. You are\ndeveloping a mobile app that users can use to order from a restaurant in their\narea. The app uses the following workflow:\n1. A driver selects the restaurants for which they will deliver orders.\n2. Orders are sent to all available drivers in an area.\n3. Only orders for the selected restaurants will appear for the driver.\n4. The first driver to accept an order removes it from the list of available orders.\nYou need to implement an Azure Service Bus solution.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Create a single Service Bus Namespace\nTo begin using Service Bus messaging entities in Azure, you must first create a\nnamespace with a name that is unique across Azure. A namespace provides a scoping\ncontainer for addressing Service Bus resources within your application.\nBox 2: Create a Service Bus Topic for each restaurant for which a driver can receive\nmessages.\nCreate topics.\nBox 3: Create a Service Bus subscription for each restaurant for which a driver can\nreceive orders.\nTopics can have multiple, independent subscriptions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-\noverview\nCommunity Discussion\nCreate a single Service Bus Namespace. Create a single Service Bus Topic. Create a\nService Bus subscription for each restaurant for which a driver can receive orders.\nIn fact all the drivers, who have subscribed to restaurant will get notification. The first\ndriver, who accepts it, will click the order, and it shall be dequeued.\nIn fact all the drivers, who have subscribed to restaurant will get notification. The first\ndriver, who accepts it, will click the order, and it shall be dequeued.\nIn fact all the drivers, who have subscribed to restaurant will get notification. The first\ndriver, who accepts it, will click the order, and it shall be dequeued.\nIn fact all the drivers, who have subscribed to restaurant will get notification. The first\ndriver, who accepts it, will click the order, and it shall be dequeued.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "246",
    "question": "HOTSPOT -\nYou develop a news and blog content app for Windows devices.\nA notification must arrive on a user's device when there is a new article\navailable for them to view.\nYou need to implement push notifications.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: NotificationHubClient -\nBox 2: NotificationHubClient -\nBox 3: CreateClientFromConnectionString\n// Initialize the Notification Hub\nNotificationHubClient hub =\nNotificationHubClient.CreateClientFromConnectionString(listenConnString, hubName);\nBox 4: SendWindowsNativeNotificationAsync\nSend the push notification.\nvar result = await hub.SendWindowsNativeNotificationAsync(windowsToastPayload);\nReference:\nhttps://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-push-\nnotification-registration-management https://github.com/MicrosoftDocs/azure-docs/\nblob/master/articles/app-service-mobile/app-service-mobile-windows-store-dotnet-\nget-started-push.md\nCommunity Discussion\n\n\nAnswer is correct.\nThe answer is correct, but the first reference is not very helpful and the second one\ndoesn't even exists. Here are references of the methods used in the answer: https://\ndocs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.notificationhubs.notificationhubclient.createclientfromconnectionstring?\nview=azure-\ndotnet#Microsoft_Azure_NotificationHubs_NotificationHubClient_CreateClientFromConnectionString_System_String_System_String_\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.notificationhubs.notificationhubclient.sendwindowsnativenotificationasync?\nview=azure-\ndotnet#Microsoft_Azure_NotificationHubs_NotificationHubClient_SendWindowsNativeNotificationAsync_System_String_\n1- NotificationHubClient 2 - NotificationHubClient 3- CreateClientFromConnectionString 4-\nSendWindowsNativeNotificationAsync\nThis the GitHub link https://github.com/uglide/azure-content/blob/master/articles/app-\nservice-mobile/app-service-mobile-windows-store-dotnet-get-started-push.md\nIs this one still a valid question for AZ-204? Didn't see anything about Notification Hubs\nin study guide anymore.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "247",
    "question": "You are developing an Azure messaging solution.\nYou need to ensure that the solution meets the following requirements:\n✑ Provide transactional support.\n✑ Provide duplicate detection.\n✑ Store the messages for an unlimited period of time.\nWhich two technologies will meet the requirements? Each correct answer\npresents a complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Azure Service Bus Topic",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure Service Bus Queue",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Storage Queue",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Event Hub\nPage 573 of 1410\n574 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B\nThe Azure Service Bus Queue and Topic has duplicate detection.\nEnabling duplicate detection helps keep track of the application-controlled MessageId of\nall messages sent into a queue or topic during a specified time window.\nIncorrect Answers:\nC: There is just no mechanism that can query a Storage queue and find out if a message\nwith the same contents is already there or was there before.\nD: Azure Event Hub does not have duplicate detection\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/duplicate-detection"
  },
  {
    "number": "248",
    "question": "DRAG DROP -\nYou develop a gateway solution for a public facing news API.\nThe news API back end is implemented as a RESTful service and hosted in an\nAzure App Service instance.\nYou need to configure back-end authentication for the API Management service\ninstance.\nWhich target and gateway credential type should you use? To answer, drag the\nappropriate values to the correct parameters. Each value may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: Azure Resource -\n\n\nBox 2: Client cert -\nAPI Management allows to secure access to the back-end service of an API using client\ncertificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-\napi-management-rest-api-backend-entity\nCommunity Discussion\n#1 Http ?\nAnswer : 1. Https(s) endpoint, 2. Client cert Reference : https://docs.microsoft.com/en-us/\nazure/api-management/api-management-howto-mutual-certificates#configure-an-api-\nto-use-client-certificate-for-gateway-authentication\nYou might be right: https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-howto-mutual-certificates#configure-an-api-to-use-client-certificate-for-\ngateway-authentication\nYou might be right: https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-howto-mutual-certificates#configure-an-api-to-use-client-certificate-for-\ngateway-authentication\nagreed 1. Https(s) endpoint, 2. Client cert",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "249",
    "question": "HOTSPOT -\nYou are creating an app that uses Event Grid to connect with other services.\nYour app's event data will be sent to a serverless function that checks\ncompliance.\nThis function is maintained by your company.\nYou write a new event subscription at the scope of your resource. The event\nmust be invalidated after a specific period of time.\nYou need to configure Event Grid.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: SAS tokens -\nCustom topics use either Shared Access Signature (SAS) or key authentication. Microsoft\nrecommends SAS, but key authentication provides simple programming, and is\ncompatible with many existing webhook publishers.\nIn this case we need the expiration time provided by SAS tokens.\nBox 2: ValidationCode handshake -\nEvent Grid supports two ways of validating the subscription: ValidationCode handshake\n(programmatic) and ValidationURL handshake (manual).\nIf you control the source code for your endpoint, this method is recommended.\nIncorrect Answers:\nValidationURL handshake (manual): In certain cases, you can't access the source code of\nthe endpoint to implement the ValidationCode handshake. For example, if you use a\nthird-party service (like Zapier or IFTTT), you can't programmatically respond with the\nvalidation code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/security-authentication\nCommunity Discussion\nDropdowns placement in Answer area is incorrect WebHook Event Delivery ---\nValidationCode handsShake Topic publishing --- SAS Tokens https://docs.microsoft.com/\nen-us/azure/event-grid/concepts https://docs.microsoft.com/en-us/azure/event-grid/\nwebhook-event-delivey\n\n\nAgreed and your last link is not working so here: https://docs.microsoft.com/en-us/\nazure/event-grid/webhook-event-delivery\ngot this question today, go with this answer - 7/30/2023, score 895/1000\nAgreed and your last link is not working so here: https://docs.microsoft.com/en-us/\nazure/event-grid/webhook-event-delivery\ngot this question today, go with this answer - 7/30/2023, score 895/1000",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "250",
    "question": "HOTSPOT -\nYou are working for Contoso, Ltd.\nYou define an API Policy object by using the following XML markup:\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Yes -\nUse the set-backend-service policy to redirect an incoming request to a different\nbackend than the one specified in the API settings for that operation. Syntax:\n<set-backend-service base-url=\"base URL of the backend service\" />\nBox 2: No -\nThe condition is on 512k, not on 256k.\nBox 3: No -\nThe set-backend-service policy changes the backend service base URL of the incoming\nrequest to the one specified in the policy.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-\ntransformation-policies\nCommunity Discussion\nAnswer: Yes No No\nIt's doing nothing in the When condition. So if msg is <512, it does nothing. The otherwise\ngets executed only for >512 ??? So it will set backend service only for msgs >512? Question\ndoesn't talk about this case. Also, like someone said in the other discussion for this\nquestion: technically >256k can be >512k Poorly framed question!!!\nI would say \"it depends\" for the third question. Since the set-backend-service policy is in\nthe otherwise block, it would only rewrite the URL for requests with a body size of\n>=512000. Or am I missing a clue here?\n\n\nI think that no action in <when> means nothing needs to be changed (for requests with\nbody < 512000) so the back-end service URL remains as specified in the policy. And only\nlarge requests (> 512000) will be redirected to API 9.1 using <set-backend-service> in\n<otherwise> branch. https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-transformation-policies#SetBackendService\nI think that no action in <when> means nothing needs to be changed (for requests with\nbody < 512000) so the back-end service URL remains as specified in the policy. And only\nlarge requests (> 512000) will be redirected to API 9.1 using <set-backend-service> in\n<otherwise> branch. https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-transformation-policies#SetBackendService",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "251",
    "question": "You are developing a solution that will use Azure messaging services.\nYou need to ensure that the solution uses a publish-subscribe model and\neliminates the need for constant polling.\nWhat are two possible ways to achieve the goal? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Service Bus",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Event Hub",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Event Grid",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Queue",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C\nIt is strongly recommended to use available messaging products and services that\nsupport a publish-subscribe model, rather than building your own. In Azure, consider\nusing Service Bus or Event Grid. Other technologies that can be used for pub/sub\nmessaging include Redis, RabbitMQ, and Apache Kafka.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber"
  },
  {
    "number": "252",
    "question": "A company is implementing a publish-subscribe (Pub/Sub) messaging\ncomponent by using Azure Service Bus. You are developing the first subscription\napplication.\nIn the Azure portal you see that messages are being sent to the subscription for\neach topic. You create and initialize a subscription client object by supplying\nthe correct details, but the subscription application is still not consuming the\nmessages.\nYou need to ensure that the subscription client processes all messages.\nWhich code segment should you use?",
    "options": [
      {
        "letter": "A",
        "text": "await subscriptionClient.AddRuleAsync(new\nRuleDescription(RuleDescription.DefaultRuleName, new TrueFilter()));",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "subscriptionClient = new SubscriptionClient(ServiceBusConnectionString, TopicName,\nSubscriptionName);",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "await subscriptionClient.CloseAsync();",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "subscriptionClient.RegisterMessageHandler(ProcessMessagesAsync,\nPage 582 of 1410\n583 Microsoft - AZ-204 Practice Questions - SecExams.com\nmessageHandlerOptions);",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Using topic client, call RegisterMessageHandler which is used to receive messages\ncontinuously from the entity. It registers a message handler and begins a new thread to\nreceive messages. This handler is waited on every time a new message is received by the\nreceiver. subscriptionClient.RegisterMessageHandler(ReceiveMessagesAsync,\nmessageHandlerOptions);\nReference:\nhttps://www.c-sharpcorner.com/article/azure-service-bus-topic-and-subscription-pub-\nsub/"
  },
  {
    "number": "253",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data\nwhen it receives a message from a mobile application. Messages may not be\nsent to the service consistently.\nYou have the following requirements:\n✑ Queue size must not grow larger than 80 gigabytes (GB).\n✑ Use first-in-first-out (FIFO) ordering of messages.\n✑ Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Storage Queue from the\nmobile application. Create an Azure VM that is triggered from Azure Storage\nQueue events.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus\nQueue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-\nqueue-triggered-function"
  },
  {
    "number": "254",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data\nwhen it receives a message from a mobile application. Messages may not be\nsent to the service consistently.\nYou have the following requirements:\n✑ Queue size must not grow larger than 80 gigabytes (GB).\n✑ Use first-in-first-out (FIFO) ordering of messages.\n✑ Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Service Bus Queue from\nthe mobile application. Create an Azure Windows VM that is triggered from\nAzure Service Bus Queue.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes\nPage 585 of 1410\n586 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus\nQueue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-\nqueue-triggered-function"
  },
  {
    "number": "255",
    "question": "DRAG DROP -\nYou are developing a REST web service. Customers will access the service by\nusing an Azure API Management instance.\nThe web service does not correctly handle conflicts. Instead of returning an\nHTTP status code of 409, the service returns a status code of 500. The body of\nthe status message contains only the word conflict.\nYou need to ensure that conflicts produce the correct response.\nHow should you complete the policy? To answer, drag the appropriate code\nsegments to the correct locations. Each code segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: on-error -\nPolicies in Azure API Management are divided into inbound, backend, outbound, and on-\nerror.\nIf there is no on-error section, callers will receive 400 or 500 HTTP response messages if\nan error condition occurs.\nBox 2: context -\nBox 3: context -\nBox 4: set-status -\nThe return-response policy aborts pipeline execution and returns either a default or\ncustom response to the caller. Default response is 200 OK with no body.\nCustom response can be specified via a context variable or policy statements.\nSyntax:\n<return-response response-variable-name=\"existing context variable\">\n<set-header/>\n<set-body/>\n<set-status/>\n</return-response>\nBox 5: on-error -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-error-\nhandling-policies https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-transformation-policies\nCommunity Discussion\n\n\nJust want to add that \"set-status\" also has code and reason (e.g. set-status code=\"409\"\nreason=\"Conflict\"). https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-advanced-policies#SetStatus\nTHE ANSWER IS CORRECT.\nThanks. set-status on it's own seemed a bit strange. Minor detail: Last on-error must be\npreceded by a slash.\nThanks. set-status on it's own seemed a bit strange. Minor detail: Last on-error must be\npreceded by a slash.\nGot this in 11/2022 Went with on-error, context, context, set-status, override-status, on-\nerror One more blank space after <otherwise />, I selected override-status",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "256",
    "question": "DRAG DROP -\nYou are a developer for a Software as a Service (SaaS) company. You develop\nsolutions that provide the ability to send notifications by using Azure\nNotification\nHubs.\nYou need to create sample code that customers can use as a reference for how\nto send raw notifications to Windows Push Notification Services (WNS) devices.\nThe sample code must not use external packages.\nHow should you complete the code segment? To answer, drag the appropriate\ncode segments to the correct locations. Each code segment may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: windows -\nExample code:\nvar request = new HttpRequestMessage(method, $\"{resourceUri}?api-version=2017-04\");\nrequest.Headers.Add(\"Authorization\", createToken(resourceUri, KEY_NAME,\nKEY_VALUE));\nrequest.Headers.Add(\"X-WNS-Type\", \"wns/raw\");\nrequest.Headers.Add(\"ServiceBusNotification-Format\", \"windows\"); return request;\nBox 2: application/octet-stream -\nExample code capable of sending a raw notification:\nstring resourceUri = $\"https://{NH_NAMESPACE}.servicebus.windows.net/{HUB_NAME}/\nmessages/\"; using (var request = CreateHttpRequest(HttpMethod.Post, resourceUri))\n{\nrequest.Content = new StringContent(content, Encoding.UTF8,\n\"application/octet-stream\");\nrequest.Content.Headers.ContentType.CharSet = string.Empty;\nvar httpClient = new HttpClient();\nvar response = await httpClient.SendAsync(request);\nConsole.WriteLine(response.StatusCode);\n}\nReference:\nhttps://stackoverflow.com/questions/31346714/how-to-send-raw-notification-to-azure-\nnotification-hub/31347901\nCommunity Discussion\nAnswer is correct. https://docs.microsoft.com/en-us/rest/api/notificationhubs/send-\nwns-native-notification\nGot this one 01/2022. Went with most voted (to avoid writing answers again)\nabout second box: Set to application/json;charset=utf-8 or application/xml. If the\nnotification type (X-WNS-Type) is wns/raw, set to application/octet-stream\nNotification hub and logic apps are removed from scope\ngot this question on 29/06/2023",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "257",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data\nfrom 2,000 stores located throughout the world. A single device can produce\n2 megabytes (MB) of data every 24 hours. Each store location has one to five\ndevices that send data.\nYou must store the device data in Azure Blob storage. Device data must be\ncorrelated based on a device identifier. Additional stores are expected to open\nin the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Hub. Configure the machine identifier as the\npartition key and enable capture.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-programming-guide"
  },
  {
    "number": "258",
    "question": "DRAG DROP -\nYou are developing an Azure solution to collect inventory data from thousands\nof stores located around the world. Each store location will send the inventory\ndata hourly to an Azure Blob storage account for processing.\nThe solution must meet the following requirements:\n✑ Begin processing when data is saved to Azure Blob storage.\n✑ Filter data based on store location information.\n✑ Trigger an Azure Logic App to process the data for output to Azure Cosmos\nDB.\n✑ Enable high availability and geographic distribution.\n✑ Allow 24-hours for retries.\n✑ Implement an exponential back off data processing.\nYou need to configure the solution.\nWhat should you implement? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Azure Event Grid -\nBlob storage events are pushed using Azure Event Grid to subscribers such as Azure\nFunctions, Azure Logic Apps, or even to your own http listener. Event Grid provides\nreliable event delivery to your applications through rich retry policies and dead-lettering.\nBox 2: Azure Logic App -\nEvent Grid uses event subscriptions to route event messages to subscribers. This image\nillustrates the relationship between event publishers, event subscriptions, and event\nhandlers.\nBox 3: Azure Service Bus -\nThe Event Grid service doesn't store events. Instead, events are stored in the Event\nHandlers, including ServiceBus, EventHubs, Storage Queue, WebHook endpoint, or many\nother supported Azure Services.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview\nhttps://docs.microsoft.com/en-us/java/api/overview/azure/messaging-eventgrid-\nreadme\n\n\nCommunity Discussion\nSource -> blob storage Receiver -> event grid Handler -> logic app\nI'm always wondering who is the big brain behind the secexams question solutions...\nGot it on 03/2022, chose Source: Blob storage, Receiver: Event Grid, Handler: Logic App.\nReceiver ?? https://docs.microsoft.com/en-us/azure/event-grid/overview\nReceiver ?? https://docs.microsoft.com/en-us/azure/event-grid/overview",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "259",
    "question": "You are creating an app that will use CosmosDB for data storage. The app will\nprocess batches of relational data.\nYou need to select an API for the app.\nWhich API should you use?",
    "options": [
      {
        "letter": "A",
        "text": "MongoDB API",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Table API",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL API",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Cassandra API",
        "is_correct": false
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "260",
    "question": "HOTSPOT -\nYou are developing a .NET application that communicates with Azure Storage.\nA message must be stored when the application initializes.\nYou need to implement the message.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-\nqueues?tabs=dotnetv11\nCommunity Discussion\nCorrect, just that this is deprecated For .NET V12 SDK is as follows : string\nconnectionString = ConfigurationManager.AppSettings[\"StorageConnectionString\"];\nQueueClient queueClient = new QueueClient(connectionString, queueName);\nqueueClient.CreateIfNotExists();\nThis was on the exam (July 2023). Went with proposed. Scored 917\nAnswer is correct: 1. Create the queue client 2. Retrieve a reference to a queue\nGot it in the exam 14/12/23. Scored 912/1000. All questions are from SecExams. Case\nstudy - VanArsdel, Ltd (11 questions)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "261",
    "question": "HOTSPOT -\nA software as a service (SaaS) company provides document management\nservices. The company has a service that consists of several Azure web apps. All\nAzure web apps run in an Azure App Service Plan named PrimaryASP.\nYou are developing a new web service by using a web app named ExcelParser.\nThe web app contains a third-party library for processing Microsoft Excel files.\nThe license for the third-party library stipulates that you can only run a single\ninstance of the library.\nYou need to configure the service.\nHow should you complete the script? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/manage-scale-per-app\nCommunity Discussion\nAnswer is correct, -PerSiteScaling $true; $app.SiteConfig.NumberOfWorkers = 1\nyes, the Answer is correct.\nThis was on the exam (9th August 2023). Went with proposed.\n\n\nWhy PerSiteScaling $true is needed?\ni guess not to impact other apps on the service plan",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "262",
    "question": "DRAG DROP -\nYou have an application that provides weather forecasting data to external\npartners. You use Azure API Management to publish APIs.\nYou must change the behavior of the API to meet the following requirements:\n✑ Support alternative input parameters\n✑ Remove formatting text from responses\n✑ Provide additional context to back-end services\nWhich types of policies should you implement? To answer, drag the policy types\nto the correct requirements. Each policy type may be used once, more than\nonce, or not at all. You may need to drag the split bar between panes or scroll\nto view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-\npolicies https://docs.microsoft.com/en-us/azure/api-management/api-management-\ntransformation-policies#forward-context-information-to-the-backend-service\nCommunity Discussion\nInbound Outbound Backend\nBased on https://docs.microsoft.com/en-us/azure/api-management/policies/send-\nrequest-context-info-to-backend-service Inbound is correct answer for the third question\n<policies> <inbound> <base /> <!-- Forward the name of the product associated with the\nsubscription key in the request to the backend service. --> <set-query-parameter\nname=\"x-product-name\" exists-action=\"override\"> <value>@(context.Product.Name)</\nvalue> </set-query-parameter> <!-- Forward the user id associated with the subscription\nkey in the request as well as the region where the proxy processing the request is\nhosted. --> <set-header name=\"x-request-context-data\" exists-action=\"override\">\n<value>@(context.User.Id)</value> <value>@(context.Deployment.Region)</value> </set-\nheader> </inbound>\nlast one is backend https://learn.microsoft.com/en-us/azure/api-management/forward-\nrequest-policy\nlast one is backend https://learn.microsoft.com/en-us/azure/api-management/forward-\nrequest-policy\nI'm not 100% sure, but based on the following statement I think that the third option\nshould be backend: \"statements to be applied before the request is forwarded to the\nbackend service go here\" Source: https://docs.microsoft.com/en-us/azure/api-\nmanagement/api-management-howto-policies#-understanding-policy-configuration",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "263",
    "question": "You are developing an e-commerce solution that uses a microservice\narchitecture.\nYou need to design a communication backplane for communicating\ntransactional messages between various parts of the solution. Messages must\nbe communicated in first-in-first-out (FIFO) order.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Storage Queue",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure Event Hub",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Service Bus",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Event Grid",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "As a solution architect/developer, you should consider using Service Bus queues when:\n✑ Your solution requires the queue to provide a guaranteed first-in-first-out (FIFO)\nordered delivery.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-\nservice-bus-queues-compared-contrasted"
  },
  {
    "number": "264",
    "question": "DRAG DROP -\nA company backs up all manufacturing data to Azure Blob Storage. Admins\nmove blobs from hot storage to archive tier storage every month.\nYou must automatically move blobs to Archive tier after they have not been\nmodified within 180 days. The path for any item that is not archived must be\nplaced in an existing queue. This operation must be performed automatically\nonce a month. You set the value of TierAgeInDays to -180.\nHow should you configure the Logic App? To answer, drag the appropriate\ntriggers or action blocks to the correct trigger or action slots. Each trigger or\naction block may be used once, more than once, or not at all. You may need to\ndrag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nBox 1: Reoccurance..\nTo regularly run tasks, processes, or jobs on specific schedule, you can start your logic\napp workflow with the built-in Recurrence - Schedule trigger. You can set a date and time\nas well as a time zone for starting the workflow and a recurrence for repeating that\nworkflow.\nSet the interval and frequency for the recurrence. In this example, set these properties to\nrun your workflow every week.\n\n\nBox 2: Condition..\nTo run specific actions in your logic app only after passing a specified condition, add a\nconditional statement. This control structure compares the data in your workflow against\nspecific values or fields. You can then specify different actions that run based on whether\nor not the data meets the condition.\nBox 3: Put a message on a queue -\nThe path for any item that is not archived must be placed in an existing queue.\nNote: Under If true and If false, add the steps to perform based on whether the condition\nis met.\nBox 4: ..tier it to Cool or Archive tier.\nArchive item.\nBox 5: List blobs 2 -\nReference:\nhttps://docs.microsoft.com/en-us/azure/connectors/connectors-native-recurrence\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-loops\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-conditional-\nstatement\nCommunity Discussion\nLogic App is not part of 204 exam since March 2021\nFor Box 3, refer to the answer's text that says \"Put a message on a queue\". The answer's\ngraphic implies \"When there are messages in a queue\" forBox 3 by mistake.\nIt is clear that : box 1 should be recurrence box 2 should be the condition However, there\nis confusion about the condition, and rightly so: the right half, ticks(addDaysInMonth(),\nvariables(\"TierAgeInDays\")) has a few problems, first off: addDaysInMonth() doesn't exist\nas a function at all Second, ticks does not accept two parameters as is done here. (see\nreference:https://docs.microsoft.com/en-us/azure/logic-apps/workflow-definition-\nlanguage-functions-reference#date-and-time-functions) So, the condition block is bogus.\nHowever, they probably wanted to write something like: addDays(utcNow(),\nvariables(\"TierAgeInDays\")) which IS valid. The less-than will return true for anything\nolder, which leaves the next boxes to be: box 3: Tier blob box 4: Message queue box 5:\noptional, they probably want to show the results with list blob2\nYou're right!! Let's not waste our time with questions about Logic Apps anymore! Please\nupvote Cricketer!\n\n\nYou're right!! Let's not waste our time with questions about Logic Apps anymore! Please\nupvote Cricketer!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "265",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data\nwhen it receives a message from a mobile application. Messages may not be\nsent to the service consistently.\nYou have the following requirements:\n✑ Queue size must not grow larger than 80 gigabytes (GB).\n✑ Use first-in-first-out (FIFO) ordering of messages.\n✑ Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Service Bus Queue from\nthe mobile application. Create an Azure Function App that uses an Azure\nService Bus Queue trigger.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "You can create a function that is triggered when messages are submitted to an Azure\nStorage queue.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-\nqueue-triggered-function"
  },
  {
    "number": "266",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data\nfrom 2,000 stores located throughout the world. A single device can produce 2\nmegabytes (MB) of data every 24 hours. Each store location has one to five\ndevices that send data.\nYou must store the device data in Azure Blob storage. Device data must be\ncorrelated based on a device identifier. Additional stores are expected to open\nin the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Notification Hub. Register all devices with the hub.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No \nPage 612 of 1410\n613 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead use an Azure Service Bus, which is used order processing and financial\ntransactions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services"
  },
  {
    "number": "267",
    "question": "You are building a loyalty program for a major snack producer. When customers\nbuy a snack at any of 100 participating retailers the event is recorded in Azure\nEvent Hub. Each retailer is given a unique identifier that is used as the primary\nidentifier for the loyalty program.\nRetailers must be able to be added or removed at any time. Retailers must only\nbe able to record sales for themselves.\nYou need to ensure that retailers can record sales.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use publisher policies for retailers.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create a partition for each retailer.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Define a namespace for each retailer.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Event Hubs enables granular control over event publishers through publisher policies.\nPublisher policies are run-time features designed to facilitate large numbers of\nindependent event publishers. With publisher policies, each publisher uses its own\nunique identifier when publishing events to an event hub.\nIncorrect:\nNot C: An Event Hubs namespace is a management container for event hubs (or topics, in\nKafka parlance). It provides DNS-integrated network endpoints and a range of access\ncontrol and network integration management features such as IP filtering, virtual\nnetwork service endpoint, and Private Link.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features"
  },
  {
    "number": "268",
    "question": "DRAG DROP -\nYou develop and deploy a web app to Azure App Service in a production\nenvironment. You scale out the web app to four instances and configure a\nstaging slot to support changes.\nYou must monitor the web app in the environment to include the following\nrequirements:\n✑ Increase web app availability by re-routing requests away from instances\nwith error status codes and automatically replace instances if they remain in an\nerror state after one hour.\n✑ Send web server logs, application logs, standard output, and standard error\nmessaging to an Azure Storage blob account.\nYou need to configure Azure App Service.\nWhich values should you use? To answer, drag the appropriate configuration\nvalue to the correct requirements. Each configuration value may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Health check -\nHealth check increases your application's availability by re-routing requests away from\nunhealthy instances, and replacing instances if they remain unhealthy. Your\nApp Service plan should be scaled to two or more instances to fully utilize Health check.\nBox 2: Diagnostic setting -\nAzure provides built-in diagnostics to assist with debugging an App Service app.\nWith the new Azure Monitor integration, you can create Diagnostic Settings to send logs\nto Storage Accounts, Event Hubs and Log Analytics.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check\nhttps://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs\nCommunity Discussion\nBox 1: Health check Box 2: Diagnostic setting\ndisregard my answer please\n1. Autoscale rule 2. Diagnostic setting\nThe answer looks correct, https://learn.microsoft.com/en-us/azure/app-service/monitor-\ninstances-health-check?tabs=dotnet#what-happens-if-my-app-is-running-on-a-single-\ninstance\nI got this question on my exam, 2023Dec, go with what I remember was the most voted\nanswer. Score 902, most of the questions were here, slightly different on wording because\nthe Azure Ad <-> Entra Id change. Case was City Power & Light. Good luck! Important tip,\nyou have access to microsoft learn during the exam!",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "269",
    "question": "You develop a solution that uses Azure Virtual Machines (VMs).\nThe VMs contain code that must access resources in an Azure resource group.\nYou grant the VM access to the resource group in Resource Manager.\nYou need to obtain an access token that uses the VM's system-assigned\nmanaged identity.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.",
    "options": [
      {
        "letter": "A",
        "text": "From the code on the VM, call Azure Resource Manager using an access token.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Use PowerShell on a remote machine to make a request to the local managed identity for\nAzure resources endpoint.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Use PowerShell on the VM to make a request to the local managed identity for Azure\nresources endpoint.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "From the code on the VM, call Azure Resource Manager using a SAS token. \nE) From the code on the VM, generate a user delegation SAS token.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D"
  },
  {
    "number": "270",
    "question": "You are developing a road tollway tracking application that sends tracking\nevents by using Azure Event Hubs using premium tier.\nEach road must have a throttling policy uniquely assigned.\nYou need to configure the event hub to allow for per-road throttling.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use a unique consumer group for each road.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Ensure each road stores events in a different partition.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Ensure each road has a unique connection string.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use a unique application group for each road.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "application ID. Azure\nEvent Hubs enables you to define resource access policies such as throttling policies for\na given application group and controls event streaming (publishing or consuming)\nbetween client applications and Event Hubs. For more information, see Resource\ngovernance for client applications with application groups.\nSelected Answer: D\nDefinitely D. Never heard of before and wondering how many things to remember\nCan you share the resource you used to claim this answer ?\nSelected Answer: A\nChatGPT: A. Use a unique consumer group for each road. In order to configure the Event\nHub to allow for per-road throttling, you can use a unique consumer group for each\nroad. This way, you can assign a unique throttling policy to each consumer group, which\nin turn controls the number of events that can be sent and received by the\ncorresponding road. Option B and C are not correct solutions to this problem, because\npartitioning and unique connection strings are not related to throttling policies. Option D\nis not correct solution too, application groups are not related to throttling policies and\nthey are used to group related Event Hubs together.\nApplication groups An application group is a collection of client applications that\nconnect to an Event Hubs namespace sharing a unique identifying condition such as the\nsecurity context - shared access policy or Azure Active Directory (Azure AD) application ID.\nAzure Event Hubs enables you to define resource access policies such as throttling\npolicies for a given application group and controls event streaming (publishing or\nconsuming) between client applications and Event Hubs.\nPage 620 of 1410\n621 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "271",
    "question": "You develop and deploy an ASP.NET Core application that connects to an Azure\nDatabase for MySQL instance.\nConnections to the database appear to drop intermittently and the application\ncode does not handle the connection failure.\nYou need to handle the transient connection errors in code by implementing\nretries.\nWhat are three possible ways to achieve this goal? Each correct answer presents\npart of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Close the database connection and immediately report an error.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Disable connection pooling and configure a second Azure Database for MySQL instance.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Wait five seconds before repeating the connection attempt to the database. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set a maximum number of connection attempts to 10 and report an error on subsequent\nconnections. \nE) Increase connection repeat attempts exponentially up to 120 seconds.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "CD"
  },
  {
    "number": "272",
    "question": "You are building a B2B web application that uses Azure B2B collaboration for\nauthentication. Paying customers authenticate to Azure B2B using federation.\nThe application allows users to sign up for trial accounts using any email\naddress.\nWhen a user converts to a paying customer, the data associated with the trial\nshould be kept, but the user must authenticate using federation.\nYou need to update the user in Azure Active Directory (Azure AD) when they\nconvert to a paying customer.\nWhich Graph API parameter is used to change authentication from one-time\npasscodes to federation?",
    "options": [
      {
        "letter": "A",
        "text": "resetRedemption",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Status",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "userFlowType",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "invitedUser",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "273",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\nApp.\n\n\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\n\n\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nHOTSPOT -\nYou need to configure Azure CDN for the Shipping web site.\nWhich configuration options should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nScenario: Shipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for\ndynamic content while minimizing latency and costs.\nTier: Standard -\nProfile: Akamai -\nOptimization: Dynamic site acceleration\nDynamic site acceleration (DSA) is available for Azure CDN Standard from Akamai, Azure\nCDN Standard from Verizon, and Azure CDN Premium from Verizon profiles.\nDSA includes various techniques that benefit the latency and performance of dynamic\ncontent. Techniques include route and network optimization, TCP optimization, and more.\nYou can use this optimization to accelerate a web app that includes numerous responses\nthat aren't cacheable. Examples are search results, checkout transactions, or real-time\ndata. You can continue to use core Azure CDN caching capabilities for static data.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-optimization-overview\n\n\nCommunity Discussion\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) Wide World Importers 46 7 1 Azure CDN\ntiers 46 7 2 azure tools on/with VM 49 12 1 secure Function app : auth + token + trigger\ntype 49 12 2 secure Logic App : tool used 55 23 1 setup workflow in logic app 55 23 2 setup\nnetwork to trigger logic app from external server\nCorrect. Standard Akamai does support dynamic site acceleration. Microsoft requires\nfront door, meaning additional costs. Verizon should also work I suppose, but isn't an\noptions, so let's ignore that.\nGreat help\nGreat help\nStandard Microsoft General Web Delivery https://learn.microsoft.com/en-us/azure/cdn/\ncdn-features",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "274",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\nApp.\n\n\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\n\n\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nHOTSPOT -\nYou need to correct the VM issues.\nWhich tools should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Azure Backup -\nThe VM is critical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of failure.\nIn-Place restore of disks in IaaS VMs is a feature of Azure Backup.\nPerformance: Accelerated Networking\nScenario: The VM shows high network latency, jitter, and high CPU utilization.\nBox 2: Accelerated networking -\nThe VM shows high network latency, jitter, and high CPU utilization.\nAccelerated networking enables single root I/O virtualization (SR-IOV) to a VM, greatly\nimproving its networking performance. This high-performance path bypasses the host\nfrom the datapath, reducing latency, jitter, and CPU utilization, for use with the most\ndemanding network workloads on supported VM types.\nReference:\nhttps://azure.microsoft.com/en-us/blog/an-easy-way-to-bring-back-your-azure-vm-with-\nin-place-restore/\nCommunity Discussion\nHoow the f could you memorize all this.\n\n\nAccelerated Networking only works in conjunction with a Azure Virtual Network (VNet).\nThat is mentioned in the case and is a clue as well.\nVirtual Machines no longer exists on az 204\nThe given answer is correct. https://learn.microsoft.com/en-us/azure/virtual-network/\naccelerated-networking-overview#benefits\nsame thoughts.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "275",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\nEmployees may upload receipts in two ways:\n\n\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\nenabled.\n\n\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\nthe file in the file share and use the web application, which returns a 500\n\n\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion DRAG DROP -\nYou need to add code at line PC32 in Processing.cs to implement the\nGetCredentials method in the Processing class.\nHow should you complete the code? To answer, drag the appropriate code\nsegments to the correct locations. Each code segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: AzureServiceTokenProvider()\nBox 2: tp.GetAccessTokenAsync(\"..\")\nAcquiring an access token is then quite easy. Example code:\nprivate async Task<string> GetAccessTokenAsync()\n{\nvar tokenProvider = new AzureServiceTokenProvider();\nreturn await tokenProvider.GetAccessTokenAsync(\"https://storage.azure.com/\");\n}\nReference:\nhttps://joonasw.net/view/azure-ad-authentication-with-azure-storage-and-managed-\nservice-identity\nCommunity Discussion\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) Litware Inc 47 8 1 code : GetCredentials\nmethod 47 8 2 code : disaster recovery requirements 51 14 1 code : set encryption with\nrequest to blob storage 51 14 2 ARM set keyVault access policy 53 20 1 prevent blob\nwriting concurrency problems 54 20 2 resolve capacity issue at function processing 54 20\n3 resolve capacity issue at extensive logging\nAnswer is correct: https://docs.microsoft.com/en-us/azure/app-service/overview-\nmanaged-identity?context=azure%2Factive-directory%2Fmanaged-identities-azure-\nresources%2Fcontext%2Fmsi-context&tabs=python#asal\nWide World Importers 7/1 7/2 12/1 12/2 23/1 23/2 Litware Inc. 8/1 8/2 14/1 14/2 20/1 20/2\n20/3 Coho Winery produces 9/1 9/2 22/1 22/2 28/1 28/2 VanArsdel, Ltd. 10/1 10/2 10/3\n10/4 15/1 15/2 21/1 29/1 29/2 30/1 Contoso, Ltd. 11/1 11/2 11/3 11/4 11/5 16/1 16/2 27/1 27/2\n31/1 31/2 31/3 City Power & Light 13/1 13/2 13/3 13/4 13/5 13/6 17/1 17/2 18/1 24/1 24/2 32/1\nProseware, Inc. 19/1 19/2 19/3 19/4 25/1 25/2 25/3 26/1\nWide World Importers 7/1 7/2 12/1 12/2 23/1 23/2 Litware Inc. 8/1 8/2 14/1 14/2 20/1 20/2\n20/3 Coho Winery produces 9/1 9/2 22/1 22/2 28/1 28/2 VanArsdel, Ltd. 10/1 10/2 10/3\n10/4 15/1 15/2 21/1 29/1 29/2 30/1 Contoso, Ltd. 11/1 11/2 11/3 11/4 11/5 16/1 16/2 27/1 27/2\n\n\n31/1 31/2 31/3 City Power & Light 13/1 13/2 13/3 13/4 13/5 13/6 17/1 17/2 18/1 24/1 24/2 32/1\nProseware, Inc. 19/1 19/2 19/3 19/4 25/1 25/2 25/3 26/1\nThis is Microsoft.Azure.Services.AppAuthentication and is since 2019 legacy. Instead use\nthe Azure Identity client library for .NET Azure.Identity using Azure.Identity; using\nAzure.Security.KeyVault.Secrets; // Create a secret client using the DefaultAzureCredential\nvar client = new SecretClient(new Uri(\"https://myvault.azure.vaults.net/\"), new\nDefaultAzureCredential()); etc.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "276",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\nEmployees may upload receipts in two ways:\n\n\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\nenabled.\n\n\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\nthe file in the file share and use the web application, which returns a 500\n\n\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion DRAG DROP -\nYou need to ensure disaster recovery requirements are met.\nWhat code should you add at line PC16?\nTo answer, drag the appropriate code fragments to the correct locations. Each\ncode fragment may be used once, more than once, or not at all. You may need\nto drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nScenario: Disaster recovery. Regional outage must not impact application availability. All\nDR operations must not be dependent on application running and must ensure that data\nin the DR region is up to date.\nBox 1: DirectoryTransferContext -\nWe transfer all files in the directory.\nNote: The TransferContext object comes in two forms: SingleTransferContext and\nDirectoryTransferContext. The former is for transferring a single file and the latter is for\ntransferring a directory of files.\nBox 2: ShouldTransferCallbackAsync\nThe DirectoryTransferContext.ShouldTransferCallbackAsync delegate callback is invoked\nto tell whether a transfer should be done.\nBox 3: False -\nIf you want to use the retry policy in Copy, and want the copy can be resume if break in\nthe middle, you can use SyncCopy (isServiceCopy = false).\nNote that if you choose to use service side copy ('isServiceCopy' set to true), Azure\n(currently) doesn't provide SLA for that. Setting 'isServiceCopy' to false will download the\nsource blob loca\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-use-data-movement-\nlibrary https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.windowsazure.storage.datamovement.directorytransfercontext.shouldtransfercallbackasync?\nview=azure- dotnet\nCommunity Discussion\nAnswers in box 1 and 2 arent' correct. They should be SingleTranferConext and\nShouldOverwriteCallbackAsync because we are copying a single blob (see CopyAsync\nmethod). We are inside a foreach loop that scan each file in the share.\nSingleTranferConext ShouldOverwriteCallbackAsync true\n\n\nCorrect 1. https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.datamovement.singletransfercontext?view=azure-dotnet 2.\nAccording to a link above, there is only \"ShouldOverwriteCallbackAsync\" available for\n\"SingleTranferConext \" 3. https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.datamovement.transfermanager.copyasync?view=azure-\ndotnet#Microsoft_Azure_Storage_DataMovement_TransferManager_CopyAsync_Microsoft_Azure_Storage_Blob_CloudBlob_Microsoft_Azure_Storage_Blob_CloudBlob_System_Boolean_Microsoft_Azure_Storage_DataMovement_CopyOptions_Microsoft_Azure_Storage_DataMovement_SingleTransferContext_\nThe question is should it be server side copy or not? I would say \"true\".\nCorrect 1. https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.datamovement.singletransfercontext?view=azure-dotnet 2.\nAccording to a link above, there is only \"ShouldOverwriteCallbackAsync\" available for\n\"SingleTranferConext \" 3. https://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.datamovement.transfermanager.copyasync?view=azure-\ndotnet#Microsoft_Azure_Storage_DataMovement_TransferManager_CopyAsync_Microsoft_Azure_Storage_Blob_CloudBlob_Microsoft_Azure_Storage_Blob_CloudBlob_System_Boolean_Microsoft_Azure_Storage_DataMovement_CopyOptions_Microsoft_Azure_Storage_DataMovement_SingleTransferContext_\nThe question is should it be server side copy or not? I would say \"true\".\nSrry im didnt read well",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "277",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nOrder.json -\nQuestion HOTSPOT -\nYou need to configure Azure Cosmos DB.\nWhich settings should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Strong -\nWhen the consistency level is set to strong, the staleness window is equivalent to zero,\nand the clients are guaranteed to read the latest committed value of the write operation.\nScenario: Changes to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nNote: You can choose from five well-defined models on the consistency spectrum. From\nstrongest to weakest, the models are: Strong, Bounded staleness,\nSession, Consistent prefix, Eventual\nBox 2: SQL -\nScenario: You identify the following requirements for data management and\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using Structured Query\nLanguage (SQL).\nCommunity Discussion\ncorrect 100%\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) Coho Winery 47 9 1 configure Azure\n\n\nCosmos DB 47 9 2 SQL 54 22 1 Review logs/history to troubleshoot workflow 54 22 2 ARM\nfunction configuration : retries etc 57 28 1 workflow for Azure Container Registry 57 28 2\naccess data from the user claim object\n(ﾉ◕ヮ◕)ﾉ*:･ﾟ✧\nAnswer is correct - Strong consistency matches requirements and MS recommends that if\nyou have no preference or specific reason to use any of the other cosmos APIs, you\nshould use the SQL api\ncorrect Answer: 1. Strong consistency 2. Sql Api",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "278",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nOrder.json -\nQuestion HOTSPOT -\nYou need to retrieve all order line items from Order.json and sort the data\nalphabetically by the city.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: orders o -\nScenario: Order data is stored as nonrelational JSON and must be queried using SQL.\nBox 2:li -\nBox 3: o.line_items -\nBox 4: o.city -\nThe city field is in Order, not in the 2s.\nCommunity Discussion\ngoodluck!\nThe last one should be o.address.city\nI passed with only these questions, so don't stress guys. 80% of these questions is\nexactly the same. Good luck!\n\n\nI Cleared it with 88%. And I got this question too.:-)\nI Cleared it with 88%. And I got this question too.:-)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "279",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question HOTSPOT -\nYou need to implement the Azure Function for delivery driver profile\ninformation.\nWhich configurations should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Azure Identity library -\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an\nAzure Function called from the corporate website.\nWe recommend that you use a managed identity for applications deployed to Azure.\nThe preceding authentication scenarios are supported by the Azure Identity client library\nand integrated with Key Vault SDKs.\nNote: What is Managed Service Identity?\nAzure Key Vault avoids the need to store keys and secrets in application code or source\ncontrol. However, in order to retrieve keys and secrets from Azure Key\nVault, you need to authorize a user or application with Azure Key Vault, which in its turn\nneeds another credential. Managed Service Identity avoids the need of storing\ncredentials for Azure Key Vault in application or environment settings by creating a\nService Principal for each application or cloud service on which\nManaged Service Identity is enabled. This Service Principal enables you to call a local MSI\nendpoint to get an access token from Azure AD using the credentials of the Service\nPrincipal. This token is then used to authenticate to an Azure Service, for example Azure\nKey Vault.\nBox 2: Azure Key Vault -\nAzure Key Vault allows you to securely access sensitive information from within your\napplications:\n* Keys, secrets, and certificates are protected without your having to write the code\nyourself, and you can easily use them from your applications.\nUse Azure Key Vault to store only secrets for your application. Examples of secrets that\nshould be stored in Key Vault include:\n\n\nClient application secrets -\nConnection strings -\nPasswords -\nShared access keys -\nSSH keys -\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/developers-guide https://\nintegration.team/blog/retrieve-azure-key-vault-secrets-using-azure-functions-and-\nmanaged-service-identity\nCommunity Discussion\nStore delivery driver profile information in Azure Active Directory (Azure AD), I would use:\nCode Library: MSAL API: Microsoft Graph https://docs.microsoft.com/en-us/azure/active-\ndirectory/develop/msal-overview\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) VanArsdel, Ltd. 47 10 1 delivery profile\ninformation in code 48 10 2 grant access SAS or AD? 48 10 3 JWT +claims 48 10 4 place to\nstore the RSA-HSM key 51 15 1 reduce read latency Cosmos DB 51 15 2 audit transactions\nin blob 54 21 1 systems for source-receiver-handler 57 29 1 funcion configuration : binding\n, trigger, direction 57 29 2 tier of SPA web app 57 30 1 Azure Blob features for point-in-time\nrestore and accidental deletion of data\nWhy...o...why are so many of the revealed answers incorrect? Without the topic\ndiscussions you would have hard times...\nMSAL MS Graph\nI got this question (12-Aug-2023) and I chose highly voted answer - 932 passed",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "280",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question You need to\ngrant access to the retail store location data for the inventory service\ndevelopment effort.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure AD access token",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure RBAC role",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Shared access signature (SAS) token",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure AD ID token\nE) Azure AD refresh token",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "A shared access signature (SAS) provides secure delegated access to resources in your\nstorage account. With a SAS, you have granular control over how a client can access your\ndata. For example:\nWhat resources the client may access.\nWhat permissions they have to those resources.\nHow long the SAS is valid.\nNote: Inventory services:\nThe company has contracted a third-party to develop an API for inventory processing\nthat requires access to a specific blob within the retail store storage account for three\nmonths to include read-only access to the data."
  },
  {
    "number": "281",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question HOTSPOT -\nYou need to reliably identify the delivery driver profile information.\nHow should you configure the system? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: ID -\nScenario: Store delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nID token - A JWT that contains claims that you can use to identify users in your\napplication. This token is securely sent in HTTP requests for communication between two\ncomponents of the same application or service. You can use the claims in an ID token as\nyou see fit. They're commonly used to display account information or to make access\ncontrol decisions in an application. ID tokens are signed, but the're not encrypted. When\nyour application or API receives an ID token, it must validate the signature to prove that\nthe token is authentic. Your application or API must also validate a few claims in the\ntoken to prove that it's valid.\nDepending on the scenario requirements, the claims validated by an application can\nvary, but your application must perform some common claim validations in every\nscenario.\nBox 2: Oid -\nOid - The immutable identifier for the \"principal\" of the request - the user or service\nprincipal whose identity has been verified. In ID tokens and app+user tokens, this is the\nobject ID of the user. In app-only tokens, this is the object ID of the calling service\nprincipal. It can also be used to perform authorization checks safely and as a key in\ndatabase tables. This ID uniquely identifies the principal across applications - two\ndifferent applications signing in the same user will receive the same value in the oid\nclaim.\nIncorrect:\nAud - Identifies the intended recipient of the token. For Azure AD B2C, the audience is the\n\n\napplication ID. Your application should validate this value and reject the token if it\ndoesn't match. Audience is synonymous with resource.\nIdp - Records the identity provider that authenticated the subject of the token. This value\nis identical to the value of the Issuer claim unless the user account not in the same\ntenant as the issuer - guests, for instance. If the claim isn't present, it means that the\nvalue of iss can be used instead. For personal accounts being used in an organizational\ncontext (for instance, a personal account invited to an Azure AD tenant), the idp claim\nmay be 'live.com' or an STS URI containing the\nMicrosoft account tenant.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory-b2c/tokens-overview https://\ndocs.microsoft.com/en-us/azure/active-directory/develop/access-tokens\nCommunity Discussion\nGot this question on 30-Sep-2022 exam. Answer is correct. Passed with 870 score.\nLooks correct. Ref.: https://learn.microsoft.com/en-us/azure/active-directory/develop/\nid-tokens\nI got this question (12-Aug-2023) and I chose highly voted answer - 932 passed\nLook correct. Got this question on 26-Mar-2023 exam. Go with suggested answer got 890\nscore.\nThis was on the exam (July 2023). Went with highly voted. Scored 917",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "282",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question You need to\nsecure the Azure Functions to meet the security requirements.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Store the RSA-HSM key in Azure Key Vault with soft-delete and purge-protection features\nenabled.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Store the RSA-HSM key in Azure Blob storage with an immutability policy applied to the\ncontainer.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a free tier Azure App Configuration instance with a new Azure AD service principal.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a standard tier Azure App Configuration instance with an assigned Azure AD\nmanaged identity. \nE) Store the RSA-HSM key in Azure Cosmos DB. Apply the built-in policies for customer-managed\nkeys and allowed locations.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D\nScenario: All Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by using a\ncompany-provided RSA-HSM key.\nMicrosoft Azure Key Vault is a cloud-hosted management service that allows users to"
  },
  {
    "number": "283",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion DRAG DROP -\nYou need to add markup at line AM04 to implement the ContentReview role.\nHow should you complete the markup? To answer, drag the appropriate json\nsegments to the correct locations. Each json segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\n\n\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: allowedMemberTypes -\nallowedMemberTypes specifies whether this app role definition can be assigned to users\nand groups by setting to \"User\", or to other applications (that are accessing this\napplication in daemon service scenarios) by setting to \"Application\", or to both.\nNote: The following example shows the appRoles that you can assign to users.\n\"appId\": \"8763f1c4-f988-489c-a51e-158e9ef97d6a\",\n\"appRoles\": [\n{\n\"allowedMemberTypes\": [\n\n\n\"User\"\n],\n\"displayName\": \"Writer\",\n\"id\": \"d1c2ade8-98f8-45fd-aa4a-6d06b947c66f\",\n\"isEnabled\": true,\n\"description\": \"Writers Have the ability to create tasks.\",\n\"value\": \"Writer\"\n}\n],\n\"availableToOtherTenants\": false,\nBox 2: User -\nScenario: In order to review content a user must be part of a ContentReviewer role.\nBox 3: value -\nvalue specifies the value which will be included in the roles claim in authentication and\naccess tokens.\nReference:\nhttps://docs.microsoft.com/en-us/graph/api/resources/approle\nCommunity Discussion\nCorrect. See MS example here: https://docs.microsoft.com/de-de/azure/active-directory/\ndevelop/howto-add-app-roles-in-azure-ad-apps#example-user-app-role\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) Contoso, Ltd 48 11 1 configure\nApplicationManifest 48 11 2 configure ApplicationManifest 49 11 3 type of SSL certificate /\nproxy used 49 11 4 YAML @Azure Container Instances for mounting 49 11 5\nApplicationManifest optional claims 52 16 1 cli : az monitor metrics alert create 52 16 2 cli\n: az for http server log output 56 27 1 code : read properties from GRID event 56 27 2 tier\nof function plan @Vnet 58 31 1 choose best storage solution 58 31 2 code : bindings for\nfunction 58 31 3 docker file : Private/Public Windows/Linux\nOn exam December 2, 2022\nHere below find same case studies topics: Wide World Importers 7, 12, 23 Litware Inc. 8, 14,\n20 Coho Winery produces 9, 22, 28 VanArsdel, Ltd. 10, 15, 21, 29, 30 Contoso, Ltd. 11, 16, 27, 31\nCity Power & Light 13, 17, 18, 24, 32 Proseware, Inc. 19, 25, 26\n\n\nGot this in the exam today! Feb 28, 2023",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "284",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion HOTSPOT -\nYou need to add code at line AM09 to ensure that users can review content\nusing ContentAnalysisService.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\n\n\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: \"oauth2Permissions\": [\"login\"]\noauth2Permissions specifies the collection of OAuth 2.0 permission scopes that the web\nAPI (resource) app exposes to client apps. These permission scopes may be granted to\nclient apps during consent.\nBox 2: \"oauth2AllowImplicitFlow\":true\nFor applications (Angular, Ember.js, React.js, and so on), Microsoft identity platform\nsupports the OAuth 2.0 Implicit Grant flow.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-\nmanifest\nCommunity Discussion\nhttps://www.secexams.com/discussions/microsoft/view/25371-exam-az-204-topic-8-\nquestion-2-discussion/ oauth2AllowImplicitFlow = true, oauth2AllowIdTokenImplicitFlow\n= true\nBox 1: \"oauth2AllowIdTokenImplicitFlow\":true This value indicates whether the web app\ncan request ID tokens of the implicit OAuth 2.0 flow. The default setting is \"false\". Box 2:\n\"oauth2AllowImplicitFlow\":true This value indicates whether the web app can request\nOAuth 2.0 implicit flow access tokens. The default setting is \"false\". Reference: https://\n\n\ndocs.microsoft.com/de-de/azure/active-directory/develop/reference-app-manifest\nhttps://docs.microsoft.com/de-de/azure/active-directory/develop/reference-app-\nmanifest#oauth2allowidtokenimplicitflow-attribute https://docs.microsoft.com/de-de/\nazure/active-directory/develop/reference-app-manifest#oauth2allowimplicitflow-\nattribute\n\"allowPublicClient\": true > Not required as infered from replyUrlsWithType\n\"oauth2Permissions\": [\"login\"] > only for resource server exposing an API\n\"oauth2AllowUrlPathMatching: true > Couldn't find in the docu\n\"oauth2AllowIdTokenImplicitFlow\": true > Correct one to choose!\n\"oauth2AllowImplicitFlow\": true > Correct one to choose! \"oauth2RequiredPostResponse\":\ntrue > Not relevant \"preAuthorizedApplications\": [\"SPA\"] > Not relevant\n\"knownClientApplications\": [\"ContentAnalysisService\"] > Not relevant https://\ndocs.microsoft.com/en-us/azure/active-directory/develop/reference-app-manifest?\nWT.mc_id=Portal-Microsoft_AAD_RegisteredApps#manifest-reference\nSince it is a browser app (React), Implicit Grant Flow, should be enabled:\noauth2AllowImplicitFlow = true, oauth2AllowIdTokenImplicitFlow = true \"To review\ncontent, the user must authenticate to the website portion of the ContentAnalysisService\nusing their Azure AD credentials. The website is built using React and all pages and API\nendpoints require authentication. In order to review content a user must be part of a\nContentReviewer role. All completed reviews must include the \"reviewer's email address\nfor auditing purposes. This is a web app so we need oauth2AllowImplicitFlow. We need to\nhave information about the user (reviewer email), so we need to have Id Token thus\noauth2AllowIdTokenImplicitFlow must be set to true\nI think the first answer should be allowPublicClient: true because this is SPA application\nand be default is is set to false. implicit flow does not make sense with confidential\nclients, it would be client credentials flow.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "285",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion HOTSPOT -\nYou need to ensure that network security policies are met.\nHow should you configure network security? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Valid root certificate -\nScenario: All websites and services must use SSL from a valid root certificate authority.\nBox 2: Azure Application Gateway\n\n\nScenario:\n✑ Any web service accessible over the Internet must be protected from cross site\nscripting attacks.\n✑ All Internal services must only be accessible from Internal Virtual Networks (VNets)\nAll parts of the system must support inbound and outbound traffic restrictions.\nAzure Web Application Firewall (WAF) on Azure Application Gateway provides centralized\nprotection of your web applications from common exploits and vulnerabilities. Web\napplications are increasingly targeted by malicious attacks that exploit commonly known\nvulnerabilities. SQL injection and cross-site scripting are among the most common\nattacks.\nApplication Gateway supports autoscaling, SSL offloading, and end-to-end SSL, a web\napplication firewall (WAF), cookie-based session affinity, URL path-based routing,\nmultisite hosting, redirection, rewrite HTTP headers and other features.\nNote: Both Nginx and Azure Application Gateway act as a reverse proxy with Layer 7 load-\nbalancing features plus a WAF to ensure strong protection against common web\nvulnerabilities and exploits.\nYou can modify Nginx web server configuration/SSL for X-XSS protection. This helps to\nprevent cross-site scripting exploits by forcing the injection of HTTP headers with X-XSS\nprotection.\nReference:\nhttps://docs.microsoft.com/en-us/azure/web-application-firewall/ag/ag-overview\nhttps://www.upguard.com/articles/10-tips-for-securing-your-nginx-deployment\nCommunity Discussion\nBox 1: Valid root certificate Scenario: All websites and services must use SSL from a valid\nroot certificate authority. Box 2: Azure Application Gateway Scenario: - Any web service\naccessible over the Internet must be protected from cross site scripting attacks. - All\nInternal services must only be accessible from Internal Virtual Networks (VNets).\nGot this in 16/02/2023\nOn my exam 2022-12-26. Chose： Valid root certificate Azure Application Gateway\nYeah! Box 1: Answer is written inside security tab: \"must use SSL from a valid root\ncertificate authority\" Must read carefully sometime to get the answer :) Box 2: nginx as\nservice is not provided by Microsoft azure\n\n\nOn my exam 2022-12-26. Chose： Valid root certificate Azure Application Gateway",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "286",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion DRAG DROP -\nYou need to add YAML markup at line CS17 to ensure that the\nContentUploadService can access Azure Storage access keys.\nHow should you complete the YAML markup? To answer, drag the appropriate\nYAML segments to the correct locations. Each YAML segment may be used once,\n\n\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\nBox 1: volumeMounts -\nExample:\nvolumeMounts:\n- mountPath: /mnt/secrets\nname: secretvolume1\nvolumes:\n- name: secretvolume1\n\n\nsecret:\nmysecret1: TXkgZmlyc3Qgc2VjcmV0IEZPTwo=\nBox 2: volumes -\nBox 3: secret -\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-\nvolume-secret\nCommunity Discussion\nBox 1: volumeMounts Box 2: volumes Box 3: secret Reference: https://\ndocs.microsoft.com/en-us/azure/container-instances/container-instances-volume-\nsecret\nAnswers are correct!\nCorrect. But a trick question. accesskey is the name of the volume\nOn my exam 2022-12-26. Chose： volumeMounts volumes secert\nOn my exam 2022-12-26. Chose： volumeMounts volumes secert",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "287",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion HOTSPOT -\nYou need to add code at line AM10 of the application manifest to ensure that\nthe requirement for manually reviewing content can be met.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\n\n\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: sid -\nSid: Session ID, used for per-session user sign-out. Personal and Azure AD accounts.\nScenario: Manual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content\na user must be part of a ContentReviewer role.\nBox 2: email -\nScenario: All completed reviews must include the reviewer's email address for auditing\npurposes.\nCommunity Discussion\nBox 1: sid Sid: Session ID, used for per-session user sign-out. Personal and Azure AD\naccounts. Scenario: To review content, the user must authenticate to the website portion\nof the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content\na user must be part of a ContentReviewer role. Box 2: email Scenario: All completed\nreviews must include the reviewer’s email address for auditing purposes. Reference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-\n\n\nmanifest https://docs.microsoft.com/en-us/azure/active-directory/develop/active-\ndirectory-optional-claims\nThe answer is correct. References: https://docs.microsoft.com/en-us/azure/active-\ndirectory/develop/reference-app-manifest https://docs.microsoft.com/en-us/azure/\nactive-directory/develop/active-directory-optional-claims\n\"To review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials.\" sid = Personal and Azure AD\naccounts Session ID https://docs.microsoft.com/en-us/azure/active-directory/develop/\nactive-directory-optional-claims\n\"To review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials.\" sid = Personal and Azure AD\naccounts Session ID https://docs.microsoft.com/en-us/azure/active-directory/develop/\nactive-directory-optional-claims\nOn my exam 2022-12-26. Chose： sid email",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "288",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\nApp.\n\n\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\n\n\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nHOTSPOT -\nYou need to secure the Shipping Function app.\nHow should you configure the app? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nScenario: Shipping Function app: Implement secure function endpoints by using app-\nlevel security and include Azure Active Directory (Azure AD).\nBox 1: Function -\nBox 2: JSON based Token (JWT)\nAzure AD uses JSON based tokens (JWTs) that contain claims\nBox 3: HTTP -\nHow a web app delegates sign-in to Azure AD and obtains a token\nUser authentication happens via the browser. The OpenID protocol uses standard HTTP\nprotocol messages.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/authentication-\nscenarios\nCommunity Discussion\n\n\nThe correct answer is: Anonymous JWT HTTP Scenario: Shipping Function app: Implement\nsecure function endpoints by using app-level security and include Azure Active Directory\n(Azure AD). 1. Authorization Level must be anonymous to use function app level security\nmethods. 2. User claims must be JWT tokens; API Key is not recommended due to security\nissues. 3. Function App is triggered from Logic App. So it must be http https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook-\ntrigger?tabs=csharp#secure-an-http-endpoint-in-production\nFunction JWT HTTP\nBased on description it should be anonymous https://learn.microsoft.com/en-us/azure/\nazure-functions/functions-bindings-http-webhook-trigger?tabs=in-\nprocess%2Cfunctionsv2&pivots=programming-language-csharp#secure-an-http-\nendpoint-in-production\n\"Implement secure function endpoints by using app-level security and include Azure\nActive Directory (Azure AD).\" How can it be Function is AAD requires \"Anonymous\"? My\nvote: Anonymous JWT HTTP\nhere is the link: Anonymous is the only authorization level that doesn't require a key. So :\nfunction, JWT HTTP is still valid . https://learn.microsoft.com/en-us/azure/azure-\nfunctions/security-concepts?tabs=v4#function-access-keys",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "289",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\nApp.\n\n\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\n\n\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nYou need to secure the Shipping Logic App.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure App Service Environment (ASE)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Integration Service Environment (ISE)",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "VNet service endpoint",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure AD B2B integration",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: The Shipping Logic App requires secure resources to the corporate VNet and\nuse dedicated storage resources with a fixed costing model.\nYou can access to Azure Virtual Network resources from Azure Logic Apps by using\nintegration service environments (ISEs).\nSometimes, your logic apps and integration accounts need access to secured resources,\nsuch as virtual machines (VMs) and other systems or services, that are inside an Azure\nvirtual network. To set up this access, you can create an integration service environment\n(ISE) where you can run your logic apps and create your integration accounts.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/connect-virtual-network-vnet-\nisolated-environment-overview"
  },
  {
    "number": "290",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to retrieve the database connection string.\nWhich values should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nAzure database connection string retrieve REST API vault.azure.net/secrets/\nBox 1: cpandlkeyvault -\nWe specify the key vault, cpandlkeyvault.\nScenario: The database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn -\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nBox 2: PostgreSQLConn -\nWe specify the secret, PostgreSQLConn\nExample, sample request:\nhttps://myvault.vault.azure.net//secrets/mysecretname/\n4387e9f3d6e14c459867679a90fd0f79?api-version=7.1\nBox 3: Querystring -\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/keyvault/getsecret/getsecret\nCommunity Discussion\nAs per requirement: - Azure Key Vault name: cpandlkeyvault - Secret name:\nPostgreSQLConn - Id: 80df3e46ffcd4f1cb187f79905e9a1e8 https://myvault.vault.azure.net//\nsecrets/mysecretname/4387e9f3d6e14c459867679a90fd0f79?api-version=7.1 Box 1:\ncpandlkeyvault We specify the key vault, cpandlkeyvault. Box 2: PostgreSQLConn We\nspecify the secret, PostgreSQLConn. Box 3: Environment If a reference is not resolved\nproperly, the reference value will be used instead. This means that for application\nsettings, an environment variable would be created\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) City Power & Light company 50 13 1 API\nendpoint to Key Vault + variable 50 13 2 create and import certificate in azure web app 50\n13 3 configure APIM for authentication with JWT 50 13 4 authenticate user by JWT 50 13 5\nMI Authentication to Azure Logic app 51 13 6 Azure Service Bus to Event Grid integration\nTier of bus + RBAC role 52 17 1 Application Insights/Monitor/Log Analytics 52 17 2 Azure\nBlob storage settings SAS or MI, file auditing 52 18 1 solve function timeout 55 24 1 cli for\nintegration Azure Service Bus and Azure Event Grid 55 24 2 ingestion for Grid events 58 32\n1 create appropriate storage account + geo settings + cool/hot\n\n\nBox 3 is QueryString (query string to specify the API version along with the secret\nversion), the other part is fine. (Answer provided is CORRECT)\nBox 3 is QueryString (query string to specify the API version along with the secret\nversion), the other part is fine. (Answer provided is CORRECT)\nGot this case on my exam 2023July13. Went with highest wote and scored 917.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "291",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion DRAG DROP -\nYou need to correct the corporate website error.\nWhich four actions should you recommend be performed in sequence? To\nanswer, move the appropriate actions from the list of actions to the answer\narea and arrange them in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nScenario: Corporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nStep 1: Generate a certificate -\nStep 2: Upload the certificate to Azure Key Vault\nScenario: All SSL certificates and credentials must be stored in Azure Key Vault.\nStep 3: Import the certificate to Azure App Service\nStep 4: Update line SCO5 of Security.cs to include error handling and then redeploy the\ncode\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/configure-ssl-certificate\nCommunity Discussion\nAnswer: • Generate a certificate • Upload the certificate to Azure key vault • Import the\ncertificate to Azure App Service • Add the certificate thumbprint to the\nWEBSITE_LOAD_CERTIFICATES app setting https://ankitvijay.net/2021/04/14/certificate-\nazure-app-service-linux/\n1. Generate a certificate. 2. Upload the certificate to Azure Key Vault. 3. Import the\ncertificate to Azure App Service. 4. Add the certificate thumbprint to the\nWEBSITE_LOAD_CERTIFICATES app setting. Prerequisite: Scale up your App Service plan if\nyour app is in a shared infrastructure tier. https://docs.microsoft.com/en-us/azure/app-\n\n\nservice/configure-ssl-certificate https://docs.microsoft.com/en-us/azure/app-service/\nconfigure-ssl-certificate-in-code#load-certificate-in-linuxwindows-containers\nreceived 2023-04-17 went with above answer, score 926\nreceived 2023-04-17 went with above answer, score 926\nGot this on Dec 16th, 2022. Scored 921 and went with: • Generate a certificate • Upload the\ncertificate to Azure key vault • Import the certificate to Azure App Service • Add the\ncertificate thumbprint to the WEBSITE_LOAD_CERTIFICATES app setting",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "292",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to configure API Management for authentication.\nWhich policy values should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Validate JWT -\nThe validate-jwt policy enforces existence and validity of a JWT extracted from either a\nspecified HTTP Header or a specified query parameter.\nScenario: User authentication (see step 5 below)\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an\naccess token.\n5. The web application calls an API and includes the access token in the authentication\nheader. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\nIncorrect Answers:\n✑ Limit call rate by key - Prevents API usage spikes by limiting call rate, on a per key\nbasis.\n✑ Restrict caller IPs - Filters (allows/denies) calls from specific IP addresses and/or\naddress ranges.\n✑ Check HTTP header - Enforces existence and/or value of a HTTP Header.\nBox 2: Outbound -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-\nrestriction-policies\nCommunity Discussion\n\n\nThe second box should be Inbound instead of Outbound. https://docs.microsoft.com/en-\nus/azure/api-management/api-management-access-restriction-policies Quoting: This\npolicy can be used in the following policy sections and scopes. Policy sections: inbound\nPolicy scopes: all scopes\nBox 1: Validate JWT The validate-jwt policy enforces existence and validity of a JWT\nextracted from either a specified HTTP Header or a specified query parameter. Box 2:\nInbound Authentication should be done on Incoming Request and that should be done\nin Inbound section of the policy of course. This policy can be used in the following policy\nsections and scopes. Policy sections: inbound Policy scopes: all scopes Reference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-\nrestriction-policies https://docs.microsoft.com/en-us/azure/api-management/api-\nmanagement-access-restriction-policies#ValidateJWT https://docs.microsoft.com/en-us/\nazure/api-management/api-management-access-restriction-policies\nyou are correct. it should be INBOUND for Validate JWT https://docs.microsoft.com/en-\nus/azure/api-management/api-management-access-restriction-policies#ValidateJWT\nyou are correct. it should be INBOUND for Validate JWT https://docs.microsoft.com/en-\nus/azure/api-management/api-management-access-restriction-policies#ValidateJWT\nreceived 2023-04-17 went with above answer, score 926",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "293",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion You need to authenticate the user to the corporate website as\nindicated by the architectural diagram.\nWhich two values should you use? Each correct answer presents part of the\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "ID token signature",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "ID token claims",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "HTTP response code",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure AD endpoint URI \nE) Azure AD tenant ID",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an\nPage 778 of 1410\n779 Microsoft - AZ-204 Practice Questions - SecExams.com\naccess token.\n5. The web application calls an API and includes the access token in the authentication\nheader. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-\nrestriction-policies",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "D\nA: Claims in access tokens -\nJWTs (JSON Web Tokens) are split into three pieces:\n✑ Header - Provides information about how to validate the token including information\nabout the type of token and how it was signed.\n✑ Payload - Contains all of the important data about the user or app that is attempting\nto call your service.\n✑ Signature - Is the raw material used to validate the token.\nE: Your client can get an access token from either the v1.0 endpoint or the v2.0 endpoint\nusing a variety of protocols.\nScenario: User authentication (see step 5 below)\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an"
  },
  {
    "number": "294",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to correct the Azure Logic app error message.\nWhich configuration values should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nScenario: You test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nNote: If the inbound call's request body doesn't match your schema, the trigger returns\nan HTTP 400 Bad Request error.\nBox 1: function -\nIf you have an Azure function where you want to use the system-assigned identity, first\nenable authentication for Azure functions.\nBox 2: system-assigned -\nYour logic app or individual connections can use either the system-assigned identity or a\nsingle user-assigned identity, which you can share across a group of logic apps, but not\nboth.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity\nCommunity Discussion\nLogic app: You test the Logic app in a development environment. The following error\nmessage displays: '400 Bad Request'. Troubleshooting of the error shows an HttpTrigger\naction to call the RequestUserApproval function. Box 1: anonymous To use your logic\napp's managed identity in your function, you must set your function's authentication\nlevel to anonymous. Otherwise, your logic app throws a \"BadRequest\" error. Box 2:\nsystem-assigned Your logic app or individual connections can use either the system-\nassigned identity or a single user-assigned identity, which you can share across a group\n\n\nof logic apps, but not both. On your logic app menu, under Settings, select Identity >\nSystem assigned\nAccording to this article Function authz should be set to anonymous as we're using AAD\nauth. https://adatum.no/azure/azure-ad-authentication-in-azure-functions\nReference: https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-azure-\nfunctions#set-up-anonymous-authentication-in-your-function\nReference: https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-azure-\nfunctions#set-up-anonymous-authentication-in-your-function\nFor some weird reason Microsoft decided that logic apps cannot use the user-assigned\nidentity when it is given to multiple. Totally random restriction that shows the inter-\ndepartmental issues in this organization. And they think that fault-line is a good\nquestion to ask.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "295",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to configure Azure Service Bus to Event Grid integration.\nWhich Azure Service Bus settings should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Premium -\nService Bus can now emit events to Event Grid when there are messages in a queue or a\nsubscription when no receivers are present. You can create Event Grid subscriptions to\nyour Service Bus namespaces, listen to these events, and then react to the events by\nstarting a receiver. With this feature, you can use Service\nBus in reactive programming models.\nTo enable the feature, you need the following items:\nA Service Bus Premium namespace with at least one Service Bus queue or a Service Bus\ntopic with at least one subscription.\nContributor access to the Service Bus namespace.\nBox 2: Contributor -\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-to-event-\ngrid-integration-concept\nCommunity Discussion\nTo enable the feature, you need the following items: - A Service Bus Premium namespace\nwith at least one Service Bus queue or a Service Bus topic with at least one subscription.\n- Contributor access to the Service Bus namespace. Box 1: Premium Box 2: Contributor\nReference: https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-\nto-event-grid-integration-concept\nThe answer is correct\n\n\nTier = Premium, because the requirement says \"Data must be replicated to a secondary\nregion and three availability zones\". Standard and basic don't support Geo Disasster\nRecover and Availability Zones.\nTier = Premium, because the requirement says \"Data must be replicated to a secondary\nregion and three availability zones\". Standard and basic don't support Geo Disasster\nRecover and Availability Zones.\nAzure Service Bus to Event Grid integration overview To enable the feature, you need the\nfollowing items: A Service Bus *Premium* namespace with at least one Service Bus\nqueue or a Service Bus topic with at least one subscription. *Contributor access* to the\nService Bus namespace. Navigate to your Service Bus namespace in the Azure portal, and\nthen select Access control (IAM), and select Role assignments tab. Verify that you have\nthe contributor access to the namespace. https://docs.microsoft.com/en-us/azure/\nservice-bus-messaging/service-bus-to-event-grid-integration-concept?tabs=event-grid-\nevent-schema",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "296",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\nEmployees may upload receipts in two ways:\n\n\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\nenabled.\n\n\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\nthe file in the file share and use the web application, which returns a 500\n\n\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion HOTSPOT -\nYou need to add code at line PC26 of Processing.cs to ensure that security\npolicies are met.\nHow should you complete the code that you will add at line PC26? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: var key = await\nResolver.ResolveKeyAsyn(keyBundle,KeyIdentifier.CancellationToken.None);\nBox 2: var x = new BlobEncryptionPolicy(key,resolver);\nExample:\n// We begin with cloudKey1, and a resolver capable of resolving and caching Key Vault\nsecrets.\nBlobEncryptionPolicy encryptionPolicy = new BlobEncryptionPolicy(cloudKey1,\ncachingResolver); client.DefaultRequestOptions.EncryptionPolicy = encryptionPolicy;\nBox 3: cloudblobClient. DefaultRequestOptions.EncryptionPolicy = x;\nReference:\nhttps://github.com/Azure/azure-storage-net/blob/master/Samples/GettingStarted/\nEncryptionSamples/KeyRotation/Program.cs\nCommunity Discussion\nThe answer is correct\nAlso Microsoft says : you need not know .Net for this certification and proceed to ask\nquestions deep in the dark corners of .Net libraries\nBlobEncryptionPolicy accept \"Ikey\" on constructor => https://docs.microsoft.com/en-us/\ndotnet/api/microsoft.azure.storage.blob.blobencryptionpolicy.-ctor?view=azure-dotnet-\n\n\nlegacy#Microsoft_Azure_Storage_Blob_BlobEncryptionPolicy__ctor_Microsoft_Azure_KeyVault_Core_IKey_Microsoft_Azure_KeyVault_Core_IKeyResolver_\n(keyBundle.Key return a Microsoft.Azure.KeyVault.WebKey.JsonWebKey). The answer is\ncorrect !\n1. var key = keyBundle.Key; 2. var x = new BlobEncryptionPolicy(key, resolver); 3.\ncloudBlobClient.DefaultRequestOptions.EncryptionPolicy = x; But I'm afraid I've wasted\nmy time. As you can see in the links, it's all legacy code. https://docs.microsoft.com/en-\nus/dotnet/api/microsoft.azure.keyvault.keyvaultclientextensions.getkeyasync?\nview=azure-dotnet-legacy&viewFallbackFrom=azure-dotnet https://docs.microsoft.com/\nen-us/dotnet/api/microsoft.azure.storage.blob.blobencryptionpolicy.-ctor?view=azure-\ndotnet-\nlegacy#Microsoft_Azure_Storage_Blob_BlobEncryptionPolicy__ctor_Microsoft_Azure_KeyVault_Core_IKey_Microsoft_Azure_KeyVault_Core_IKeyResolver_\nhttps://docs.microsoft.com/en-us/dotnet/api/\nmicrosoft.azure.storage.blob.blobrequestoptions.encryptionpolicy?view=azure-dotnet-\nlegacy#Microsoft_Azure_Storage_Blob_BlobRequestOptions_EncryptionPolicy\nTo be honest, if you are an experienced programmer you can guess the right answer to\nthis question by pure logic, no .Net code or even Azure knowledge is needed.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "297",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\nEmployees may upload receipts in two ways:\n\n\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\nenabled.\n\n\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\nthe file in the file share and use the web application, which returns a 500\n\n\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion You need to ensure the security policies are met.\nWhat code do you add at line CS07 of ConfigureSSE.ps1?",
    "options": [
      {
        "letter": "A",
        "text": "ג€\"PermissionsToKeys create, encrypt, decrypt",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "ג€\"PermissionsToCertificates create, encrypt, decrypt",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "ג€\"PermissionsToCertificates wrapkey, unwrapkey, get",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "ג€\"PermissionsToKeys wrapkey, unwrapkey, get",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: All certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which are\nessential to perform the intended function.\nThe Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToKeys specifies an array\nof key operation permissions to grant to a user or service principal.\nThe acceptable values for this parameter: decrypt, encrypt, unwrapKey, wrapKey, verify,\nsign, get, list, update, create, import, delete, backup, restore, recover, purge\nIncorrect Answers:\nA, C: The Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToCertificates\nspecifies an array of certificate permissions to grant to a user or service principal. The\nacceptable values for this parameter: get, list, delete, create, import, update,\nmanagecontacts, getissuers, listissuers, setissuers, deleteissuers, manageissuers, recover,\npurge, backup, restore\nReference:"
  },
  {
    "number": "298",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question You need to\nreduce read latency for the retail store solution.\nWhat are two possible ways to achieve the goal? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create a new composite index for the store location data queries in Azure Cosmos DB. Modify\nthe queries to support parameterized SQL and update the Azure Function app to call the new\nqueries.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Provision an Azure Cosmos DB dedicated gateway. Update the Azure Function app\nconnection string to use the new dedicated gateway endpoint.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Configure Azure Cosmos DB consistency to session consistency. Cache session tokens in a\nnew Azure Redis cache instance after every write. Update reads to use the session token\nstored in Azure Redis.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Provision an Azure Cosmos DB dedicated gateway. Update blob storage to use the new\ndedicated gateway endpoint.\nE) Configure Azure Cosmos DB consistency to strong consistency. Increase the RUs for the\ncontainer supporting store location data.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage"
  },
  {
    "number": "299",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question You need to\naudit the retail store sales transactions.\nWhat are two possible ways to achieve the goal? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Update the retail store location data upload process to include blob index tags. Create an\nAzure Function to process the blob index tags and filter by store location.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Process the change feed logs of the Azure Blob storage account by using an Azure Function.\nSpecify a time range for the change feed data.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Enable blob versioning for the storage account. Use an Azure Function to process a list of the\nblob versions per day.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Process an Azure Storage blob inventory report by using an Azure Function. Create rule filters\non the blob inventory report.\nE) Subscribe to blob storage events by using an Azure Function and Azure Event Grid. Filter the\nevents by store location.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "E\nScenario: Audit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\n\"Process the change feed logs of the Azure Blob storage account by using an Azure"
  },
  {
    "number": "300",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion You need to monitor ContentUploadService according to the\nrequirements.\nWhich command should you use?",
    "options": [
      {
        "letter": "A",
        "text": "az monitor metrics alert create ג€\"n alert ג€\"g ג€ג - -scopes ג€ג - -condition \"avg Percentage\nCPU > 8\"\nPage 831 of 1410\n832 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "az monitor metrics alert create ג€\"n alert ג€\"g ג€¦ - -scopes ג€¦ - -condition \"avg\nPercentage CPU > 800\"",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "az monitor metrics alert create ג€\"n alert ג€\"g ג€ג - -scopes ג€ג - -condition \"CPU Usage >\n800\"",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "az monitor metrics alert create ג€\"n alert ג€\"g ג€ג - -scopes ג€ג - -condition \"CPU Usage > 8\"",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: An alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\nReference:\nhttps://docs.microsoft.com/sv-se/cli/azure/monitor/metrics/alert"
  },
  {
    "number": "301",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion You need to investigate the http server log output to resolve the issue\nwith the ContentUploadService.\nWhich command should you use first?",
    "options": [
      {
        "letter": "A",
        "text": "az webapp log",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "az ams live-output\nPage 837 of 1410\n838 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "az monitor activity-log",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "az container attach",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Scenario: Users of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\n\"502 bad gateway\" and \"503 service unavailable\" are common errors in your app hosted\nin Azure App Service.\nMicrosoft Azure publicizes each time there is a service interruption or performance\ndegradation.\nThe az monitor activity-log command manages activity logs.\nNote: Troubleshooting can be divided into three distinct tasks, in sequential order:\n1. Observe and monitor application behavior\n2. Collect data\n3. Mitigate the issue\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/monitor/activity-log"
  },
  {
    "number": "302",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion You need to investigate the Azure Function app error message in the\ndevelopment environment.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Connect Live Metrics Stream from Application Insights to the Azure Function app and filter\nthe metrics.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create a new Azure Log Analytics workspace and instrument the Azure Function app with\nApplication Insights.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update the Azure Function app with extension methods from Microsoft.Extensions.Logging to\nlog events by using the log instance.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add a new diagnostic setting to the Azure Function app to send logs to Log Analytics.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "- Add new diagnostics settings. https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-monitor-log-analytics?\ntabs=csharp \"From the Monitoring section of your function app in the Azure portal, select\nDiagnostic settings, and then select Add diagnostic setting.\" Live Metrics is used for\nmonitoring a live system and should not be necessary in a development environment,\nbecause there are not that many events to use the \"big canon\". https://\ndocs.microsoft.com/en-us/azure/azure-monitor/app/live-stream\nThe question specifically says \"in a development environment\". Live Metrics Stream is\nrecommended for production applications. I would go for D. https://docs.microsoft.com/\nen-us/azure/azure-monitor/app/live-stream\nshouldn't be D as mentioned in Issues section it's 400 error https://docs.microsoft.com/\nen-us/azure/azure-functions/functions-diagnostics\nPage 845 of 1410\n846 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure Functions offers built-in integration with Azure Application Insights to monitor\nfunctions.\nThe following areas of Application Insights can be helpful when evaluating the behavior,\nperformance, and errors in your functions:\nLive Metrics: View metrics data as it's created in near real-time.\nFailures -\nPerformance -\nMetrics -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-monitoring"
  },
  {
    "number": "303",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to configure security and compliance for the corporate website files.\nWhich Azure Blob storage settings should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: role-based access control (RBAC)\nAzure Storage supports authentication and authorization with Azure AD for the Blob and\nQueue services via Azure role-based access control (Azure RBAC).\nScenario: File access must restrict access by IP, protocol, and Azure AD rights.\nBox 2: storage account type -\nScenario: The website uses files stored in Azure Storage\nAuditing of the file updates and transfers must be enabled to comply with General Data\nProtection Regulation (GDPR).\nCreating a diagnostic setting:\n1. Sign in to the Azure portal.\n2. Navigate to your storage account.\n3. In the Monitoring section, click Diagnostic settings (preview).\n4. Choose file as the type of storage that you want to enable logs for.\n5. Click Add diagnostic setting.\nReference:\n\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-introduction https://\ndocs.microsoft.com/en-us/azure/storage/files/storage-files-monitoring\nCommunity Discussion\nshared access signature (SAS) token change feed\nBox 1: shared access signature (SAS) token According to the diagram, blob storage is\naccessed from Azure CDN. Azure CDN doesn't support authentication with managed\nidentity. If you want to grant limited access to private storage containers, you can use the\nShared Access Signature (SAS) feature of your Azure storage account. Also, using a\nmanaged identity you can't restrict access by IP as requested. Box 2: change feed The\npurpose of the change feed is to provide transaction logs of all the changes that occur to\nthe blobs and the blob metadata in your storage account. The file updates must be read-\nonly, stored in the order in which they occurred, include only create, update, delete, and\ncopy operations, and be retained for compliance reasons. Reference: https://\ndocs.microsoft.com/en-us/azure/cdn/cdn-sas-storage-support https://\ndocs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-\nportal\nProfessor is back :)))\nProfessor is back :)))\nreceived 2023-04-17 went with above, score 926",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "304",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion You need to correct the RequestUserApproval Function app error.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Update line RA13 to use the async keyword and return an HttpRequest object value.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure the Function app to use an App Service hosting plan. Enable the Always On setting\nof the hosting plan.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update the function to be stateful by using Durable Functions to process the request\npayload.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Update the functionTimeout property of the host.json project file to 15 minutes.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Async operation tracking -\nThe HTTP response mentioned previously is designed to help implement long-running\nHTTP async APIs with Durable Functions. This pattern is sometimes referred to as the\npolling consumer pattern.\nBoth the client and server implementations of this pattern are built into the Durable\nFunctions HTTP APIs.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error\nmessage displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development\nenvironment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nReference:"
  },
  {
    "number": "305",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion DRAG\nDROP -\nYou need to implement the Log policy.\nHow should you complete the Azure Event Grid subscription? To answer, drag\nthe appropriate JSON segments to the correct locations. Each JSON segment\nmay be used once, more than once, or not at all. You may need to drag the split\nbar between panes to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1:WebHook -\nScenario: If an anomaly is detected, an Azure Function that emails administrators is\ncalled by using an HTTP WebHook. endpointType: The type of endpoint for the\nsubscription (webhook/HTTP, Event Hub, or queue).\nBox 2: SubjectBeginsWith -\nBox 3: Microsoft.Storage.BlobCreated\nScenario: Log Policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should\nbe saved to a container named logdrop. Logs must remain in the container for 15 days.\nExample subscription schema -\n{\n\"properties\": {\n\"destination\": {\n\"endpointType\": \"webhook\",\n\"properties\": {\n\"endpointUrl\": \"https://example.azurewebsites.net/api/HttpTriggerCSharp1?\ncode=VXbGWce53l48Mt8wuotr0GPmyJ/nDT4hgdFj9DpBiRt38qqnnm5OFg==\"\n}\n},\n\"filter\": {\n\"includedEventTypes\": [ \"Microsoft.Storage.BlobCreated\", \"Microsoft.Storage.BlobDeleted\"\n],\n\"subjectBeginsWith\": \"blobServices/default/containers/mycontainer/log\",\n[1]\n\"isSubjectCaseSensitive \": \"true\"\n}\n}\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema\n\n\nCommunity Discussion\nLooks correct based on the provided reference. https://docs.microsoft.com/en-us/azure/\nevent-grid/subscription-creation-schema\nTo prevent reading the cases multiple times: Please see the spots below where you can\nfind the questions (page/topic/question/subject) Proseware, Inc. 53 19 1 ARM for Azure\nEvent Grid subscription, enable + filter logging 53 19 2 use Application Insights for scaling\n53 19 3 code : telemetry filter/ telemetry processor 53 19 4 code : telemetry initializer 55\n25 1 code : if-then condition on properties of incomming event from GRID 56 25 2 code :\nclass for login event, properties needed 56 25 3 code : programatically update application\nsettings for blob 56 26 1 azure function delay : always on and tier\nmarvelous microsoftiness, probably this is for the subscription for the anomaly detection\nservice and not the policy service where they expliclty mentioned event grid to confuse\nyou. Once you pass this exam, the certification can be used when applying for a job at\nCSI new york. For finding clues where they do not exist.\nmarvelous microsoftiness, probably this is for the subscription for the anomaly detection\nservice and not the policy service where they expliclty mentioned event grid to confuse\nyou. Once you pass this exam, the certification can be used when applying for a job at\nCSI new york. For finding clues where they do not exist.\nThe question is really worded poorly. The subscription is for events that are inserted into\nthe container and a webhook is invoked when a log file is created in the storage\ncontainer. I assume that the webhook point to a function (?) that analyze the log with a\nmachine learning model and THEN if there is an anomaly call an azure function to notify\nadminstrator via mail. Am I missing something ?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "306",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion You\nneed to ensure that the solution can meet the scaling requirements for Policy\nService.\nWhich Azure Application Insights data model should you use?",
    "options": [
      {
        "letter": "A",
        "text": "an Application Insights dependency",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "an Application Insights event",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an Application Insights trace",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "an Application Insights metric",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Application Insights provides three additional data types for custom telemetry:\nTrace - used either directly, or through an adapter to implement diagnostics logging\nusing an instrumentation framework that is familiar to you, such as Log4Net or\nSystem.Diagnostics.\nEvent - typically used to capture user interaction with your service, to analyze usage\npatterns.\nMetric - used to report periodic scalar measurements.\nScenario:\nPolicy service must use Application Insights to automatically scale with the number of\npolicy actions that it is performing.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/data-model"
  },
  {
    "number": "307",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion DRAG\nDROP -\nYou need to implement telemetry for non-user actions.\nHow should you complete the Filter class? To answer, drag the appropriate code\nsegments to the correct locations. Each code segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\n\n\nExplanation\nCorrect Answer:\nScenario: Exclude non-user actions from Application Insights telemetry.\nBox 1: ITelemetryProcessor -\nTo create a filter, implement ITelemetryProcessor. This technique gives you more direct\ncontrol over what is included or excluded from the telemetry stream.\nBox 2: ITelemetryProcessor -\nBox 3: ITelemetryProcessor -\nBox 4: RequestTelemetry -\nBox 5: /health -\nTo filter out an item, just terminate the chain.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling\nCommunity Discussion\n(∩^o^)⊃━☆\n\n\n\"ITelemetryProcessor\" typo, no \"n\" https://docs.microsoft.com/en-us/azure/azure-\nmonitor/app/api-filtering-sampling#create-a-telemetry-processor-c\nAnswer: ITelemetryProcessor, ITelemetryProcessor,ITelemetryProcessor,RequestTelemetry,\n/health Reference: https://docs.microsoft.com/en-us/azure/azure-monitor/app/api-\nfiltering-sampling public class SuccessfulDependencyFilter : ITelemetryProcessor { private\nITelemetryProcessor Next { get; set; } // next will point to the next TelemetryProcessor in\nthe chain. public SuccessfulDependencyFilter(ITelemetryProcessor next) { this.Next =\nnext; } } var requestTelemetry = telemetry as RequestTelemetry;\nAlso the constructor is incorrect. Should be \"public Filter(\" instead of \"public (Filter\".\nAlso the constructor is incorrect. Should be \"public Filter(\" instead of \"public (Filter\".",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "308",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion DRAG\nDROP -\nYou need to ensure that PolicyLib requirements are met.\nHow should you complete the code segment? To answer, drag the appropriate\ncode segments to the correct locations. Each code segment may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nScenario: You have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\n✑ Exclude non-user actions from Application Insights telemetry.\n✑ Provide methods that allow a web service to scale itself.\n✑ Ensure that scaling actions do not disrupt application usage.\nBox 1: ITelemetryInitializer -\nUse telemetry initializers to define global properties that are sent with all telemetry; and\nto override selected behavior of the standard telemetry modules.\nBox 2: Initialize -\nBox 3: Telemetry.Context -\nBox 4: ((EventTelemetry)telemetry).Properties[\"EventID\"]\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling\nCommunity Discussion\n* EventId is held by class EventGridController * You can add properties to telemetry by\nimplementing ITelemetryInitializer which defines the Initialize method. *\nITelemetry.Context.Properties is correct, but shouldnt be used any more as obsoloete\npublic class IncludeEventId : ITelemetryInitializer { public void Initialize (ITelemetry\ntelemetry) { telemetry.Context.Properties[\"EventId\"] = EventgridController.EventId.Value; } }\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-custom-events-\nmetrics#sampling-filtering-and-processing-telemetry https://docs.microsoft.com/en-us/\ndotnet/api/microsoft.applicationinsights.datacontracts.telemetrycontext.properties?\nview=azure-\ndotnet#Microsoft_ApplicationInsights_DataContracts_TelemetryContext_Properties\nNo, in my opinion it has no sense assign event id to telemetry object from the same\nobject itself. you should take in from other objects, and in this case the only option\npossible seems to be EventGridController class.\n\n\nNo, in my opinion it has no sense assign event id to telemetry object from the same\nobject itself. you should take in from other objects, and in this case the only option\npossible seems to be EventGridController class.\nBox 1: ITelemetryInitializer Box 2: Initialize Box 3: Telemetry.Context Box 4:\nEventGridController.EventId.Value Scenario: The application reacts to events from Azure\nEvent Grid and performs policy actions based on those events. The application must\ninclude the Event Grid Event ID field in all Application Insights telemetry. \\You can add\nproperties to telemetry by implementing ITelemetryInitializer which defines the Initialize\nmethod. https://docs.microsoft.com/en-us/azure/azure-monitor/app/api-custom-\nevents-metrics#sampling-filtering-and-processing-telemetry\nNot correct. = > Use the EventId static prop of EventgridController",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "309",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\n\n\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\n\n\nenabled.\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\n\n\nthe file in the file share and use the web application, which returns a 500\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion You need to ensure receipt processing occurs correctly.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use blob properties to prevent concurrency problems",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Use blob SnapshotTime to prevent concurrency problems",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Use blob metadata to prevent concurrency problems",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use blob leases to prevent concurrency problems",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "You can create a snapshot of a blob. A snapshot is a read-only version of a blob that's\ntaken at a point in time. Once a snapshot has been created, it can be read, copied, or\ndeleted, but not modified. Snapshots provide a way to back up a blob as it appears at a\nmoment in time.\nScenario: Processing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in\nAzure Blob Storage and an Azure SQL database. Then, an email summary is sent to the\nuser with a link to the processing report. The link to the report must remain valid if the\nemail is forwarded to another user.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/creating-a-snapshot-of-a-\nblob"
  },
  {
    "number": "310",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\n\n\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\n\n\nenabled.\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\n\n\nthe file in the file share and use the web application, which returns a 500\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion You need to resolve the capacity issue.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Convert the trigger on the Azure Function to an Azure Blob storage trigger",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Ensure that the consumption plan is configured correctly to allow scaling",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Move the Azure Function to a dedicated App Service Plan",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Update the loop starting on line PC09 to process items in parallel",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "replace the\nsolution with durable functions => looks even better than D If D2 is an option I'd go for\nthat. Maybe they realized that the current \"D\" is not a really good solution and D2 is also\nway more \"azure\" Otherwise D.\nI believe it's \"B\" - since issue happens in busy period when CPU is over-utilized. Then\nonly reasonable action will be to scale. And for that we should properly configure\nConsumption Plan. \"D\" - could be an answer if Q was about slow speed of processing in\nnormal situation, when CPU resources in enough. In this case, I/O operations are the\nbottleneck. But, if we try to spawn more thread when CPU is already super-busy, it would\neven worsen user experience. And it's not \"C\" since Dedicated Plan is used in very\nspecific situation. Exerpt: \"Consider a dedicated App Service plan in the following\nsituations: - You have existing, underutilized VMs that are already running other App\nService instances. - You want to provide a custom image on which to run your functions.\"\n1000% D !!!!!!!! CORRECT!\nPage 909 of 1410\n910 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": "If you want to read the files in parallel, you cannot use forEach. Each of the async\ncallback function calls does return a promise. You can await the array of promises that\nyou'll get with Promise.all.\nScenario: Capacity issue: During busy periods, employees report long delays between the\ntime they upload the receipt and when it appears in the web application.\nReference:\nhttps://stackoverflow.com/questions/37576685/using-async-await-with-a-foreach-loop"
  },
  {
    "number": "311",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution\nfor managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is\ncomplete, the employee receives a summary report email that details the\nprocessing results. Employees then use a web application to manage their\nreceipts and perform any additional tasks needed for reimbursement.\nReceipt processing -\n\n\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\nDocumentation -\nEmployees are provided with a getting started document when they first use the\nsolution. The documentation includes details on supported operating systems\nfor\nAzure File upload, and instructions on how to configure the mounted folder.\nSolution details -\nUsers table -\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security\nprincipal name WebAppIdentity.\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure\nFunction runtime. Once processing is completed, results are stored in Azure\nBlob\nStorage and an Azure SQL database. Then, an email summary is sent to the user\nwith a link to the processing report. The link to the report must remain valid if\nthe email is forwarded to another user.\nLogging -\nAzure Application Insights is used for telemetry and logging in both the\nprocessor and the web application. The processor also has TraceWriter logging\n\n\nenabled.\nApplication Insights must always contain all log messages.\nRequirements -\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations\nmust not be dependent on application running and must ensure that data in\nthe DR region is up to date.\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database\ndoes not allow the viewing of SecurityPins. The web application is the only\nsystem that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key\nVault.\nYou must adhere to the principle of least privilege and provide privileges which\nare essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's\nManaged Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All\nother views of the expense account number should include only the last\nsegment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked\nwithout impacting other parts of the system.\nIssues -\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web\napplication. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete\n\n\nthe file in the file share and use the web application, which returns a 500\nInternal\nServer error page.\nCapacity issue -\nDuring busy periods, employees report long delays between the time they\nupload the receipt and when it appears in the web application.\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the\nprocessor is too high, resulting in lost log messages.\nApplication code -\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\nConfigureSSE.ps1 -\n\n\nQuestion You need to resolve the log capacity issue.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create an Application Insights Telemetry Filter",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Change the minimum log level in the host.json file for the function",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Implement Application Insights Sampling",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Set a LogCategoryFilter during startup",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Scenario, the log capacity issue: Developers report that the number of log message in the\ntrace output for the processor is too high, resulting in lost log messages.\nSampling is a feature in Azure Application Insights. It is the recommended way to reduce\ntelemetry traffic and storage, while preserving a statistically correct analysis of\napplication data. The filter selects items that are related, so that you can navigate\nbetween items when you are doing diagnostic investigations. When metric counts are\npresented to you in the portal, they are renormalized to take account of the sampling, to\nminimize any effect on the statistics.\nSampling reduces traffic and data costs, and helps you avoid throttling.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling"
  },
  {
    "number": "312",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question HOTSPOT -\nYou need to implement event routing for retail store location data.\nWhich configurations should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Azure Blob Storage -\nAzure event publishers and event handlers are at the core of the Event Grid routing-\nservice. Event Grid listens to Azure event publishers, such as Blog Storage, then reacts by\nrouting specific events to Azure event handlers, such as WebHooks. You can easily\ncontrol this entire process at a granular level through event subscriptions and event\nfilters.\nBox 2: Azure Event Grid -\nAzure Event Grid is a highly scalable event-routing service that listens for specific system\nevents, then reacts to them according to your precise specifications. In the past, event\nhandling has relied largely on polling ג€\" a high latency, low-efficiency approach that can\nprove prohibitively expensive at scale.\nBox 3: Azure Logic App -\nEvent Grid's supported event handlers currently include Event Hubs, WebHooks, Logic\nApps, Azure Functions, Azure Automation and Microsoft Flow.\n\n\nReference:\nhttps://www.appliedi.net/blog/using-azure-event-grid-for-highly-scalable-event-routing\nCommunity Discussion\nShouldn't it be Azure Function App for the handler?\nThis was on the exam (July 2023). Went with blob/grid/function. Scored 917\nRetail store locations - Azure Functions must process data immediately when data is\nuploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native\nSQL language queries. Audit store sale transaction information nightly to validate data,\nprocess sales financials, and reconcile inventory. Logic app is not mentioned anywhere in\nthe case study. So #3 should be Function app?\nAgree, there is no logic app in the solution.\nAgree, there is no logic app in the solution.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "313",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nQuestion You\nneed to troubleshoot the order workflow.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Review the API connections.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Review the activity log.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Review the run history.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Review the trigger history. \nPage 930 of 1410\n931 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D\nScenario: The order workflow fails to run upon initial deployment to Azure.\nCheck runs history: Each time that the trigger fires for an item or event, the Logic Apps\nengine creates and runs a separate workflow instance for each item or event. If a run\nfails, follow these steps to review what happened during that run, including the status\nfor each step in the workflow plus the inputs and outputs for each step.\nCheck the workflow's run status by checking the runs history. To view more information\nabout a failed run, including all the steps in that run in their status, select the failed run.\nExample:\nCheck the trigger's status by checking the trigger history\nTo view more information about the trigger attempt, select that trigger event, for\nexample:\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-diagnosing-failures"
  },
  {
    "number": "314",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nQuestion\nHOTSPOT -\nYou need to update the order workflow to address the issue when calling the\nPrinter API App.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: fixed -\nThe 'Default' policy does 4 exponential retries and from my experience the interval times\nare often too short in situations.\nBox 2: PT60S -\nWe could set a fixed interval, e.g. 5 retries every 60 seconds (PT60S).\nPT60S is 60 seconds.\nScenario: Calls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only\nreceive up to 5 attempts within one minute.\nBox 3: 5 -\nReference:\nhttps://michalsacewicz.com/error-handling-in-power-automate/\nCommunity Discussion\n\n\n1. Fixed 2. PT10S 3. 5 PT10 means retry after 10sec , https://docs.microsoft.com/en-us/\nazure/logic-apps/logic-apps-exception-handling\nI think Given ans is right \"PT60 and count 5\" menas try 5 times in 60sec.\nIf you read the following: https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-\nexception-handling#fixed-interval pt60s = 60s delay between each attempt. PT10s will be\nthe better answer, 5 attempts with each 10 seconds delay satisfy the requirement “The\nlabel printer must only receive up to 5 attempts within one minute”.\nIf you read the following: https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-\nexception-handling#fixed-interval pt60s = 60s delay between each attempt. PT10s will be\nthe better answer, 5 attempts with each 10 seconds delay satisfy the requirement “The\nlabel printer must only receive up to 5 attempts within one minute”.\nAgreed. PT10S means a 10 second delay between each try defined as the retry value.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "315",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\n\n\nApp.\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\n\n\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nDRAG DROP -\nYou need to support the message processing for the ocean transport workflow.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Create an integration account in the Azure portal\nYou can define custom metadata for artifacts in integration accounts and get that\nmetadata during runtime for your logic app to use. For example, you can provide\nmetadata for artifacts, such as partners, agreements, schemas, and maps - all store\nmetadata using key-value pairs.\nStep 2: Link the Logic App to the integration account\nA logic app that's linked to the integration account and artifact metadata you want to\nuse.\nStep 3: Add partners, schemas, certificates, maps, and agreements\nStep 4: Create a custom connector for the Logic App.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/logic-apps/logic-apps-enterprise-\nintegration-metadata\nCommunity Discussion\nThis is about logic app, which is not part anymore of the exam However : 1.Create\nintegration account 2.Link Logic app 3.Add partners... 4.Update logic app https://\nlearn.microsoft.com/en-us/azure/logic-apps/add-artifacts-integration-service-\nenvironment-ise#create-integration-accounts Why not custom connector? X12 is Managed\nconnector, as it is opposed to a custom connector https://learn.microsoft.com/en-us/\nazure/logic-apps/add-artifacts-integration-service-environment-ise#add-ise-connectors\nSource?\nSource?\n\n\ndisregard above ans 1.Create integration account 2.Link Logic app 3.Add partners...\n4.Update logic app\ndisregard above ans 1.Create integration account 2.Link Logic app 3.Add partners...\n4.Update logic app",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "316",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has\ndeveloped several applications and services to support supply chain operations\nand would like to leverage serverless computing where possible.\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following\nworkflows:\nOcean Transport `\" This workflow gathers and validates container information\nincluding container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information\nincluding fuel usage, number of stops, and routes.\n\n\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight,\ncontents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports\nof call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display\non the shipping website.\nShipping Data -\nThe application uses MongoDB JSON document storage database for all\ncontainer and transport information.\nShipping Web Site -\nThe site displays shipping container tracking information and container\ncontents. The site is located at http://shipping.wideworldimporters.com/\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has\nbeen migrated to a new Standard_D16s_v3 Azure VM by using Azure Site\nRecovery and must remain running in Azure to complete the BizTalk component\nmigrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The\nAzure architecture diagram for the proposed solution is shown below:\nRequirements -\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic\n\n\nApp.\nSupport industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources\nwith a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final\nBizTalk migrations.\nShipping Function app -\nImplement secure function endpoints by using app-level security and include\nAzure Active Directory (Azure AD).\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring\nadditional costs.\nAutomatically scale to double capacity during peak shipping times while not\ncausing application downtime.\nMinimize costs when selecting an Azure payment model.\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance\nfor dynamic content while minimizing latency and costs.\nIssues -\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is\ncritical and has not been backed up in the past. The VM must enable a quick\nrestore from a 7-day snapshot to include in-place restore of disks in case of\nfailure.\nShipping website and REST APIs -\n\n\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-\nControl-Allow-Origin' header is present on the requested resource. Origin\n'http://test.wideworldimporters.com/' is therefore not allowed access. Question\nYou need to support the requirements for the Shipping Logic App.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Active Directory Application Proxy",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Site-to-Site (S2S) VPN connection",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "On-premises Data Gateway",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Point-to-Site (P2S) VPN connection",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Before you can connect to on-premises data sources from Azure Logic Apps, download\nand install the on-premises data gateway on a local computer. The gateway works as a\nbridge that provides quick data transfer and encryption between data sources on\npremises (not in the cloud) and your logic apps.\nThe gateway supports BizTalk Server 2016.\nNote: Microsoft have now fully incorporated the Azure BizTalk Services capabilities into\nLogic Apps and Azure App Service Hybrid Connections.\nLogic Apps Enterprise Integration pack bring some of the enterprise B2B capabilities like\nAS2 and X12, EDI standards support\nScenario: The Shipping Logic app must meet the following requirements:\n✑ Support the ocean transport and inland transport workflows by using a Logic App.\n✑ Support industry-standard protocol X12 message format for various messages\nincluding vessel content details and arrival notices.\n✑ Secure resources to the corporate VNet and use dedicated storage resources with a\nfixed costing model.\n✑ Maintain on-premises connectivity to support legacy applications and final BizTalk\nmigrations.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install"
  },
  {
    "number": "317",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to configure the integration for Azure Service Bus and Azure Event\nGrid.\nHow should you complete the CLI statement? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: eventgrid -\nTo create event subscription use: az eventgrid event-subscription create\n\n\nBox 2: event-subscription -\nBox 3: servicebusqueue -\nScenario: Azure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in\nbuffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to\nAzure Event Grid for processing.\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/eventgrid/event-subscription?view=azure-\ncli-latest#az_eventgrid_event_subscription_create\nCommunity Discussion\ncorrecto... https://docs.microsoft.com/en-us/azure/event-grid/handler-service-bus\nGot this one 01/2022. Went with most voted (to avoid writing answers again)\nAgreed. Note that the casus also denotes the opposite integration: \"Events from Azure\nService Bus and other Azure services must continue to be routed to Azure Event Grid for\nprocessing\". But none of the given options seem to do just this, so the given answer is\ncorrect.\nAgreed. Note that the casus also denotes the opposite integration: \"Events from Azure\nService Bus and other Azure services must continue to be routed to Azure Event Grid for\nprocessing\". But none of the given options seem to do just this, so the given answer is\ncorrect.\nreceived 2023-04-17 went with given answer, score 926",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "318",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion You need to ensure that all messages from Azure Event Grid are\nprocessed.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Event Grid topic",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Service Bus topic",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Service Bus queue",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure Storage queue\nE) Azure Logic App custom connector",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "As a solution architect/developer, you should consider using Service Bus queues when:\n✑ Your solution needs to receive messages without having to poll the queue. With\nService Bus, you can achieve it by using a long-polling receive operation using the TCP-\nbased protocols that Service Bus supports.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-\nservice-bus-queues-compared-contrasted"
  },
  {
    "number": "319",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion DRAG\nDROP -\nYou need to add code at line EG15 in EventGridController.cs to ensure that the\nLog policy applies to all services.\nHow should you complete the code? To answer, drag the appropriate code\nsegments to the correct locations. Each code segment may be used once, more\nthan once, or not at all. You may need to drag the split bar between panes or\nscroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nScenario, Log policy: All Azure App Service Web Apps must write logs to Azure Blob\nstorage.\nBox 1: Status -\nBox 2: Succeeded -\nBox 3: operationName -\nMicrosoft.Web/sites/write is resource provider operation. It creates a new Web App or\nupdates an existing one.\nReference:\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-\noperations\nCommunity Discussion\nAnswer seems to be correct. Once a new web app is created, an event is triggered from\nthe resource group... https://docs.microsoft.com/en-us/azure/event-grid/event-schema-\nresource-groups?tabs=event-grid-event-schema That event contains the status, which\nmust be Succeeded in order for it to make sense to apply the policy. The above site also\nmentions that the event[\"data\"] contains an operationName property for resource group\nevents, which in this case is an \"Microsoft.web/sites/write\" operation.\noperationName ... Name of the operation. status ... String describing the status of the\noperation. Some common values are: Started, In Progress, Succeeded, Failed, Active,\nResolved. https://docs.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log-\nschema#administrative-category Microsoft.Web/sites/Write ... Create a new Web App or\nupdate an existing one https://docs.microsoft.com/en-us/azure/role-based-access-\ncontrol/resource-provider-operations But why do we need to call EnsureLogging() upon\ncreating/updating the Web App? I cannot find such requirement in the assignment. Or\nam I wrong?\n\n\noperationName ... Name of the operation. status ... String describing the status of the\noperation. Some common values are: Started, In Progress, Succeeded, Failed, Active,\nResolved. https://docs.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log-\nschema#administrative-category Microsoft.Web/sites/Write ... Create a new Web App or\nupdate an existing one https://docs.microsoft.com/en-us/azure/role-based-access-\ncontrol/resource-provider-operations But why do we need to call EnsureLogging() upon\ncreating/updating the Web App? I cannot find such requirement in the assignment. Or\nam I wrong?\nConfusing is the opening brace - it should be \"(\" instead of \"{\" as it is a logical expression\nfor the \"if\" statement. ;-)\nConfusing is the opening brace - it should be \"(\" instead of \"{\" as it is a logical expression\nfor the \"if\" statement. ;-)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "320",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion HOTSPOT\n-\nYou need to insert code at line LE03 of LoginEvent.cs to ensure that all\nauthentication events are processed correctly.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: id -\nid is a unique identifier for the event.\nBox 2: eventType -\neventType is one of the registered event types for this event source.\nBox 3: dataVersion -\ndataVersion is the schema version of the data object. The publisher defines the schema\nversion.\nScenario: Authentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be processed\nas quickly as possible.\nThe following example shows the properties that are used by all event publishers:\n[\n{\n\"topic\": string,\n\"subject\": string,\n\"id\": string,\n\"eventType\": string,\n\"eventTime\": string,\n\"data\":{\nobject-unique-to-each-publisher\n},\n\n\n\"dataVersion\": string,\n\"metadataVersion\": string\n}\n]\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/event-schema\nCommunity Discussion\nThis question proves how ridiculous this test is.\nThis question proves how ridiculous this test is.\nThis question proves how ridiculous this test is.\nThis question proves how ridiculous this test is.\nThis question proves how ridiculous this test is.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "321",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion HOTSPOT\n-\nYou need to implement the Log policy.\nHow should you complete the EnsureLogging method in EventGridController.cs?\nTo answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: logdrop -\nAll log files should be saved to a container named logdrop.\nBox 2: 15 -\nLogs must remain in the container for 15 days.\nBox 3: UpdateApplicationSettings\nAll Azure App Service Web Apps must write logs to Azure Blob storage.\nReference:\nhttps://blog.hompus.nl/2017/05/29/adding-application-logging-blob-to-a-azure-web-\napp-service-using-powershell/\nCommunity Discussion\nanswer is correct\nRight, maybe MS needs even more complicated tests, so we can memorize even more\ninfo that we can find using Google in 10 minutes or less.\nRight, maybe MS needs even more complicated tests, so we can memorize even more\ninfo that we can find using Google in 10 minutes or less.\n\n\nFirst 2 are simple and straight from the requirements but why do you have to know a\nproperty name for a .NET class for the 3rd??\nBlobStoreAccountSAS() is just a method listed in the case on line EG64",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "322",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that\napplies a set of governance policies for Proseware's internal services, external\nservices, and applications. The application will also provide a shared library for\ncommon functionality.\nRequirements -\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named\nPolicy service to an Azure App Service Web App. The application reacts to events\nfrom Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application\n\n\nInsights telemetry.\nPolicy service must use Application Insights to automatically scale with the\nnumber of policy actions that it is performing.\nPolicies -\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log\nfiles should be saved to a container named logdrop. Logs must remain in the\ncontainer for 15 days.\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All\nauthentication events must be processed by Policy service. Sign outs must be\nprocessed as quickly as possible.\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common\nto all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\nOther -\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for\nanomalies. It is implemented as an Azure Machine Learning model. The model is\ndeployed as a web service. If an anomaly is detected, an Azure Function that\nemails administrators is called by using an HTTP WebHook.\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service\nendpoint.\nIssues -\n\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the\nprocess of being applied during the deployment.\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns\nand rejects connections.\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several\nminutes after an anomaly is detected.\nApp code -\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\n\n\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only and include a two-character prefix that denotes the specific\nfile to which they belong.\nQuestion You\nneed to resolve a notification latency issue.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Set Always On to true.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Ensure that the Azure Function is using an App Service plan.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Set Always On to false.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Ensure that the Azure Function is set to use a consumption plan.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B\nAzure Functions can run on either a Consumption Plan or a dedicated App Service Plan. If\nyou run in a dedicated mode, you need to turn on the Always On setting for your\nFunction App to run properly. The Function runtime will go idle after a few minutes of\ninactivity, so only HTTP triggers will actually \"wake up\" your functions. This is similar to\nhow WebJobs must have Always On enabled.\nScenario: Notification latency: Users report that anomaly detection emails can\nsometimes arrive several minutes after an anomaly is detected.\nAnomaly detection service: You have an anomaly detection service that analyzes log\ninformation for anomalies. It is implemented as an Azure Machine Learning model. The\nmodel is deployed as a web service.\nIf an anomaly is detected, an Azure Function that emails administrators is called by using"
  },
  {
    "number": "323",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\nMessages are sent to ContentUploadService.\nContent is processed by ContentAnalysisService.\nAfter processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion HOTSPOT -\nYou need to ensure that validation testing is triggered per the requirements.\nHow should you complete the code segment? To answer, select the appropriate\nvalues in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: RepositoryUpdated -\nWhen a new version of the ContentAnalysisService is available the previous seven days of\ncontent must be processed with the new version to verify that the new version does not\nsignificantly deviate from the old version.\nBox 2: service -\nBox 3: imageCollection -\nReference:\nhttps://docs.microsoft.com/en-us/azure/devops/notifications/oob-supported-event-\ntypes\nCommunity Discussion\nIncorrect... 1. ImagePushed 2. repository 3. topic https://docs.microsoft.com/hu-hu/\nazure/event-grid/event-schema-container-registry?tabs=event-grid-event-schema\nOk, I need to start memorizing all schemes of possible event system topics: Azure API\nManagement Azure App Configuration Azure App Service Azure Blob Storage Azure Cache\nfor Redis Azure Communication Services Azure Container Registry Azure Event Hubs Azure\n\n\nFarmBeats Azure Health Data Services Azure IoT Hub Azure Key Vault Azure Kubernetes\nService Azure Machine Learning Azure Maps Azure Media Services Azure Policy Azure\nresource groups Azure Service Bus Azure SignalR Azure subscriptions....\nThe number of questions with incorrect responses is overwhelming. SecExams should\nmark the correct answers properly to make life easier for us. :(\nthanks for the tip! The answer indicated by the test seems to be completely \"off topic\" ;-)\nthanks for the tip! The answer indicated by the test seems to be completely \"off topic\" ;-)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "324",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\nMessages are sent to ContentUploadService.\nContent is processed by ContentAnalysisService.\nAfter processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion You need to deploy the CheckUserContent Azure Function. The\nsolution must meet the security and cost requirements.\nWhich hosting model should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Premium plan",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "App Service plan",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Consumption plan\nPage 1012 of 1410\n1013 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario:\nYou must minimize costs for all Azure services.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nBest for long-running scenarios where Durable Functions can't be used. Consider an App\nService plan in the following situations:\n✑ You have existing, underutilized VMs that are already running other App Service\ninstances.\n✑ You want to provide a custom image on which to run your functions.\n✑ Predictive scaling and costs are required.\nNote: When you create a function app in Azure, you must choose a hosting plan for your\napp. There are three basic hosting plans available for Azure Functions:\nConsumption plan, Premium plan, and Dedicated (App Service) plan.\nIncorrect Answers:\nA: A Premium plan would be more costly.\nC: Need the VNET functionality.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale"
  },
  {
    "number": "325",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nOrder.json -\nQuestion DRAG DROP -\nYou need to deploy a new version of the LabelMaker application to ACR.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\n\n\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Build a new application image by using dockerfile\nStep 2: Create an alias if the image with the fully qualified path to the registry\nBefore you can push the image to a private registry, you've to ensure a proper image\nname. This can be achieved using the docker tag command. For demonstration purpose,\nwe'll use Docker's hello world image, rename it and push it to ACR.\n# pulls hello-world from the public docker hub\n$ docker pull hello-world\n# tag the image in order to be able to push it to a private registry\n$ docker tag hello-word <REGISTRY_NAME>/hello-world\n# push the image\n\n\n$ docker push <REGISTRY_NAME>/hello-world\nStep 3: Log in to the registry and push image\nIn order to push images to the newly created ACR instance, you need to login to ACR\nform the Docker CLI. Once logged in, you can push any existing docker image to your ACR\ninstance.\nScenario:\nCoho Winery plans to move the application to Azure and continue to support label\ncreation.\nLabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads\nthat are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nReference:\nhttps://thorsten-hans.com/how-to-use-a-private-azure-container-registry-with-\nkubernetes-9b86e67b93b6 https://docs.microsoft.com/en-us/azure/container-registry/\ncontainer-registry-tutorial-quick-task\nCommunity Discussion\nCorrect answer given. Step 1: Build a new application image by using dockerfile FROM\nnode:8.9.3-alpine RUN mkdir -p /usr/src/app COPY ./app/ /usr/src/app/ WORKDIR /usr/\nsrc/app RUN npm install CMD node /usr/src/app/index.js docker build ./aci-helloworld -\nt aci-test-app docker images docker run -d -p 8080:80 aci-test-app (Example to build\nimage only and run locally) Step 2: Create an alias if the image with the fully qualified\npath to the registry docker tag mcr.microsoft.com/oss/nginx/nginx:1.15.5-alpine\nmyregistry.azurecr.io/samples/nginx Step 3: Log in to the registry and push image docker\npush myregistry.azurecr.io/samples/nginx\nCorrect https://docs.microsoft.com/en-us/azure/container-registry/container-registry-\nget-started-docker-cli?tabs=azure-cli\nIt is a Microsoft question. Keep intelligence out of it\nIt is a Microsoft question. Keep intelligence out of it\nStrange question. I realize that from the provided options this is the only solution one\ncan assemble. But the project describes LabelMaker app to be a set of images running in\n\n\nAKS. Pushing a new image into ACR won't change a thing, as one needs to update the\nYAML deployment file for Kubernetes to pick up the image, and then tell AKS to reconcile.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "326",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You\nare a developer implementing highly scalable and resilient applications to\nsupport online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles.\nThe application sends data to several printers. The application consists of five\nmodules that run independently on virtual machines (VMs). Coho Winery plans\nto move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork\nand text for custom label designs.\nRequirements. Data -\nYou identify the following requirements for data management and\n\n\nmanipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All\nreads to the Order data must fetch the most recent writes.\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to\ndocuments, resources, and applications to external partners.\nExternal partners must use their own credentials and authenticate with their\norganization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user\naccount administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key\nVault.\nE-commerce application sign-ins must be secured by using Azure App Service\nauthentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect\ncompany content.\nThe LabelMaker application must be secured by using an AAD account that has\nfull access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of\nworkloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS\ndeployment.\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication\ntimeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must\nonly receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included\nfor reference only.\nThis JSON file contains a representation of the data for an order that includes a\nsingle item.\n\n\nOrder.json -\nQuestion You need to access\ndata from the user claim object in the e-commerce web app.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Write custom code to make a Microsoft Graph API call from the e-commerce web app.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Assign the Contributor RBAC role to the e-commerce web app by using the Resource Manager\ncreate role assignment API.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update the e-commerce web app to read the HTTP request header values.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Using the Azure CLI, enable Cross-origin resource sharing (CORS) from the e-commerce\nPage 1027 of 1410\n1028 Microsoft - AZ-204 Practice Questions - SecExams.com\ncheckout API to the e-commerce web app.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Methods to Get User Identity and Claims in a .NET Azure Functions App include:\n✑ ClaimsPrincipal from the Request Context\nThe ClaimsPrincipal object is also available as part of the request context and can be\nextracted from the HttpRequest.HttpContext.\n✑ User Claims from the Request Headers.\nApp Service passes user claims to the app by using special request headers.\nReference:\nhttps://levelup.gitconnected.com/four-alternative-methods-to-get-user-identity-and-\nclaims-in-a-net-azure-functions-app-df98c40424bb"
  },
  {
    "number": "327",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question HOTSPOT -\nYou need to implement the retail store location Azure Function.\nHow should you configure the solution? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nScenario: Retail store locations: Azure Functions must process data immediately when\ndata is uploaded to Blob storage.\nBox 1: HTTP -\nBinding configuration example: https://<storage_account_name>.blob.core.windows.net\nBox 2: Input -\nRead blob storage data in a function: Input binding\nBox 3: Blob storage -\nThe Blob storage trigger starts a function when a new or updated blob is detected.\nAzure Functions integrates with Azure Storage via triggers and bindings. Integrating with\nBlob storage allows you to build functions that react to changes in blob data as well as\nread and write values.\nReference:\n\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-\nblob-trigger\nCommunity Discussion\n1. azure cosmos 2. output 3. blob storage\n1. (binding) = Azure CosmosDB reason: Azure Functions must update Azure Cosmos DB 2.\n(direction) = output reason: all triggers are input, so we are not talking about the trigger\nbinding. we must update Azure Cosmos DB so we need output binding. If input and\noutput were selectable, i'd probably go for that 3. (trigger) = EventGrid reason: the azure\nfunction is triggered by an EventGrid event so that processing happens immediately\nSeems ok, te person who writes answers in sec exams doesn't read questions...\nSeems ok, te person who writes answers in sec exams doesn't read questions...\nAgreed: We need write to cosmos output, so that deals box1 and 2, and func is triggered\nby changes on bloob",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "328",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question HOTSPOT -\nYou need to implement the corporate website.\nHow should you configure the solution? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Standard -\nBelow is a high-level comparison of the features as per the pricing tier for the App\nService Plan.\n\n\nNote: Corporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The\nwebsite consists of a React JavaScript user interface, HTML, CSS, image assets, and\nseveral APIs hosted in Azure Functions.\nCorporate website requirements:\n✑ Secure the website by using SSL.\n✑ Minimize costs for data storage and hosting.\n✑ Implement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\n✑ Distribute the website content globally for local use.\n✑ Implement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\n✑ The website must have 99.95 percent uptime.\nBox 2: App Service Web App -\nA Web App is a web application that is hosted in an App Service. The App Service is the\n\n\nmanaged service in Azure that enables you to deploy a web application and make it\navailable to your customers on the Internet in a very short amount of time.\nIncorrect:\nA Static Web Application is any web application that can be delivered directly to an end\nuser's browser without any server-side alteration of the HTML, CSS, or\nJavaScript content.\nReference:\nhttps://azure-training.com/2018/12/27/understanding-app-services-app-service-plan-\nand-ase/ https://docs.microsoft.com/en-us/azure/app-service/overview\nCommunity Discussion\nGot this question on 30-Sep-2022 exam. Answer is correct. Passed with 870 score.\nStatic web apps only support HTTP triggered functions (https://learn.microsoft.com/en-\nus/azure/static-web-apps/functions-bring-your-own#link-an-existing-azure-functions-\napp). The top of the case study reads: \"several APIs hosted in Azure Functions.\" If we\nassume that the referenced azure functions here (APIs) are NOT the Azure functions for\nprocessing blob storage, then they can be HTTP triggered and we are fine choosing Static\nWeb App. I think this is a reasonable assumption, but I leave it to the test taker to\ninterpret which is correct.\nI would have said host it in azure static web app (a fine option) that has static part as\nwell as support functions. A perfect option. If functions are to be created outside in\nanother function app and then it contains only the static part then I will host it in blob-\nstorage to get the 99.95% availability SLA of blob storage hot tier and CDN is also easy. It\nis very difficult to choose one when I cannot look into the head of the question creator.\nAnother vague question from the \"M$\"\nI would have said host it in azure static web app (a fine option) that has static part as\nwell as support functions. A perfect option. If functions are to be created outside in\nanother function app and then it contains only the static part then I will host it in blob-\nstorage to get the 99.95% availability SLA of blob storage hot tier and CDN is also easy. It\nis very difficult to choose one when I cannot look into the head of the question creator.\nAnother vague question from the \"M$\"\nStandard plan with app service web app would be the appropriate option.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "329",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\nAudit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\n\n\nSecurity -\nAll Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling. Question You need to\nimplement a solution to resolve the retail store location data issue.\nWhich three Azure Blob features should you enable? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Soft delete",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Change feed",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Snapshots",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Versioning \nE) Object replication\nF) Immutability",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "BD\nScenario: You must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\nBefore you enable and configure point-in-time restore, enable its prerequisites for the\nstorage account: soft delete, change feed, and blob versioning.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-manage"
  },
  {
    "number": "330",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion You need to store the user agreements.\nWhere should you store the agreement after it is completed?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Storage queue",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Event Hub",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Service Bus topic\nPage 1054 of 1410\n1055 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Event Grid topic",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure Event Hub is used for telemetry and distributed data streaming.\nThis service provides a single solution that enables rapid data retrieval for real-time\nprocessing as well as repeated replay of stored raw data. It can capture the streaming\ndata into a file for processing and analysis.\nIt has the following characteristics:\n✑ low latency\n✑ capable of receiving and processing millions of events per second\n✑ at least once delivery\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services"
  },
  {
    "number": "331",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion HOTSPOT -\nYou need to implement the bindings for the CheckUserContent function.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: [BlobTrigger(..)]\nBox 2: [Blob(..)]\nAzure Blob storage output binding for Azure Functions. The output binding allows you to\nmodify and delete blob storage data in an Azure Function.\nThe attribute's constructor takes the path to the blob and a FileAccess parameter\nindicating read or write, as shown in the following example:\n[FunctionName(\"ResizeImage\")]\npublic static void Run(\n[BlobTrigger(\"sample-images/{name}\")] Stream image,\n[Blob(\"sample-images-md/{name}\", FileAccess.Write)] Stream imageSmall)\n{\n...\n}\nScenario: You must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nThe company's data science group built ContentAnalysisService which accepts user\ngenerated content as a string and returns a probable value for inappropriate content.\nAny values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-\nblob-output\n\n\nCommunity Discussion\n1- Queue Trigre[] 2- Blob[]\nThe \"content\" parameter is of type \"string\" so it must be QueueTrigger. For example\nBlobTrigger uses Stream type, CosmosDBTrigger uses IReadOnlyList<> type, it seems the\nTable Storage has no trigger binding. https://docs.microsoft.com/en-us/azure/azure-\nfunctions/functions-triggers-bindings?tabs=csharp#supported-bindings https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-\ntrigger https://docs.microsoft.com/cs-cz/azure/azure-functions/functions-bindings-\nstorage-blob-trigger https://docs.microsoft.com/en-us/azure/azure-functions/functions-\nbindings-cosmosdb-v2-trigger\nThe \"content\" parameter is of type \"string\" so it must be QueueTrigger. For example\nBlobTrigger uses Stream type, CosmosDBTrigger uses IReadOnlyList<> type, it seems the\nTable Storage has no trigger binding. https://docs.microsoft.com/en-us/azure/azure-\nfunctions/functions-triggers-bindings?tabs=csharp#supported-bindings https://\ndocs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue-\ntrigger https://docs.microsoft.com/cs-cz/azure/azure-functions/functions-bindings-\nstorage-blob-trigger https://docs.microsoft.com/en-us/azure/azure-functions/functions-\nbindings-cosmosdb-v2-trigger\nCorrect. Ref: https://docs.microsoft.com/en-us/azure/azure-functions/functions-\nbindings-storage-blob-output?tabs=csharp https://docs.microsoft.com/en-us/azure/\nazure-functions/functions-bindings-storage-blob-input?tabs=csharp\nAnother CSI Miami question from Microsoft. Yes, detectives, the content has to be a\nStream if it is Blob trigger. Microsoft (don't be evil) did not give any information on how\nthe function is triggered, but stereotypically put the red-herring in the description: \"\nwebsite shows the content from azure storage blob\". Of course that does not mean the\nfunction is triggered from it. Next my only clue is the type of this parameter. Microsoft\nsays .Net knowledge is required and any SDK is enough. But how will a javascript person\nknow this that it is Stream and not the content given as String?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "332",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking\nwebsite that is developed as a Single Page Application (SPA). The main web\napplication for the social networking website loads user uploaded content from\nblob storage.\nYou are developing a solution to monitor uploaded data for inappropriate\ncontent. The following process occurs when users upload content by using the\nSPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a\n\n\nrejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a\nprivate Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest\naccounts.\nRequirements -\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts\nuser generated content as a string and returns a probable value for\ninappropriate content. Any values over a specific threshold must be reviewed by\nan employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the\ncontent checks.\nCosts -\nYou must minimize costs for all Azure services.\nManual review -\nTo review content, the user must authenticate to the website portion of the\nContentAnalysisService using their Azure AD credentials. The website is built\nusing\nReact and all pages and API endpoints require authentication. In order to\nreview content a user must be part of a ContentReviewer role. All completed\nreviews must include the reviewer's email address for auditing purposes.\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region\nmust not impact overall application availability.\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent\nof available CPU cores.\n\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site\nscripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be\navailable only to the service.\nAll Internal services must only be accessible from internal Virtual Networks\n(VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The\nagreement allows employees of Contoso, Ltd. to review content, store cookies\non user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso,\nLtd.\nUser responses must not be lost and must be available to all parties regardless\nof individual service uptime. The volume of agreements is expected to be in the\nmillions per hour.\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous\nseven days of content must be processed with the new version to verify that the\nnew version does not significantly deviate from the old version.\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502\nresponses on specific pages.\nCode -\nContentUploadService -\n\n\nApplicationManifest -\nQuestion You need to configure the ContentUploadService deployment.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Add the following markup to line CS23: type: Private",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Add the following markup to line CS24: osType: Windows\nPage 1069 of 1410\n1070 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add the following markup to line CS24: osType: Linux",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add the following markup to line CS23: type: Public",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Scenario: All Internal services must only be accessible from Internal Virtual Networks\n(VNets)\nThere are three Network Location types ג€\" Private, Public and Domain\nReference:\nhttps://devblogs.microsoft.com/powershell/setting-network-location-to-private/"
  },
  {
    "number": "333",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring\nsolutions for homes and businesses. The company is migrating solutions to\nAzure.\nCurrent environment -\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site\nis a single-page web application that runs in Azure App Service on Linux. The\nwebsite uses files stored in Azure Storage and cached in Azure Content Delivery\nNetwork (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and\n\n\nstore data in Azure Database for PostgreSQL. API Management is used to broker\ncommunications to the Azure Function app functions for Logic app integration.\nLogic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\nArchitecture diagram -\nThe company has several applications and services that support their business.\nThe company plans to implement serverless computing where possible. The\noverall architecture is shown below.\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in\npage.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL\nincludes an access token.\n5. The web application calls an API and includes the access token in the\n\n\nauthentication header. The application ID is sent as the audience ('aud') claim in\nthe access token.\n6. The back-end API validates the access token.\nRequirements -\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following\nattributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always\nuse the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use\nin buffering.\nEvents from Azure Service Bus and other Azure services must continue to be\nrouted to Azure Event Grid for processing.\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are\nessential to perform their intended function.\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with\nGeneral Data Protection Regulation (GDPR). The file updates must be read-only,\nstored in the order in which they occurred, include only create, update, delete,\n\n\nand copy operations, and be retained for compliance reasons.\nIssues -\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following\nerror message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure\ndevelopment environment when you run the following Kusto query:\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nLogic app -\nYou test the Logic app in a development environment. The following error\nmessage displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the\nRequestUserApproval function.\nCode -\nCorporate website -\nSecurity.cs:\nFunction app -\nRequestUserApproval.cs:\n\n\nQuestion HOTSPOT -\nYou need to configure the Account Kind, Replication, and Access tier options for\nthe corporate website's Azure Storage account.\nHow should you complete the configuration? To answer, select the appropriate\noptions in the dialog box in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\n\n\nAccount Kind: StorageV2 (general-purpose v2)\nScenario: Azure Storage blob will be used (refer to the exhibit). Data storage costs must\nbe minimized.\nGeneral-purpose v2 accounts: Basic storage account type for blobs, files, queues, and\ntables. Recommended for most scenarios using Azure Storage.\nIncorrect Answers:\n✑ BlockBlobStorage accounts: Storage accounts with premium performance\ncharacteristics for block blobs and append blobs. Recommended for scenarios with high\ntransactions rates, or scenarios that use smaller objects or require consistently low\n\n\nstorage latency.\n✑ General-purpose v1 accounts: Legacy account type for blobs, files, queues, and tables.\nUse general-purpose v2 accounts instead when possible.\nReplication: Geo-redundant Storage\nScenario: Data must be replicated to a secondary region and three availability zones.\nGeo-redundant storage (GRS) copies your data synchronously three times within a single\nphysical location in the primary region using LRS. It then copies your data\nasynchronously to a single physical location in the secondary region.\nIncorrect Answers:\nGeo-zone-redundant storage (GZRS), but it would be more costly.\nAccess tier: Cool -\nData storage costs must be minimized.\nNote: Azure storage offers different access tiers, which allow you to store blob object data\nin the most cost-effective manner. The available access tiers include:\nHot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30\ndays.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy https://\ndocs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-\nportal\nCommunity Discussion\nReplication should be GZRS. Requirements ask \"Data must be replicated to a secondary\nregion and three availability zones\". GRS option doesn't copy data in different availability\nzones\nWhat about cool vs hot in the last box? Shouldn't this be hot, because the site gets\naccessed frequently?\nGZRS is correct answer https://docs.microsoft.com/en-us/azure/storage/common/\nstorage-redundancy\nGZRS is correct answer https://docs.microsoft.com/en-us/azure/storage/common/\nstorage-redundancy\n\n\nGZRS is correct answer https://docs.microsoft.com/en-us/azure/storage/common/\nstorage-redundancy",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "334",
    "question": "HOTSPOT\n-\nYou are developing a web application that uses the Microsoft Identity platform\nfor user and resource authentication. The web application called several REST\nAPIs.\nYou are implementing various authentication and authorization flows for the\nweb application.\nYou need to validate the claims in the authentication token.\nWhich token type should you use? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nHow to remember: 1) Identify users -> ID (ID of the user) 2) Permissions -> Access (how to\naccess? with permissions) 3) Without interactions -> Refresh (with refresh, there is no\ninteraction) 4) Provide XML -> SAML (similar in writing)\nGot this on 6/28/2023 and passed with 850. Answer is correct.\nlooks correct https://learn.microsoft.com/en-us/azure/active-directory/develop/active-\ndirectory-configurable-token-lifetimes#token-lifetime-policies-for-access-saml-and-id-\ntokens https://learn.microsoft.com/en-us/azure/active-directory/develop/refresh-\ntokens#refresh-token-lifetime\n\n\nAnswer is correct, got this one in Nov182023, scored 962\nSuggested answer makes sense",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "335",
    "question": "HOTSPOT\n-\nYou are developing a content management application for technical manuals.\nThe application is deployed as an Azure Static Web app.\nAuthenticated users can view pages under/manuals but only contributors can\naccess the page /manuals/new.html.\nYou need to configure the routing for the web app.\nHow should you complete the configuration? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect. \"Rule evaluation stops at the first match. A match occurs when the route\nproperty and a value in the methods array (if specified) match the request. Each request\ncan match at most one rule.\" So the first rule must be for the /manuals/new.html page.\nhttps://learn.microsoft.com/en-us/azure/static-web-apps/configuration#routes\nThis was on the exam (July 2023). Went with proposed. Scored 917\ncouldn't it just have easily have been manuals/authenticated then html/contributers?\n\n\nRule evaluation stops at the first match. If authenticated came first all users including\ncontributors would have access to /manuals/new.html page which goes against the\nrequirement\ngot it exam 31/07/2023. correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "336",
    "question": "You are developing a web application that uses the Microsoft identity platform\nfor user and resource authentication. The web application calls several REST\nAPIs.\nA REST API call must read the user’s calendar. The web application requires\npermission to send an email as the user.\nYou need to authorize the web application and the API.\nWhich parameter should you use?",
    "options": [
      {
        "letter": "A",
        "text": "tenant",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "code_challenge",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "state",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "client_id\nE) scope",
        "is_correct": true
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "337",
    "question": "HOTSPOT\n-\nYou develop and deploy a web app to Azure App service. The web app allows\nusers to authenticate by using social identity providers through the Azure B2C\nservice. All user profile information is stored in Azure B2C.\nYou must update the web app to display common user properties from Azure\nB2C to include the following information:\n• Email address\n• Job title\n• First name\n• Last name\n• Office location\nYou need to implement the user properties in the web app.\nWhich code library and API should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nright.. https://learn.microsoft.com/en-us/azure/active-directory-b2c/microsoft-graph-\noperations https://learn.microsoft.com/en-us/azure/active-directory/develop/msal-net-\naad-b2c-considerations\nGot this on 6/28/2023 and passed with 850. Graph, MASL is correct.\nReceived on 27 May 2023 - Microsoft Graph - MSAL\nReceived on 31/07/2023\nCorrect - Microsoft Graph - MSAL",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "338",
    "question": "HOTSPOT\n-\nYou develop and deploy the following staticwebapp.config.json file to the\napp_location value specified in the workflow file of an Azure Static Web app:\n\n\n\n\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\n1 should be no. Unauthenticated users get a 401 - Unauthorized response and are\nredirected by the responseOverrides to \"/.auth/login/aad\". I believe that is not Github.\nYou will only be redirected to GitHub if the an unauthenticated user routes to \"/login\".\n\n\n1 should be no. Unauthenticated users get a 401 - Unauthorized response and are\nredirected by the responseOverrides to \"/.auth/login/aad\". I believe that is not Github.\nYou will only be redirected to GitHub if the an unauthenticated user routes to \"/login\".\n1) Y. login with github 2 ) Y. true because it is excluded from returning index.html in\nnavigation fallback, so the other option is a 404 not found 3) Y. straight forward from first\nroutes object 4) N. from https://learn.microsoft.com/en-us/azure/static-web-apps/\nconfiguration 401 is for requesting when unauthenticated but here are you are\nauthenticated and so the error should be 403, because you are authenticated but don't\nhave authorization ( due to not having the registeredusers role)\nIt should be: 1. N In the example written here: https://learn.microsoft.com/en-us/azure/\nstatic-web-apps/configuration#example-configuration-file - If you go to the login page,\nyou redirect to Github, just like in the question. - If the request got a 401 error\n(Unauthorized), it redirects the user to the login route (with 302 response), which then\nredirects to the GitHub login. But in this question, if you check what happens when the\nrequest got a 401 error (Unauthorized), it redirects to the AAD route (with 302 response),\nnot to Github. So this part of the question and the example are different. 2. Y That part of\nthe question seems the same as the example above. A non-existent file in the /images/\nfolder -> A 404 error. 3. Y That part of the question seems the same as the example\nabove. GET requests from authenticated users in the registeredusers role are sent to the\nAPI. Authenticated users not in the registeredusers role and unauthenticated users are\nserved a 401 error. 4. N It overrides the 401 error to the 302 and redirects the user to the\nAAD URL.\nreceived 2023-04-17 went with N,Y,Y,N , score 926 i have doubts about second one - I don't\nnow that status code will persist or not. We set new page 404.html without status code\nlike in example with 401 where status code id set to 302. Needs to be verified, good luck",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "339",
    "question": "You develop and deploy an Azure App Service web app named App1. You create\na new Azure Key Vault named Vault1. You import several API keys, passwords,\ncertificates, and cryptographic keys into Vault1.\nYou need to grant App1 access to Vault1 and automatically rotate credentials.\nCredentials must not be stored in code.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable App Service authentication for Appl. Assign a custom RBAC role to Vault1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add a TLS/SSL binding to App1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Upload a self-signed client certificate to Vault1. Update App1 to use the client certificate.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Assign a managed identity to App1.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "340",
    "question": "You are developing a Java application to be deployed in Azure. The application\nstores sensitive data in Azure Cosmos DB.\nYou need to configure Always Encrypted to encrypt the sensitive data inside the\napplication.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Create a new container to include an encryption policy with the JSON properties to be\nencrypted.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a customer-managed key (CMK) and store the key in a new Azure Key Vault instance.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a data encryption key (DEK) by using the Azure Cosmos DB SDK and store the key in\nAzure Cosmos DB.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create an Azure AD managed identity and assign the identity to a new Azure Key Vault\ninstance.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "341",
    "question": "HOTSPOT\n-\nYou develop a web app that interacts with Azure Active Directory (Azure AD)\ngroups by using Microsoft Graph.\nYou build a web page that shows all Azure AD groups that are not of the type\n'Unified'.\nYou need to build the Microsoft Graph query for the page.\nHow should you complete the query? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\n~/groups?$filter=NOT groupTypes/any(c:c eq 'Unified')&$count=true This exact example is\nmentioned in Microsoft documentation: https://learn.microsoft.com/en-us/graph/filter-\nquery-parameter?tabs=http#examples-using-the-filter-query-operator\nit is correct https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-always-\nencrypted?tabs=dotnet\nNot sure since then but now it supports the ne parameter too: \"As Microsoft Entra ID\ncontinues to deliver more capabilities and improvements in stability, availability, and\nperformance, Microsoft Graph also continues to evolve and scale to efficiently access the\ndata. One way is through Microsoft Graph's increasing support for advanced query\ncapabilities on various Microsoft Entra ID objects, also called directory objects, and their\nproperties. For example, the addition of not (not), not equals (ne), and ends with\n(endsWith) operators on the $filter query parameter.\" https://learn.microsoft.com/en-us/\ngraph/aad-advanced-queries?tabs=http#count-of-a-collection-in-a-filter-expression\nNot sure since then but now it supports the ne parameter too: \"As Microsoft Entra ID\ncontinues to deliver more capabilities and improvements in stability, availability, and\nperformance, Microsoft Graph also continues to evolve and scale to efficiently access the\ndata. One way is through Microsoft Graph's increasing support for advanced query\ncapabilities on various Microsoft Entra ID objects, also called directory objects, and their\nproperties. For example, the addition of not (not), not equals (ne), and ends with\n(endsWith) operators on the $filter query parameter.\" https://learn.microsoft.com/en-us/\ngraph/aad-advanced-queries?tabs=http#count-of-a-collection-in-a-filter-expression\ngot this question on 06/29/2023",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "342",
    "question": "DRAG DROP\n-\nYou are developing an Azure solution.\nYou need to develop code to access a secret stored in Azure Key Vault.\nHow should you complete the code segment? To answer, drag the appropriate\ncode segments to the correct location. Each code segment may be used once,\nmore than once, or not at all. You may need to drag the split bar between panes\nor scroll to view content.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nLooks good but I think this URL should be more accurate https://learn.microsoft.com/en-\nus/python/api/azure-keyvault-secrets/azure.keyvault.secrets.secretclient?view=azure-\npython#examples\nLooks good but I think this URL should be more accurate https://learn.microsoft.com/en-\nus/python/api/azure-keyvault-secrets/azure.keyvault.secrets.secretclient?view=azure-\npython#examples\nit's correct https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-always-\nencrypted?tabs=dotnet\nGot this on 6/28/2023 and passed with 850. Correct answer.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "343",
    "question": "HOTSPOT\n-\nYou are a developer building a web site using a web app. The web site stores\nconfiguration data in Azure App Configuration.\nAccess to Azure App Configuration has been configured to use the identity of the\nweb app for authentication. Security requirements specify that no other\nauthentication systems must be used.\nYou need to load configuration data from Azure App Configuration.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI think it should be ManagedIdentityCredential: ManagedIdentityCredential: attempts to\nauthenticate using a managed identity that is assigned to the deployment environment\n(if any). DefaultAzureCredential can not be, since only one type of authentication is\nallowed. ChainedTokenCredential could work if there is only one authentication type is\nspecified in code. Source: https://yourazurecoach.com/2020/08/13/managed-identity-\nsimplified-with-the-new-azure-net-sdks/\nAnswer is correct. It is here: https://learn.microsoft.com/en-us/python/api/overview/\nazure/appconfiguration-readme?view=azure-python#create-a-client\nfrom azure.identity import DefaultAzureCredential from azure.appconfiguration import\nAzureAppConfigurationClient credential = DefaultAzureCredential() client =\nAzureAppConfigurationClient(base_url=\"your_endpoint_url\", credential=credential)\n\n\nfrom azure.identity import DefaultAzureCredential from azure.appconfiguration import\nAzureAppConfigurationClient credential = DefaultAzureCredential() client =\nAzureAppConfigurationClient(base_url=\"your_endpoint_url\", credential=credential)\nTo load configuration data from Azure App Configuration using the identity of the web\napp for authentication, you would typically use the ManagedIdentityCredential class for\nauthentication. Here's how you should complete the code: from azure.identity import\nManagedIdentityCredential from azure.appconfiguration import\nAzureAppConfigurationClient credential = ManagedIdentityCredential() client =\nAzureAppConfigurationClient(base_url=\"your_endpoint_url\", credential=credential)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "344",
    "question": "You are developing an online game that includes a feature that allows players\nto interact with other players on the same team within a certain distance. The\ncalculation to determine the players in range occurs when players move and are\ncached in an Azure Cache for Redis instance.\nThe system should prioritize players based on how recently they have moved\nand should not prioritize players who have logged out of the game.\nYou need to select an eviction policy.\nWhich eviction policy should you use?",
    "options": [
      {
        "letter": "A",
        "text": "allkeys-Iru",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "volatile-Iru",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "allkeys-lfu",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "volatile-ttl",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "345",
    "question": "You develop an Azure App Service web app and deploy to a production\nenvironment. You enable Application Insights for the web app.\nThe web app is throwing multiple exceptions in the environment.\nYou need to examine the state of the source code and variables when the\nexceptions are thrown.\nWhich Application Insights feature should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Smart detection",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Profiler",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Snapshot Debugger",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Standard test",
        "is_correct": false
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "346",
    "question": "DRAG DROP\n-\nYou develop and deploy a Java application to Azure. The application has been\ninstrumented by using the Application Insights SDK.\nThe telemetry data must be enriched and processed before it is sent to the\nApplication Insights service.\nYou need to modify the telemetry data.\nWhich Application Insights SDK features should you use? To answer, drag the\nappropriate features to the correct requirements. Each feature may be used\nonce, more than once, or not at all. You may need to drag the split bar between\npanes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect answers Link: https://learn.microsoft.com/en-us/azure/azure-monitor/app/api-\nfiltering-sampling\nI got this question on 6th August 2023. chose given answers. passed with 904. I got Case\nstudy: city and Lights. All questions are from SecExams.\nNormally, 1 case study per exam , with 5 questions\nNormally, 1 case study per exam , with 5 questions\nNormally, 1 case study per exam , with 5 questions",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "347",
    "question": "HOTSPOT\n-\nYou develop new functionality in a web application for a company that provides\naccess to seismic data from around the world. The seismic data is stored in\nRedis Streams within an Azure Cache for Redis instance.\nThe new functionality includes a real-time display of seismic events as they\noccur.\nYou need to implement the Azure Cache for Redis command to receive seismic\ndata.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\npuuh... knowing all this little things/ remembering all this little things for the exam\nsounds impossible :(\nCorrect. BLOCK 0 means that we will wait infinitely (timeout 0 means it will never expire)\nfor new items in a stream. $ ID means that we will receive only new messages, starting\nfrom the time we started listening https://redis.io/docs/data-types/streams-tutorial/\n#listening-for-new-items-with-xread\nThis question is so ridiculous that I actually remember its answer better than any other.\nThese AZ exam questions are some of the most ridiculous ones ever. Looks like MS wants\nyou to fail unless you senselessly memorize stupid things. I wonder how dumb the team\nthat sets these questions must be.\nExactly! That's why we study the exam in question bank :)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "348",
    "question": "You develop an ASP.NET Core app that uses Azure App Configuration. You also\ncreate an App Configuration containing 100 settings.\nThe app must meet the following requirements:\n• Ensure the consistency of all configuration data when changes to individual\nsettings occur.\n• Handle configuration data changes dynamically without causing the\napplication to restart.\n• Reduce the overall number of requests made to App Configuration APIs.\nYou must implement dynamic configuration updates in the app.\nWhat are two ways to achieve this goal? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create and register a sentinel key in the App Configuration store. Set the refreshAll\nparameter of the Register method to true.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Increase the App Configuration cache expiration from the default value.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Decrease the App Configuration cache expiration from the default value.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create and configure Azure Key Vault. Implement the Azure Key Vault configuration provider.\nE) Register all keys in the App Configuration store. Set the refreshAll parameter of the Register\nmethod to false.\nF) Create and implement environment variables for each App Configuration store setting.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B"
  },
  {
    "number": "349",
    "question": "HOTSPOT\n-\nYou develop an image upload service that is exposed using Azure API\nManagement. Images are analyzed after upload for automatic tagging.\nImages over 500 KB are processed by a different backend that offers a lower tier\nof service that costs less money. The lower tier of service is denoted by a\nheader named x-large-request. Images over 500 KB must never be processed by\nbackends for smaller images and must always be charged the lower price.\nYou need to implement API Management policies to ensure that images are\nprocessed correctly.\nHow should you complete the API Management inbound policy? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI got this question on 6th August 2023. chose given answers. passed with 904. I got Case\nstudy: city and Lights. All questions are from SecExams.\nx-large-request should be set on Size>=512.if Size<512kb,remove this header.\nx-large-request should be set on Size>=512.if Size<512kb,remove this header.\nx-large-request should be set on Size>=512.if Size<512kb,remove this header.\nx-large-request should be set on Size>=512.if Size<512kb,remove this header.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "350",
    "question": "HOTSPOT\n-\nYou develop several Azure Functions app functions to process JSON documents\nfrom a third-party system. The third-party system publishes events to Azure\nEvent Grid to include hundreds of event types, such as billing, inventory, and\nshipping updates.\nEvents must be sent to a single endpoint for the Azure Functions app to process.\nThe events must be filtered by event type before processing. You must have\nauthorization and authentication control to partition your tenants to receive\nthe event data.\nYou need to configure Azure Event Grid.\nWhich configuration should you use? To answer, select the appropriate values in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nCheck this from official trainig material: https://learn.microsoft.com/en-us/training/\nmodules/azure-event-grid/2-event-grid-overview 1. custom topic Because according to\nlink: \"Topics. The event grid topic provides an endpoint where the source sends\nevents(...)Custom topics are application and third-party topics\" 2. event subscription\nBecause according to link: \"Event subscriptions. A subscription tells Event Grid which\nevents on a topic you're interested in receiving. When creating the subscription, you\nprovide an endpoint for handling the event\"\nI don't know, but from the description of the event domain, I'd say the correct answer is:\n1. event domain 2. event subscription https://learn.microsoft.com/en-us/azure/event-\ngrid/event-domains 1. event domain: Using event domain, the third party system does\nnot send data directly to topics. Event domain provides a single endpoint for the system\nto publish events. 2. event subscription: Azure functions are event handlers. They use\nsubscriptions. It is true that the event domain also provides a single subscription\nendpoint, but the handler would then receive all events, not just a filtered set as\nrequired: \"Event domains also allow for domain-scope subscriptions. An event\nsubscription on an event domain will receive all events sent to the domain regardless of\nthe topic the events are sent to.\" https://learn.microsoft.com/en-us/azure/event-grid/\nevent-domains#domain-scope-subscriptions\n\n\ni received this one on 2023-04-17, I did not know what to do I selected custom topic twice.\nI passed with 926. This definitely need more reasarch because it is still there\nI got this question on 6th August 2023. I was confused so chose a custom topic for the\nboth boxs passed with 904. I got a Case study: city and Lights. All questions are from\nSecExams.\nReceived the same question today on the exam, 15th of June 2023. Scored 887. Answered\nthe same.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "351",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "B)",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Page 1121 of 1410\n1122 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "352",
    "question": "You are developing several microservices to deploy to a new Azure Kubernetes\nService cluster. The microservices manage data stored in Azure Cosmos DB and\nAzure Blob storage. The data is secured by using customer-managed keys stored\nin Azure Key Vault.\nYou must automate key rotation for all Azure Key Vault keys and allow for\nmanual key rotation. Keys must rotate every three months. Notifications of\nexpiring keys must be sent before key expiry.\nYou need to configure key rotation and enable key expiry notifications.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create and configure a new Azure Event Grid instance.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Configure Azure Key Vault alerts.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create and assign an Azure Key Vault access policy.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create and configure a key rotation policy during key creation.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C"
  },
  {
    "number": "353",
    "question": "HOTSPOT\n-\nYou develop and deploy an Azure App Service web app that connects to Azure\nCache for Redis as a content cache. All resources have been deployed to the\nEast US 2 region.\nThe security team requires the following audit information from Azure Cache for\nRedis:\n• The number of Redis client connections from an associated IP address.\n• Redis operations completed on the content cache.\n• The location (region) in which the Azure Cach3e for Redis instance was\naccessed.\nThe audit information must be captured and analyzed by a security team\napplication deployed to the Central US region.\nYou need to log information on all client connections to the cache.\nWhich configuration values should you use? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nCorrect. Only Log Analytics supports cross-region logging Reference: https://\nlearn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-monitor-diagnostic-\nsettings?tabs=basic-standard-premium#enable-connection-logging-using-the-azure-\nportal\nLog Analytics Workspace - used to store log information. Diagnostic setting - diagnostic\nlogs are streamed to that workspace as soon as new event data is generated",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "354",
    "question": "You develop and deploy a web app to Azure App Service. The Azure App Service\nuses a Basic plan in a single region.\nUsers report that the web app is responding slow. You must capture the\ncomplete call stack to help identify performance issues in the code. Call stack\ndata must be correlated across app instances. You must minimize cost and\nimpact to users on the web app.\nYou need to capture the telemetry.\nWhich three actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Restart all apps in the App Service plan.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable Application Insights site extensions.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Upgrade the Azure App Service plan to Premium.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable Profiler. \nE) Enable the Always On setting for the app service. \nF) Enable Snapshot debugger. \nG) Enable remote debugging.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "EF"
  },
  {
    "number": "355",
    "question": "You are building an application to track cell towers that are available to phones\nin near real time. A phone will send information to the application by using the\nAzure Web PubSub service. The data will be processed by using an Azure\nFunctions app. Traffic will be transmitted by using a content delivery network\n(CDN).\nThe Azure function must be protected against misconfigured or unauthorized\ninvocations.\nYou need to ensure that the CDN allows for the Azure function protection.\nWhich HTTP header should be on the allowed list?",
    "options": [
      {
        "letter": "A",
        "text": "Authorization",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "WebHook-Request-Callback",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Resource",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "WebHook-Request-Origin",
        "is_correct": true
      },
      {
        "letter": "A",
        "text": "is widely used for authentication in HTTP requests.\nIt is a standard header for including credentials, tokens, or other authentication\ninformation. By allowing the Authorization header on the CDN, you can ensure that only\nrequests with valid authorization tokens or credentials can invoke your Azure function,\nproviding a secure method of protection against unauthorized invocations.\nSelected Answer: A\nWhy not A? To ensure that the Azure Function is protected against misconfigured or\nunauthorized invocations when using a CDN, you should allow the \"Authorization\" HTTP\nheader.",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "356",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "Page 1131 of 1410\n1132 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "C)",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "357",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "B)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "358",
    "question": "DRAG DROP\n-\nYou develop and deploy several APIs to Azure API Management.\nYou create the following policy fragment named APICounts:\nThe policy fragment must be reused across various scopes and APIs. The policy\nfragment must be applied to all APIs and run when a calling system invokes any\nAPI.\nYou need to implement the policy fragment.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nThis was on the exam (July 2023). Went with proposed answer. Scored 917\nprovided answer seems correct according to https://learn.microsoft.com/en-us/azure/\napi-management/include-fragment-policy\nGot it in the exam 14/12/23. Went with provided answer. Scored 912/1000. All questions\nare from SecExams. Case study - VanArsdel, Ltd (11 questions)\naround 60-70%\naround 60-70%",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "359",
    "question": "HOTSPOT\n-\nYou are developing a solution that uses several Azure Service Bus queues. You\ncreate an Azure Event Grid subscription for the Azure Service Bus namespace.\nYou use Azure Functions as subscribers to process the messages.\nYou need to emit events to Azure Event Grid from the queues. You must use the\nprincipal of least privilege and minimize costs.\nWhich Azure Service Bus values should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nRemember with Service bus + event grid its always premimum\nI got this question on 6th August 2023. chose given answers. passed with 904. I got Case\nstudy: city and Lights. All questions are from SecExams.\ngot this question on 29/06/2023\nReceived the question, 15th of June 2023. Scored 887. Went with the given answers.\nLooks correct: https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-\nbus-to-event-grid-integration-concept?tabs=event-grid-event-schema",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "360",
    "question": "You are developing a web application that uses the Microsoft identity platform\nto authenticate users and resources. The web application calls several REST\nAPIs.\nThe APIs require an access token from the Microsoft identity platform.\nYou need to request a token.\nWhich three properties should you use? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Redirect URI/URL",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Application ID",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Application name",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Application secret \nE) Supported account type",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "BD"
  },
  {
    "number": "361",
    "question": "HOTSPOT\n-\nYou are developing an application that uses Azure Storage to store customer\ndata. The data must only be decrypted by the customer and the customer must\nbe provided a script to rotate keys.\nYou need to provide a script to rotate keys to the customer.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAnswers is correct. https://learn.microsoft.com/en-us/cli/azure/storage/account?\nview=azure-cli-latest#az-storage-account-update https://learn.microsoft.com/en-us/cli/\nazure/keyvault/key?view=azure-cli-latest\ngot this question on 29/06/2023\nmaybe the answer should be , secret and Microsoft.Keyvault ??\nWould really help if you provided documentation to back up your answer.\nWould really help if you provided documentation to back up your answer.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "362",
    "question": "You are developing an Azure App Service web app.\nThe web app must securely store session information in Azure Redis Cache.\nYou need to connect the web app to Azure Redis Cache.\nWhich three Azure Redis Cache properties should you use? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Access key",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "SSL port",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Subscription name",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Location\nE) Host name \nF) Subscription id",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "BE"
  },
  {
    "number": "363",
    "question": "HOTSPOT\n-\nYou are developing several microservices to run on Azure Container Apps.\nYou need to monitor and diagnose the microservices.\nWhich features should you use? To answer, select the appropriate feature in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI got this question (12-Aug-2023) and I chose the given answers - 932 passed\nAnswer is correct! https://learn.microsoft.com/en-us/azure/container-apps/observability\nCorrect. It is not deployed yet. So: Development and test During the development and\ntest phase, real-time access to your containers' application logs and console is critical\nfor debugging issues. Container Apps provides: Log streaming: View real-time log streams\nfrom your containers. Container console: Access the container console to debug your\napplication. https://learn.microsoft.com/en-us/azure/container-apps/\nobservability#development-and-test\nCorrect. It is not deployed yet. So: Development and test During the development and\ntest phase, real-time access to your containers' application logs and console is critical\nfor debugging issues. Container Apps provides: Log streaming: View real-time log streams\nfrom your containers. Container console: Access the container console to debug your\napplication. https://learn.microsoft.com/en-us/azure/container-apps/\nobservability#development-and-test\nFrom this page: Log Streaming - View streaming system and console logs from a\ncontainer in near real-time. Container console - Connect to the Linux console in your\ncontainers to debug your application from inside the container.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "364",
    "question": "You are developing several Azure API Management (APIM) hosted APIs.\nThe APIs have the following requirements:\n• Require a subscription key to access all APIs.\n• Include terms of use that subscribers must accept to use the APIs.\n• Administrators must review and accept or reject subscription attempts.\n• Limit the count of multiple simultaneous subscriptions.\nYou need to implement the APIs.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure and apply header-based versioning.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create and publish a product.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure and apply query string-based versioning.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Add a new revision to all APIs. Make the revisions current and add a change log entry.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "365",
    "question": "HOTSPOT\n-\nYou are developing a solution by using the Azure Event Hubs SDK. You create a\nstandard Azure Event Hub with 16 partitions. You implement eight event\nprocessor clients.\nYou must balance the load dynamically when an event processor client fails.\nWhen an event processor client fails, another event processor must continue\nprocessing from the exact point at which the failure occurred. All events must\nbe aggregate and upload to an Azure Blob storage account.\nYou need to implement event processing recovery for the solution.\nWhich SDK features should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI actually got this on my exam 2023July13!! I chickened out and selected checkpoint on\nboth just to make sure i at least get one them correct. Scored 917.\nI actually got this on my exam 2023July13!! I chickened out and selected checkpoint on\nboth just to make sure i at least get one them correct. Scored 917.\nThis is a really stupid and confusing question but I think it’s checkpoint to ensure\nmarking and offset to mark…\nThis ref says: ”Checkpointing is a process by which readers mark or commit their position\nwithin a partition event sequence.” https://learn.microsoft.com/en-us/azure/event-\nhubs/event-hubs-features#checkpointing\nBut this ref here says: ”Consumers also engage in checkpointing. Through this process,\nsubscribers use offsets to mark their position within a partition event sequence. An\noffset is a placeholder that works like a bookmark to identify the last event that the\nconsumer read.” https://learn.microsoft.com/en-us/azure/architecture/reference-\narchitectures/event-hubs/partitioning-in-event-hubs-and-kafka#dataflow\nAn offset is the position of an event within a partition. Checkpointing is a process by\nwhich readers mark or commit their position within a partition event sequence. Can't\nmake my mind, I'll say select checkpoint on both and call it a day.\n\n\nI believe the given answer is correct. refs: https://learn.microsoft.com/en-us/azure/\nevent-hubs/event-hubs-features",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "366",
    "question": "HOTSPOT\n-\nYou are developing a new API to be hosted by Azure API Management (APIM).\nThe backend service that implements the API has not been completed. You are\ncreating a test API and operation.\nYou must enable developers to continue with the implementation and testing of\nthe APIM instance integrations while you complete the backend API\ndevelopment.\nYou need to configure a test API response.\nHow should you complete the configuration? To answer, select the appropriate\noptions in the answer area.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAnswer is wrong. It should be inbound not backend.\nCorrect is: Mock-response, inbound and 200 https://learn.microsoft.com/en-us/azure/\napi-management/mock-api-responses?tabs=azure-portal#enable-response-mocking\nAnswer should be: 1. mock-response 2. inbound 3. 200 refs: https://learn.microsoft.com/\nen-us/azure/api-management/mock-api-responses?tabs=azure-cli\ngot this question today, went with this answer - 7/30/2023, score 895/1000\n\n\ngot this question today, went with this answer - 7/30/2023, score 895/1000\nI just tested it on Azure. The documentation and the secexams answer are wrong, it says:\nUsage Policy sections: inbound, outbound, on-error here: https://learn.microsoft.com/\nen-us/azure/api-management/mock-response-policy But if I put <mock-response>\ninside <backend> I get an error: Error in element 'mock-response' on line 18, column 10:\nPolicy is not allowed in this section The correct place is <inbound> beyond any doubt. So\nthe answer is: 1. mock-response 2. inbound 3. 200",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "367",
    "question": "You are developing several Azure API Management (APIM) hosted APIs.\nYou must inspect request processing of the APIs in APIM. Requests to APIM by\nusing a REST client must also be included. The request inspection must include\nthe following information:\n• requests APIM sent to the API backend and the response it received\n• policies applied to the response before sending back to the caller\n• errors that occurred during the processing of the request and the policies\napplied to the errors\n• original request APIM received from the caller and the policies applied to the\nrequest\nYou need to inspect the APIs.\nWhich three actions should you do? Each correct answer presents part of the\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Enable the Allow tracing setting for the subscription used to inspect the API. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add the Ocp-Apim-Trace header value to the API call whit a value set to true.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add the Ocp-Apim-Subscription-Key header value to the key for a subscription that allows\naccess to the API.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create and configure a custom policy. Apply the policy to the inbound policy section with a\nglobal scope.\nPage 1155 of 1410\n1156 Microsoft - AZ-204 Practice Questions - SecExams.com\nE) Create and configure a custom policy. Apply the policy to the outbound policy section with\nan API scope.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "CE"
  },
  {
    "number": "368",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in\nCanada and has retail store locations across the world. The company is\ndeveloping several cloud-based solutions to support their stores, distributors,\nsuppliers, and delivery services.\n\n\nCurrent environment -\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com.\nThe website consists of a React JavaScript user interface, HTML, CSS, image\nassets, and several APIs hosted in Azure Functions.\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations\nsend data every hour to an Azure Blob storage account to support inventory,\npurchasing and delivery services. Each record includes a location identifier and\nsales transaction information.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\n• Secure the website by using SSL.\n• Minimize costs for data storage and hosting.\n• Implement native GitHub workflows for continuous integration and continuous\ndeployment (CI/CD).\n• Distribute the website content globally for local use.\n• Implement monitoring by using Application Insights and availability web tests\nincluding SSL certificate validity and custom header value verification.\n• The website must have 99.95 percent uptime.\nRetail store locations -\n\n\n• Azure Functions must process data immediately when data is uploaded to Blob\nstorage. Azure Functions must update Azure Cosmos DB by using native SQL\nlanguage queries.\n• Audit store sale transaction information nightly to validate data, process sales\nfinancials, and reconcile inventory.\nDelivery services -\n• Store service telemetry data in Azure Cosmos DB by using an Azure Function.\nData must include an item id, the delivery vehicle license plate, vehicle package\ncapacity, and current vehicle location coordinates.\n• Store delivery driver profile information in Azure Active Directory (Azure AD) by\nusing an Azure Function called from the corporate website.\nInventory services -\nThe company has contracted a third-party to develop an API for inventory\nprocessing that requires access to a specific blob within the retail store storage\naccount for three months to include read-only access to the data.\nSecurity -\n• All Azure Functions must centralize management and distribution of\nconfiguration data for different environments and geographies, encrypted by\nusing a company-provided RSA-HSM key.\n• Authentication and authorization must use Azure AD and services must use\nmanaged identities where possible.\nIssues -\nRetail Store Locations -\n\n\n• You must perform a point-in-time restoration of the retail store location data\ndue to an unexpected and accidental deletion of data.\n• Azure Cosmos DB queries from the Azure Function exhibit high Request Unit\n(RU) usage and contain multiple, complex queries that exhibit high point read\nlatency for large items as the function app is scaling.\nYou need to test the availability of the corporate website.\nWhich two test types can you use? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Standard",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "URL ping",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Custom testing using the TrackAvailability API method",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Multi-step",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C"
  },
  {
    "number": "369",
    "question": "You are developing several Azure API Management (APIM) hosted APIs.\nYou must transform the APIs to hide private backend information and obscure\nthe technology stack used to implement the backend processing.\nYou need to protect all APIs.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure and apply a new inbound policy scoped to a product.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure and apply a new outbound policy scoped to the operation.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Configure and apply a new outbound policy scoped to global.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure and apply a new backend policy scoped to global.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "370",
    "question": "You have an Azure API Management (APIM) Standard tier instance named APIM1\nthat uses a managed gateway.\nYou plan to use APIM1 to publish an API named API1 that uses a backend\ndatabase that supports only a limited volume of requests per minute. You also\nneed a policy for API1 that will minimize the possibility that the number of\nrequests to the backend database from an individual IP address you specify\nexceeds the supported limit.\nYou need to identify a policy for API1 that will meet the requirements.\nWhich policy should you use?",
    "options": [
      {
        "letter": "A",
        "text": "ip-filter",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "quota-by-key\nPage 1163 of 1410\n1164 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "rate-limit-by-key",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "rate-limit",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "371",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "B)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "D)",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "372",
    "question": "HOTSPOT\n-\nYou plan to implement an Azure Functions app.\nThe Azure Functions app has the following requirements:\n• Must be triggered by a message placed in an Azure Storage queue.\n• Must use the queue name set by an app setting named input_queue.\n• Must create an Azure Blob Storage named the same as the content of the\nmessage.\nYou need to identify how to reference the queue and blob name in the\nfunction.json file of the Azure Functions app.\nHow should you reference the names? To answer, select the appropriate values\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nQueue name: %input_queue% Reference: https://learn.microsoft.com/en-us/azure/\nazure-functions/functions-bindings-expressions-patterns#binding-expressions---app-\nsettings Blob name: {queue Trigger} Reference: https://learn.microsoft.com/en-us/azure/\nazure-functions/functions-bindings-expressions-patterns#trigger-metadata Blobname\n{ \"bindings\": [ { \"type\": \"queueTrigger\", \"name\": \"myQueueItem\", \"direction\": \"in\",\n\"queueName\": \"%input_queue%\" // Reference the input_queue app setting }, { \"type\":\n\"blob\", \"name\": \"outputBlob\", \"path\": \"{QueueTrigger}\", // Use the value from the queue\nmessage as the blob name \"connection\": \"AzureWebJobsStorage\", \"direction\": \"out\",\n\"containerName\": \"%output_blobcontainer%\" // Reference the output_blobcontainer app\nsetting } ] }",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "373",
    "question": "HOTSPOT\n-\nYou have an Azure API Management instance named API1 that uses a managed\ngateway.\nYou plan to implement a policy that will apply at a product scope and will set\nthe header of inbound requests to include information about the region hosting\nthe gateway of API1. The policy definition contains the following content:\nYou have the following requirements for the policy definition:\n• Ensure that the header contains the information about the region hosting the\ngateway of API1.\n• Ensure the policy applies only after any global level policies are processed\nfirst.\nYou need to complete the policy definition.\nWhich values should you choose? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nGiven answer is correct: https://learn.microsoft.com/en-us/azure/api-management/api-\nmanagement-howto-policies#use-policy-expressions-to-modify-requests\nGiven answer correct. Ref: https://safe.menlosecurity.com/https://learn.microsoft.com/\nen-us/azure/api-management/api-management-howto-policies Article Section: Use\npolicy expressions to modify requests\nCorrect https://learn.microsoft.com/en-us/azure/api-management/api-management-\nhowto-deploy-multi-region\n<base /> is correct: https://learn.microsoft.com/en-us/azure/azure-functions/functions-\nbindings-expressions-patterns#trigger-metadata\nTARGET1: <base/> TARGET2: context",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "374",
    "question": "You are developing several Azure API Management (APIM) hosted APIs.\nYou must make several minor and non-breaking changes to one of the APIs. The\nAPI changes include the following requirements:\n• Must not disrupt callers of the API.\n• Enable roll back if you find issues.\n• Documented to enable developers to understand what is new.\n• Tested before publishing.\nYou need to update the API.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure and apply header-based versioning.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create and publish a product.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure and apply a custom policy.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Add a new revision to the API.\nE) Configure and apply query string-based versioning. \nPage 1172 of 1410\n1173 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "375",
    "question": "HOTSPOT\n-\nYou are developing an application to store millions of images in Azure blob\nstorage.\nThe application has the following requirements:\n• Store the Exif (exchangeable image file format) data from the image as blob\nmetadata when the application uploads the image.\n• Retrieve the Exif data from the image while minimizing bandwidth and\nprocessing time.\n• Utilizes the REST API.\nYou need to use the image Exif data as blob metadata in the application.\nWhich HTTP verbs should you use? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nRetrieve via HEAD because \"For example, if your application needs the Exif (exchangeable\nimage format) data from a photo, it can retrieve the photo and extract it. To save\nbandwidth and improve performance, your application can store the Exif data in the\nblob's metadata when the application uploads the photo. You can then retrieve the Exif\ndata in metadata using only a HEAD request.\" Ref: https://learn.microsoft.com/en-us/\nazure/storage/blobs/storage-performance-checklist#use-metadata\nwe want to store as Metadata : so PUT for set and GET/HEAD to retrieve. https://\nlearn.microsoft.com/en-us/training/modules/work-azure-blob-storage/6-set-retrieve-\nproperties-metadata-rest?ns-enrollment-type=learningpath&ns-enrollment-\nid=learn.wwl.develop-solutions-that-use-blob-storage\nI got this question on 6th August 2023. chose highly voted first box: PUT, Second Box:\nPOST. passed with 904. I got a Case study: city and Lights. All questions are from\nSecExams.\n\n\nTo store Exif data, you should use the PUT verb. To retrieve Exif data, you should use the\nHEAD verb. Explanation: PUT is used to upload the blob to Azure Storage and set the\nblob's metadata at the same time. The metadata is sent in the headers of the PUT\nrequest. HEAD is used to retrieve the metadata of a blob without downloading the blob\nitself. This minimizes bandwidth and processing time as only the headers of the response\n(which contain the metadata) are sent.\nStore can be PUT or POST, make no sense, I think Head for store and retrieve.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "376",
    "question": "You are developing several microservices to run on Azure Container Apps for a\ncompany. External TCP ingress traffic from the internet has been enabled for the\nmicroservices.\nThe company requires that the microservices must scale based on an Azure\nEvent Hub trigger.\nYou need to scale the microservices by using a custom scaling rule.\nWhich two Kubernetes Event-driven Autoscaling (KEDA) trigger fields should you\nuse? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "metadata",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "type",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "authenticationRef",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "name\nE) metricType",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B"
  },
  {
    "number": "377",
    "question": "HOTSPOT\n-\nYou are developing an Azure Function App named App1. You also plan to use\ncross-origin requests (CORS).\nYou have the following requirements:\n• App1 functions must securely access an Azure Blob Storage account.\n• Access to the Azure Blob Storage account must not require the provisioning or\nrotation of secrets.\n• JavaScript code running in a browser on an external host must not be allowed\nto interact with the function.\nYou need to implement App1.\nWhich configuration should you use? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nCorrect answer is : System managed Identity Configure allowed origins to disable\naccording to this link: https://learn.microsoft.com/en-us/cli/azure/functionapp/cors?\nview=azure-cli-latest#az-functionapp-cors-credentials\nSystem Managed Identity: System managed identities are tied to your Azure service and\nare automatically cleaned up when the resource is deleted. They are easier to use if your\napplication only needs to authenticate to services that support Azure AD authentication.\nConfigure CORS allowed origins to none: To disallow JavaScript code running in a browser\non an external host from interacting with the function, you should not include that host\nin your CORS policy. Configuring CORS allowed origins to none will prevent any domain\nfrom accessing your function app.\nSystem Managed Identity: System managed identities are tied to your Azure service and\nare automatically cleaned up when the resource is deleted. They are easier to use if your\napplication only needs to authenticate to services that support Azure AD authentication.\nConfigure CORS allowed origins to none: To disallow JavaScript code running in a browser\non an external host from interacting with the function, you should not include that host\nin your CORS policy. Configuring CORS allowed origins to none will prevent any domain\nfrom accessing your function app.\n\n\nis this question is part of any case study\nAnswer is correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "378",
    "question": "You develop a web application that sells access to last-minute openings for\nchild camps that run on the weekends. The application uses Azure Application\nInsights for all alerting and monitoring.\nThe application must alert operators when a technical issue is preventing sales\nto camps.\nYou need to build an alert to detect technical issues.\nWhich alert type should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Metric alert using multiple time series",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Metric alert using dynamic thresholds",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Log alert using multiple time series",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Log alert using dynamic thresholds",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "379",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "B)",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "D)",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "380",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "C)\nPage 1182 of 1410\n1183 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "381",
    "question": "A company is developing a solution that allows smart refrigerators to send\ntemperature information to a central location.\nThe solution must receive and store messages until they can be processed. You\ncreate an Azure Service Bus instance by providing a name, pricing tier,\nsubscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "B)",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "D)",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "382",
    "question": "HOTSPOT\n-\nYou develop a containerized application. The application must be deployed to\nan existing Azure Kubernetes Service (AKS) cluster from an Azure Container\nRegistry (ACR) instance. You use the Azure command-line interface (Azure CLI) to\ndeploy the application image to AKS.\nImages must be pulled from the registry. You must be able to view all registries\nwithin the current Azure subscription. Authentication must be managed by\nMicrosoft Entra ID and removed when the registry is deleted. The solution must\nuse the principle of least privilege.\nYou need to configure authentication to the registry.\nWhich authentication configuration should you use? To answer, select the\nappropriate configuration values in the answer area,\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nRegistry Azure RBAC Role: should be reader . to meet the requirements \"Images must be\npulled from the registry.\" \"You must be able to view all registries within the current Azure\nsubscription.\" Check the table :Role/Permission https://learn.microsoft.com/en-us/\nazure/container-registry/container-registry-roles?tabs=azure-cli#pull-image\nSystem-assigned managed identity Reader: The Reader has permissions to view the\nregistry and pull images.\ncorrect. Registry Authentication Method: A System-assigned Managed Identity. It's tied to\nthe AKS service and automatically managed by Azure, aligning with the requirement for\nauthentication to be managed by Microsoft Entra ID and removed when the registry is\ndeleted. It adheres to the principle of least privilege as it's specific to the AKS resource.\nRegistry Azure RBAC Role: The AcrPull role is the most fitting. It provides just enough\npermission to pull images from ACR, aligning with the principle of least privilege and\nmeeting the requirement of the deployment process.\nCorrect https://learn.microsoft.com/en-us/azure/container-registry/container-registry-\nroles?tabs=azure-cli\nCorrect https://learn.microsoft.com/en-us/azure/container-registry/container-registry-\nroles?tabs=azure-cli",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "383",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website -\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms -\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors -\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n\n\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms -\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors -\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff -\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity -\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues -\n\n\nCorporate website -\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors -\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\nYou need to implement farmer authentication.\nWhich three actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Add the shared access signature (SAS) token to the app.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a shared access signature (SAS) token.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a user flow.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Add the app to the user flow. \nE) Register the app in Microsoft Entra ID.",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "DE"
  },
  {
    "number": "384",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website -\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms -\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors -\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n\n\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms -\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors -\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff -\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity -\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues -\n\n\nCorporate website -\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors -\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\nYou need to secure the corporate website to meet the security requirements.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create an Azure Cache for Redis instance. Update the code to support the cache.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an Azure Content Delivery Network profile and endpoint. Configure the endpoint.\nС. Create an App Service instance with a standard plan. Configure the custom domain with a\nTLS/SSL certificate.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure Application Gateway with a Web Application Firewall (WAF). Configure end-to-\nend TLS encryption and the WAF.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "385",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to display the profile photo and email for signed-in internal staff on\nthe website.\nWhich Microsoft Graph configuration should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is: 1. /v1.0/me 2. User.Read profilePhoto is not valid name in graph api. https://\nlearn.microsoft.com/en-us/graph/api/profilephoto-get?view=graph-rest-1.0&tabs=http\nAnswer is: Endpoint: /v1.0/users?$select=profilePhoto,mail Permission:\nUser.ManageIdentities.All 1. You cannot use the /me endpoint because you need to list\nall the employees on the website. 2. Second /me endpoint wont return the user photo.\ngiven answer is correct: https://learn.microsoft.com/en-us/graph/api/resources/profile-\nexample?view=graph-rest-beta&viewFallbackFrom=graph-rest-1.0\nCan Anyone help me and tell me where I can find all question related to Farmers? Topics\nand questions number? Thank you very much !\nUse search button",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "386",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website -\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms -\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors -\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n\n\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms -\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors -\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff -\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity -\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues -\n\n\nCorporate website -\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors -\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\nYou need to configure all site configuration settings for the corporate website.\nWhich three actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create a managed identity.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Update the role assignments for the Azure Key Vault.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create an Azure App Configuration store.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Update the role assignments for the Azure App Configuration store.\nE) Create an Azure Key Vault.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "BE"
  },
  {
    "number": "387",
    "question": "You are developing an application that uses keys stored in Azure Key Vault.\nYou need to enforce a specific cryptographic algorithm and key size for keys\nstored in the vault.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Secret versioning",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Policy",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Key Vault Firewall",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Access policies",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "388",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website -\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms -\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors -\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements -\nThe application components must meet the following requirements:\nCorporate website -\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n\n\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms -\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors -\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff -\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity -\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues -\n\n\nCorporate website -\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors -\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\nYou need to implement an aggregate of telemetry values for distributor API\ncalls.\nWhich Application Insights API method should you use?",
    "options": [
      {
        "letter": "A",
        "text": "TrackEvent",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "TrackDependency",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "TrackMetric",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "TrackException",
        "is_correct": false
      },
      {
        "letter": "E",
        "text": "TrackTrace",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "389",
    "question": "DRAG DROP\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to correct the internal staff issue with webpages.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nit seems correct. https://learn.microsoft.com/en-us/azure/cdn/cdn-improve-\nperformance\nAnswer looks correct. Profile and Endpoint are covered here as the two steps: https://\nlearn.microsoft.com/en-us/azure/cdn/cdn-create-new-endpoint The text mentions\n\"webpage load sizes are large\" so compression matches that.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "390",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to correct the errors for farmers and distributors.\nWhich solution should you use? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nsorry for distributors' errors: Creating a custom autoscale rule to increase the instance\ncount may help to solve the error\nsorry for distributors' errors: Creating a custom autoscale rule to increase the instance\ncount may help to solve the error\nSeems to me this is a scaling up and scaling out solution. Which makes sense in MS's\nbusiness objectives.\nit seems correct. Reference: https://learn.microsoft.com/en-us/azure/app-service/\ntroubleshoot-http-502-http-503\nCorrect. Farmers' errors: HTTP 503 errors: These errors are most likely due to insufficient\ncompute resources on the App Service plan. Scaling up the App Service plan to Premium\nwill provide more CPU and memory resources, which will help to handle the increased\ndemand and reduce the likelihood of HTTP 503 errors. Distributors' errors: HTTP 502\nerrors: These errors are typically caused by communication problems between the\napplication and the underlying infrastructure. Restarting the application from the App\nService portal may resolve the issue.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "391",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to resolve the authentication errors for developers.\nWhich Service Bus security configuration should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nContributor (The principle of least privilege must be applied when providing any user\nrights or process access rights.) Queue (A queue-based load leveling pattern must be\nimplemented by using Azure Service Bus queues to support high volumes of website\nagricultural production resource requests.)\ncorrect. Azure RBAC Role: Service Bus Data Owner. This role is appropriate for developers\nwho need to debug Service Bus as it provides comprehensive access to the data plane,\nincluding both sending and receiving messages, which are essential capabilities for\ndebugging. Service Bus Scope: Namespace. The Namespace scope is recommended if the\ndevelopers need to work across multiple queues or topics within the same Service Bus\nnamespace. If the debugging is limited to a single queue, then \"Queue\" could be more\nappropriate. However, \"Namespace\" offers more flexibility without being overly broad.\n\"using Azure Service Bus queues to support high volumes of website agricultural\nproduction resource requests.\" Service bus data sender --> principle of least privilege\nqueue\nThink the answer is correct, reference: https://learn.microsoft.com/en-us/azure/service-\nbus-messaging/service-bus-managed-service-identity\nCorrect answer is Service Bus Data Owner, Namespace",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "392",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are implementing an application by using Azure Event Grid to push near-\nreal-time information to customers.\nYou have the following requirements:\n• You must send events to thousands of customers that include hundreds of\nvarious event types.\n• The events must be filtered by event type before processing.\n• Authentication and authorization must be handled by using Microsoft Entra ID.\n• The events must be published to a single endpoint.\nYou need to implement Azure Event Grid.\nSolution: Publish events to an event domain. Create a custom topic for each\ncustomer.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "393",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are implementing an application by using Azure Event Grid to push near-\nreal-time information to customers.\nYou have the following requirements:\n• You must send events to thousands of customers that include hundreds of\nvarious event types.\n• The events must be filtered by event type before processing.\n• Authentication and authorization must be handled by using Microsoft Entra ID.\n• The events must be published to a single endpoint.\nYou need to implement Azure Event Grid.\nSolution: Publish events to a custom topic. Create an event subscription for\neach customer.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No \nPage 1243 of 1410\n1244 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "394",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are implementing an application by using Azure Event Grid to push near-\nreal-time information to customers.\nYou have the following requirements:\n• You must send events to thousands of customers that include hundreds of\nvarious event types.\n• The events must be filtered by event type before processing.\n• Authentication and authorization must be handled by using Microsoft Entra ID.\n• The events must be published to a single endpoint.\nYou need to implement Azure Event Grid.\nSolution: Enable ingress, create a TCP scale rule, and apply the rule to the\ncontainer app.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "395",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nMunson’s Pickles and Preserves Farm is an agricultural cooperative corporation\nbased in Washington, US, with farms located across the United States. The\ncompany supports agricultural production resources by distributing seeds\nfertilizers, chemicals, fuel, and farm machinery to the farms.\nCurrent Environment\n-\nThe company is migrating all applications from an on-premises datacenter to\nMicrosoft Azure. Applications support distributors, farmers, and internal\ncompany staff.\nCorporate website\n-\n• The company hosts a public website located at http://\nwww.munsonspicklesandpreservesfarm.com. The site supports farmers and\ndistributors who request agricultural production resources.\nFarms\n-\n• The company created a new customer tenant in the Microsoft Entra admin\ncenter to support authentication and authorization for applications.\nDistributors\n-\n• Distributors integrate their applications with data that is accessible by using\nAPIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to\nreceive and update resource data.\nRequirements\n-\n\n\nThe application components must meet the following requirements:\nCorporate website\n-\n• The site must be migrated to Azure App Service.\n• Costs must be minimized when hosting in Azure.\n• Applications must automatically scale independent of the compute resources.\n• All code changes must be validated by internal staff before release to\nproduction.\n• File transfer speeds must improve, and webpage-load performance must\nincrease.\n• All site settings must be centrally stored, secured without using secrets, and\nencrypted at rest and in transit.\n• A queue-based load leveling pattern must be implemented by using Azure\nService Bus queues to support high volumes of website agricultural production\nresource requests.\nFarms\n-\n• Farmers must authenticate to applications by using Microsoft Entra ID.\nDistributors\n-\n• The company must track a custom telemetry value with each API call and\nmonitor performance of all APIs.\n• API telemetry values must be charted to evaluate variations and trends for\nresource data.\nInternal staff\n-\n• App and API updates must be validated before release to production.\n• Staff must be able to select a link to direct them back to the production app\n\n\nwhen validating an app or API update.\n• Staff profile photos and email must be displayed on the website once they\nauthenticate to applications by using their Microsoft Entra ID.\nSecurity\n-\n• All web communications must be secured by using TLS/HTTPS.\n• Web content must be restricted by country/region to support corporate\ncompliance standards.\n• The principle of least privilege must be applied when providing any user rights\nor process access rights.\n• Managed identities for Azure resources must be used to authenticate services\nthat support Microsoft Entra ID authentication.\nIssues\n-\nCorporate website\n-\n• Farmers report HTTP 503 errors at the same time as internal staff report that\nCPU and memory usage are high.\n• Distributors report HTTP 502 errors at the same time as internal staff report\nthat average response times and networking traffic are high.\n• Internal staff report webpage load sizes are large and take a long time to load.\n• Developers receive authentication errors to Service Bus when they debug\nlocally.\nDistributors\n-\n• Many API telemetry values are sent in a short period of time. Telemetry traffic,\ndata costs, and storage costs must be reduced while preserving a statistically\ncorrect analysis of the data points sent by the APIs.\n\n\nYou need to provide internal staff access to the production site after a\nvalidation.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\nThis looks to be right based on: https://learn.microsoft.com/en-us/azure/app-service/\ndeploy-staging-slots?tabs=portal \"The string x-ms-routing-name=self specifies the\nproduction slot. After the client browser accesses the link, it's redirected to the\nproduction slot. Every subsequent request has the x-ms-routing-name=self cookie that\npins the session to the production slot.\"\n<a href=\"https://www.munsonspicklesandpreservesfarm.com/?x-ms-routing-\nname=self\">Go back to production app</a> It is \"x-ms-routing-name=self\"\n\n\nhttps://learn.microsoft.com/en-us/azure/app-service/deploy-staging-slots?\ntabs=portal#route-production-traffic-automatically\nhttps://learn.microsoft.com/en-us/azure/app-service/deploy-staging-slots?\ntabs=portal#route-production-traffic-automatically",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "396",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are implementing an application by using Azure Event Grid to push near-\nreal-time information to customers.\nYou have the following requirements:\n• You must send events to thousands of customers that include hundreds of\nvarious event types.\n• The events must be filtered by event type before processing.\n• Authentication and authorization must be handled by using Microsoft Entra ID.\n• The events must be published to a single endpoint.\nYou need to implement Azure Event Grid.\nSolution: Publish events to a partner topic. Create an event subscription for\neach customer.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 1253 of 1410\n1254 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "397",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou are implementing an application by using Azure Event Grid to push near-\nreal-time information to customers.\nYou have the following requirements:\n• You must send events to thousands of customers that include hundreds of\nvarious event types.\n• The events must be filtered by event type before processing.\n• Authentication and authorization must be handled by using Microsoft Entra ID.\n• The events must be published to a single endpoint.\nYou need to implement Azure Event Grid.\nSolution: Publish events to a system topic. Create an event subscription for\neach customer.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "398",
    "question": "DRAG DROP\n-\nYou have an Azure Cosmos DB for NoSQL API account named account1 and a\ndatabase named db1. An application named app1 will access db1 to perform\nread and write operations.\nYou plan to modify the consistency levels for read and write operations\nperformed by app1 on db1.\nYou must enforce the consistency level on a per-operation basis whenever\npossible.\nYou need to configure the consistency level for read and write operations.\nWhich locations should you configure? To answer, move the appropriate\nlocations to the correct operations. You may use each location once, more than\nonce, or not at all. You may need to move the split bar between panes or scroll\nto view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "399",
    "question": "HOTSPOT\n-\nYou are creating an Azure Functions app project in your local development\nenvironment by using Azure Functions Core Tools.\nYou must create the project in either Python or C# without using a template.\nYou need to specify the command and its parameter required to create the\nAzure Functions app project.\nWhich command and parameter should you specify? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "400",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service plan named APSPlan1 set to the Basic B1 pricing\ntier. APSPlan1 contains an App Service web app named WebApp1.\nYou plan to enable schedule-based autoscaling for APSPlan1.\nYou need to minimize the cost of running WebApp1.\nSolution: Scale down ASPPlan1 to the Shared pricing tier.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "401",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service plan named APSPlan1 set to the Basic B1 pricing\ntier. APSPlan1 contains an App Service web app named WebApp1.\nYou plan to enable schedule-based autoscaling for APSPlan1.\nYou need to minimize the cost of running WebApp1.\nSolution: Scale out APSPlan1.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "402",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service plan named APSPlan1 set to the Basic B1 pricing\ntier. APSPlan1 contains an App Service web app named WebApp1.\nYou plan to enable schedule-based autoscaling for APSPlan1.\nYou need to minimize the cost of running WebApp1.\nSolution: Scale up ASPPlan1 to the Premium V2 pricing tier.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "403",
    "question": "HOTSPOT\n-\nYou have an Azure Functions app using the Consumption hosting plan for a\ncompany. The app contains the following functions:\nYou plan to enable dynamic concurrency on the app. The company requires that\neach function has its concurrency level managed separately.\nYou need to configure the app for dynamic concurrency.\nWhich file or function names should you use? To answer, select the appropriate\nvalues in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAnswer is correct. https://learn.microsoft.com/en-us/azure/azure-functions/functions-\nconcurrency",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "404",
    "question": "You are developing an app to store globally distributed data in several Azure\nBlob Storage containers. Each container hosts multiple blobs where each\ninstance of the app will store the data. You enable versioning and soft delete\nfor the blobs.\nApp testing and incorrect code have frequently corrupted data. Development of\nthe app must allow data to be restored to a previous day for testing.\nYou need to configure the storage account to support point-in-time restore.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable the change feed on the storage account to begin capturing and recording changes.\n\nPage 1265 of 1410\n1266 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Configure object replication and specify replication rules.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a snapshot of the blob in the hot tier.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure an immutability policy that is scoped to a blob version.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "405",
    "question": "You manage an Azure subscription that contains 100 Azure App Service web\napps. Each web app is associated with an individual Application Insights\ninstance.\nYou plan to remove Classic availability tests from all Application Insights\ninstances that have this functionality configured.\nYou have the following PowerShell statement:\nGet-AzApplicationInsightsWebTest | Where-Object { $condition }\nYou need to set the value of the $condition variable.\nWhich value should you use?",
    "options": [
      {
        "letter": "A",
        "text": "$_.Type -eq \"ping\"",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "$_.WebTestKind -eq \"ping\"",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "$_.WebTestKind -eq \"standard\" \nPage 1266 of 1410\n1267 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "$_.Type -eq \"standard\"",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "406",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou deploy an Azure Container Apps app and disable ingress on the container\napp.\nUsers report that they are unable to access the container app. You investigate\nand observe that the app has scaled to 0 instances.\nYou need to resolve the issue with the container app.\nSolution: Enable ingress, create an TCP scale rule, and apply the rule to the\ncontainer app.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "407",
    "question": "DRAG DROP\n-\nYou have two Azure Container Registry (ACR) instances: ACR01 and ACR02.\nYou plan to implement a containerized application named APP1 that will use a\nbase image named BASE1. The image for APP1 will be stored in ACR01. The image\nBASE1 will be stored in ACR02.\nYou need to automate the planned implementation by using a sequence of five\nAzure command-line interface (Azure CLI) commands. Your solution must ensure\nthat the APP1 image stored in ACR01 will be automatically updated when the\nBASE1 image is updated.\nIn which order should you perform the actions? To answer, move all container\nbuild automation options from the list of container build automations to the\nanswer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit\nfor any of the correct orders you select.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nCorrect! Reference: https://learn.microsoft.com/en-us/azure/container-registry/\ncontainer-registry-tutorial-private-base-image-update\nCorrect! Ref.: https://learn.microsoft.com/en-us/azure/container-registry/container-\nregistry-tutorial-private-base-image-update",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "408",
    "question": "A company uses an Azure Blob Storage for archiving.\nThe company requires that data in the Blob Storage is only in the archive tier.\nYou need to ensure data copied to the Blob Storage is moved to the archive tier.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use a Put Block List operation with a request header of x-ms-immutability-policy-mode.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a lifecycle policy with an action of tierToArchive and configure\ndaysAfterModificationGreaterThan for 0.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Use a Put Blob operation with a request header of x-ms-immutability-policy-until-date.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a lifecycle policy with an action of tierToArchive and configure a filter for\nPage 1270 of 1410\n1271 Microsoft - AZ-204 Practice Questions - SecExams.com\nblobIndexMatch.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "409",
    "question": "HOTSPOT\n-\nYou have the following data lifecycle management policy:\nYou plan to implement an Azure Blob Storage account and apply to it Policy1.\nThe solution should maximize resiliency and performance.\nYou need to configure the account to support the policy.\n\n\nWhich redundancy option and storage account type should you use? To answer,\nselect the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI think the answer is correct. The best performance option is Premium Block Blob. And\nfor this selection, the best redundancy option offered is ZRS.\nZRS Premium block blob https://learn.microsoft.com/en-us/azure/storage/common/\nstorage-redundancy",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "410",
    "question": "HOTSPOT\n-\nYou have an Azure Cosmos DB for NoSQL API account named account1. Multiple\ninstances of an on-premises application named app1 read data from account1.\nYou plan to implement integrated cache for connections from the instances of\napp to account1.\nYou need to set the connection mode and maximum consistency level of app1.\nWhich values should you use for the configuration settings? To answer, select\nthe appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nThe answer is correct! Gateway mode using dedicated gateway: Share cache across\nmultiple app instances. Allow integrated caching, standard gateway not. Session\nconsistency level: The maximum consistency leven applied to gateway mode.\nConnectivity mode: Gateway mode using dedicated gateway https://learn.microsoft.com/\nen-us/azure/cosmos-db/dedicated-gateway Consistency level: Session https://\nlearn.microsoft.com/en-us/azure/cosmos-db/how-to-configure-integrated-cache?\ntabs=dotnet#adjust-request-consistency",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "411",
    "question": "You are developing a Cosmos DB solution that will be deployed to multiple\nAzure regions.\nYour solution must meet the following requirements:\n• Read operations will never receive write operations that are out of order.\n• Maximize concurrency of read operations in all regions.\nYou need to choose the consistency level for the solution.\nWhich consistency level should you use?",
    "options": [
      {
        "letter": "A",
        "text": "session",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "eventual",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "bounded staleness",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "consistent prefix",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "412",
    "question": "You have an Azure Queue Storage named queue1.\nYou plan to develop code that will process messages in queue1.\nYou need to implement a queue operation to set the visibility timeout value of\nindividual messages in queue1.\nWhich two operations can you use? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Peek at a message in the queue.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Delete a message in the queue.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add a message to the queue.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Update a message in the queue. \nE) Receive a message from the queue.",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D ?"
  },
  {
    "number": "413",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\nCurrent environment\n-\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website\n-\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items\n-\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders\n-\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\n\n\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\nperformance.\nThe application consists of the following Azure services:\n\n\nRequirements\n-\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\n\n\nStorage APIs.\nCorporate website\n-\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items\n-\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders\n-\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues\n-\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to save customized items to Azure Cosmos DB.\nWhich Azure Cosmos DB configuration should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nIncorrect. NoSql Session",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "414",
    "question": "HOTSPOT\n-\nA company has an Azure Cosmos DB for NoSQL account. The account is\nconfigured for session consistency. Data is written to a single Azure region and\ndata can be read from three Azure regions.\nAn application that will access the Azure Cosmos DB for NoSQL container data\nusing an SDK has the following requirements:\n• Reads from the application must return the most recent committed version of\nan item from any Azure region.\n• The container items should not automatically be deleted.\nYou need to implement the changes to the Azure Cosmos DB for NoSQL account.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "415",
    "question": "HOTSPOT\n-\nYou are developing an application that monitors data added to an Azure Blob\nstorage account.\nYou need to process each change made to the storage account.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "416",
    "question": "You manage an Azure Cosmos DB for a NoSQL API account named account1. The\naccount contains a database named db1, which contains a container named\ncontainer1. You configure account1 with a session consistency level.\nYou plan to develop an application named App1 that will access container1.\nIndividual instances of App1 must perform reads and writes. App1 must allow\nmultiple nodes to participate in the same session.\nYou need to configure an object to share the session token between the nodes.\nWhich object should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Document response",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Request options -\nС. Feed options\nPage 1291 of 1410\n1292 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Connection policy",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "417",
    "question": "DRAG DROP\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\nCurrent environment\n-\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website\n-\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items\n-\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders\n-\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\n\n\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\nperformance.\nThe application consists of the following Azure services:\n\n\nRequirements\n-\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\n\n\nStorage APIs.\nCorporate website\n-\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items\n-\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders\n-\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues\n-\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to store inventory item images.\nWhich Azure Blob Storage feature should you use? To answer, move the\nappropriate Azure Blob Storage features to the correct requirements. You may\nuse each Azure Blob Storage feature once, more than once, or not at all. You\nmay need to move the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\nIndex tags??",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "418",
    "question": "HOTSPOT\n-\nA company has an Azure storage static website with a custom domain name.\nThe company informs you that unauthorized users from a different country/\nregion are accessing the website. The company provides the following\nrequirements for the static website:\n• Unauthorized users must not be able to access the website.\n• Users must be able to access the website using the HTTPS protocol.\nYou need to implement the changes to the static website.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI think it's wrong. My purpose is: Configure the storage account to use Microsoft Entra ID\nConfigura the storage account to require secure transfer",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "419",
    "question": "DRAG DROP\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\nCurrent environment\n-\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website\n-\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items\n-\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders\n-\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\n\n\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\nperformance.\nThe application consists of the following Azure services:\n\n\nRequirements\n-\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\n\n\nStorage APIs.\nCorporate website\n-\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items\n-\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders\n-\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues\n-\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to secure the corporate website for users.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit\nfor any of the correct orders you select.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "420",
    "question": "You are developing a Microsoft Entra ID integrated app that interacts with\nMicrosoft Graph.\nYou must allow GET operations to receive unknown members that might be\ndefined in the future in Microsoft Graph API. You plan to include support for\nevolvable enumerations in the app.\nYou need to specify the HTTP request header that will provide the evolvable\nenumerations support in the app.\nWhich header should you specify?",
    "options": [
      {
        "letter": "A",
        "text": "Accept",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Content-Type",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "If-Match",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Prefer",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "421",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service web app named WebApp1 and an Azure\nFunctions app named Function1. WebApp1 is associated with an Application\nInsights instance named appinsights1.\nYou configure a web test and a corresponding alert for WebApp1 in appinsights1.\nEach alert triggers a delivery of email to your mailbox.\nYou need to ensure that each alert also triggers execution of Function1.\nSolution: Configure an Azure Monitor Insights workbook.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "422",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service web app named WebApp1 and an Azure\nFunctions app named Function1. WebApp1 is associated with an Application\nInsights instance named appinsights1.\nYou configure a web test and a corresponding alert for WebApp1 in appinsights1.\nEach alert triggers a delivery of email to your mailbox.\nYou need to ensure that each alert also triggers execution of Function1.\nSolution: Configure an Application Insights smart detection.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "423",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service web app named WebApp1 and an Azure\nFunctions app named Function1. WebApp1 is associated with an Application\nInsights instance named appinsights1.\nYou configure a web test and a corresponding alert for WebApp1 in appinsights1.\nEach alert triggers a delivery of email to your mailbox.\nYou need to ensure that each alert also triggers execution of Function1.\nSolution: Configure an Azure Monitor action group.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "424",
    "question": "You have a Standard tier instance of Azure Cache for Redis named redis1\nconfigured with the default settings.\nYou need to configure a Maxmemory policy to increase the amount of cache\navailable for read operations.\nHow should you configure the Maxmemory policy?",
    "options": [
      {
        "letter": "A",
        "text": "Decrease the value of maxmemory-reserved.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Increase the value of maxmemory-reserved.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Set the Maxmemory policy to noeviction.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set the Maxmemory policy to volatile-lru.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "425",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure App Service web app named WebApp1 and an Azure\nFunctions app named Function1. WebApp1 is associated with an Application\nInsights instance named appinsights1.\nYou configure a web test and a corresponding alert for WebApp1 in appinsights1.\nEach alert triggers a delivery of email to your mailbox.\nYou need to ensure that each alert also triggers execution of Function1.\nSolution: Configure an Application Insights funnel.\nDoes the solution meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "426",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\n\n\nCurrent environment -\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders -\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\n\n\nperformance.\nThe application consists of the following Azure services:\nRequirements -\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n\n\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\nStorage APIs.\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders -\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues -\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to mitigate the Azure Cache for Redis issue.\nWhat are two possible ways to achieve this goal? Each correct answer presents\npart of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Test application code by rebooting all nodes in the test environment.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Configure client connections to retry commands with exponential backoff.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Modify the maxmemory policy to evict the least frequently used keys out of all keys.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Increase the maxmemory-reserved and maxfragmentationmemory-reserved values.\nE) Test application code by purging the cache in the test environment.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "B ?"
  },
  {
    "number": "427",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\n\n\nCurrent environment -\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders -\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\n\n\nperformance.\nThe application consists of the following Azure services:\nRequirements -\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n\n\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\nStorage APIs.\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders -\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues -\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to serve static content from the corporate website.\nWhat are two possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Store all static content in Azure Blob Storage. Enable Azure Content Delivery Network for\nthe storage account.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Configure App Service networking to create a Content Delivery Network profile and endpoint.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure the Azure App Service Local Cache feature and set the app setting\nWEBSITE_LOCAL_CACHE_SIZEINMB value.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a nested Azure Traffic Manager profile. Configure the parent profile to the\nperformance traffic routing method and the child profile to the priority traffic routing method.\n\nE) Update the Azure Traffic Manager routing method to priority.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D ?"
  },
  {
    "number": "428",
    "question": "You develop an ASP. Net Care application by integrating the Application Insights\nSDK into your solution.\nThe application sends a very high rate of telemetry in a short time interval. You\nobserve a reduced number of events, traces, and metrics being recorded and\nincreased error rates for telemetry ingestion. Telemetry data must synchronize\nthe client and server information to allow HTTP request and response\ncorrelation.\nYou need to reduce telemetry traffic, data costs, and storage costs while\npreserving a statistically correct analysis of application telemetry data.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Set a daily cap on the Log Analytics workspace. Create an Activity log alert rule.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Modify the pricing tier for the Log Analytics workspace.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update the application code to reduce the number of DiagnosticSource events. Use filtering\nto exclude these events.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Disable adaptive sampling. Enable and configure the fixed-rate sampling module.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "429",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\n\n\nCurrent environment -\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders -\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\n\n\nperformance.\nThe application consists of the following Azure services:\nRequirements -\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n\n\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\nStorage APIs.\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders -\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues -\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to implement the processing of enqueue inventory items.\nWhich message value should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Sequence number",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Timestamp",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Session identifier",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Partition key",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "430",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\nCurrent environment\n-\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website\n-\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items\n-\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders\n-\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\n\n\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\nperformance.\nThe application consists of the following Azure services:\n\n\nRequirements\n-\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\n\n\nStorage APIs.\nCorporate website\n-\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items\n-\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders\n-\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues\n-\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to implement a function by using Azure Functions to process\ncustomized items.\nHow should you implement the function? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "431",
    "question": "HOTSPOT\n-\nYou plan to implement an Azure function named Function1 that will use the\ntimer trigger.\nYou plan to use a TimeSpan value to set the schedule of function execution.\nYou need to select the hosting model and the schedule expression assignment\nmethod.\nWhich hosting model and schedule expression assignment method should you\nuse? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "432",
    "question": "HOTSPOT\n-\nYou are developing a microservices-based application that uses Azure Container\nApps. The application consists of several containerized services that handle\ntasks, such as processing orders, managing inventory, and generating reports.\nYou must secure the container apps. All apps must reside in the same virtual\nnetwork, share the same Dapr configuration, and share the same logging\nlocation.\nApps must support the configuration of the amount of memory and compute\nresources available to containers.\nYou need to configure the Azure Container App.\nHow should you complete the CLI command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "433",
    "question": "DRAG DROP\n-\nYou have a static website hosted in an Azure Storage account named storage1.\nYou access the website by using a URL that ends with the web.core.windows.net\nsuffix.\nYou plan to configure the website to be accessible through the URL\nwww.contoso.com. The website must be accessible during configuration.\nThe contoso.com zone is hosted in Azure DNS.\nYou need to complete the website configuration.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate domain name configuration steps from the list of domain name\nconfiguration steps to the answer area and arrange them in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "434",
    "question": "You have an Azure Queue Storage account that contains a queue named queue1.\nYou plan to use Azure SDK for .NET to develop a solution that uses queue1.\nYou need to author C# code that will return an approximate number of\nmessages in queue1. Your solution must minimize the development effort.\nWhich method should you use in your code?",
    "options": [
      {
        "letter": "A",
        "text": "GetProperties method of the QueueClient class",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "GetProperties method of the QueueServiceClient class",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "PeekMessages method of the QueueClient class",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "GetStatistics method of the QueueServiceClient class",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "435",
    "question": "You manage an Azure Storage account named storage1.\nYou plan to load 1 million blobs into storage1.\nYou must assign key-value pairs to blobs so that both keys and their values are\nautomatically indexed and searchable by using the built-in services of storage1.\nYou need to run the command to assign key-value pairs.\nWhich command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "Update -AzStorageBobServiceProperty",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Set-AzStorageBlobTag",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "az storage blob service-properties update",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set-AzStorageBlobContent",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "436",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\n\n\nCurrent environment -\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders -\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\n\n\nperformance.\nThe application consists of the following Azure services:\nRequirements -\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n\n\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\nStorage APIs.\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders -\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues -\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to securely access inventory items when developing the Inventory\nItems API.\nWhat are three possible ways to achieve this goal? Each correct answer presents\na complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create a SQL role definition under the Azure Cosmos DB account.\nCreate a user-assigned managed identity and assign the identity to the function app.\nAssign the user assigned managed identity the SQL role definition.\nUpdate the function app code to implement the DefaultAzureCredential class and reference\nthe user-assigned managed identity.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Create a SQL role definition under the Azure Cosmas DB account.\nAssign the role to the function apps system-assigned managed identity.\nProgrammatically access the Azure Cosmos DB keys from the function app.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a custom Microsoft Entra role.\nAssign the custom roe to the Azure Cosmos DB account\nUpdate the function app to use certificate-based authentication.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a custom Microsoft Entra role.\nAssign the custom role to Azure Key Vault.\nAssign the custom role to the function app.\nReference the custom role in the function app code when accessing Azure Key Vault values.\nE) Create a system-assigned managed ident for the function app with read access to secrets in\nAzure Key Vault.\nStore the Azure Cosmos DB primary key and UR in Azure Key Vaults secrets.\nUse function app settings to reference the secret values.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "BE ?"
  },
  {
    "number": "437",
    "question": "You manage an Azure Key Vault named kv1 of Standard SKU.\nYou plan to programmatically store in kv1 an asymmetric key pair and use the\nkey pair for encryption and decryption.\nYou must develop an application named app1 that will access the key pair in\nkv1.\nYou need to configure an object to retrieve a key pair from kv1.\nWhich object should you use?",
    "options": [
      {
        "letter": "A",
        "text": "SecretClient",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "KeyVaultSettingsClient",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "CertificateClient",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "KeyClient",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "438",
    "question": "You develop an ASP. Net Care application by integrating the Application Insights\nSDK into your solution.\nThe application sends a very high rate of telemetry in a short time interval. You\nobserve a reduced number of events, traces, and metrics being recorded and\nincreased error rates for telemetry ingestion. Telemetry data must synchronize\nthe client and server information to allow HTTP request and response\ncorrelation.\nYou need to reduce telemetry traffic, data costs, and storage costs while\npreserving a statistically correct analysis of application telemetry data.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Set a daily cap on the Log Analytics workspace. Create an Activity log alert rule.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Modify the pricing tier for the Log Analytics workspace.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Verify adaptive sampling is enabled. Set the maxTelemetryItemsPerSecond value. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set retention and archive policies by table in the Log Analytics workspace. Purge retained\ndata beyond 30 days.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "439",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground\n-\n\n\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\nCurrent environment\n-\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website\n-\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items\n-\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders\n-\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\n\n\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\nperformance.\nThe application consists of the following Azure services:\n\n\nRequirements\n-\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\n\n\nStorage APIs.\nCorporate website\n-\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items\n-\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders\n-\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues\n-\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to implement code to process inventory changes and update orders.\nWhich configuration should you use? To answer, select the appropriate options\nin the answer area\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\nit should be push model",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "440",
    "question": "A large retail company operates online and physical stores. The company tracks\ninventory levels in real time to manage stock efficiently across all locations. You\ndevelop an Azure Event Grid solution to handle events generated by the\ninventory management system deployed to Azure.\nYou need to implement a subscription filter that dynamically adjusts to\nseasonal changes in product demand.\nWhich event filter should you use?",
    "options": [
      {
        "letter": "A",
        "text": "An advanced filter using a Boolean condition that evaluates multiple data fields, including a\nseason field within the event data",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "A prefix filter on the event type field that matches the current season's name",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "A subscription filter that uses label filter to include events tagged with seasonal promotional\ncodes",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "A static subject filter that targets events with a subject ending in “/seasonal/inventory”",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "441",
    "question": "HOTSPOT -\nYou have an Azure Service Bus namespace that contains a topic named Topic1.\nYou plan to create a subscription named Sub1 to Topic1. In Sub1, you plan to\nfilter messages from Topic1 based on their system properties and apply an\naction that will annotate each filtered message.\nYou need to configure the filtering.\nHow should you configure the filtering? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "442",
    "question": "HOTSPOT\n-\nYou have a Dockerfile that builds a container image named image1. The\ncontainer image and its base image are stored in separate repositories of an\nAzure Container registry named registry1. The codebase of image1 is stored in a\nGitHub repo named app1 of an account named account1.\nYou plan to implement automatic updates to image1 whenever its base image is\nupdated or an update to the main branch of the GitHub repo occurs.\nYou need to complete the Azure CLI command that will apply the planned\nimplementation.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "443",
    "question": "DRAG DROP\n-\nYou manage an Azure Cosmos DB for a NoSQL API account named account1. You\nconfigure account1 with the default consistency level.\nAn application named app1 must access containers in account1 to perform read\nand write operations. The connections from app1 to account1 must be\nestablished by using the direct mode.\nYou plan to configure app1 to override the default consistency level by using the\nAzure Cosmos DB SDK client.\nYou need to set the maximum consistency level for app1 to use for read and\nwrite operations.\nWhich consistency level should you set? To answer, move the appropriate\nmaximum consistency levels to the correct operation types. You may use each\nmaximum consistency level once, more than once, or not at all. You may need\nto move the split bar between panes or scroll to view content.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "444",
    "question": "DRAG DROP\n-\nYou manage an Azure subscription associated with a Microsoft Entra tenant\nnamed contoso.com. The subscription contains an Azure Blob Storage account\nnamed storage1. Your user account has the Contributor Azure role-based access\ncontrol (RBAC) role within the scope of the subscription.\nYou plan to implement secure access to containers and blobs in storage1. Your\nsolution must satisfy the following requirements:\n• Authorization requests to access storage1 content must be authenticated by\nusing Microsoft Entra credentials.\n• Authorized access to storage1 content must be time-limited based on arbitrary\nvalues specified when requests are raised.\n• The principle of least privilege must be satisfied.\nYou need to implement the plan.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate secure access implementation options from the list of secure\naccess implementations to the answer area and arrange them in the correct\norder.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "445",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. When\nyou are ready to answer a question, click the Question button to return to the\nquestion.\nBackground -\nFourth Coffee is a global coffeehouse chain and coffee company recognized as\none of the world’s most influential coffee brands. The company is renowned for\nits specialty coffee beverages, including a wide range of espresso-based drinks,\nteas, and other beverages. Fourth Coffee operates thousands of stores\nworldwide.\n\n\nCurrent environment -\nThe company is developing cloud-native applications hosted in Azure.\nCorporate website -\nThe company hosts a public website located at http://www.fourthcoffee.com/.\nThe website is used to place orders as well as view and update inventory items.\nInventory items -\nIn addition to its core coffee offerings, Fourth Coffee recently expanded its\nmenu to include inventory items such as lunch items, snacks, and merchandise.\nCorporate team members constantly update inventory. Users can customize\nitems. Corporate team members configure inventory items and associated\nimages on the website.\nOrders -\nAssociates in the store serve customized beverages and items to customers.\nOrders are placed on the website for pickup.\nThe application components process data as follows:\n1. Azure Traffic Manager routes a user order request to the corporate website\nhosted in Azure App Service.\n2. Azure Content Delivery Network serves static images and content to the user.\n3. The user signs in to the application through a Microsoft Entra ID for\ncustomers tenant.\n4. Users search for items and place an order on the website as item images are\npulled from Azure Blob Storage.\n5. Item customizations are placed in an Azure Service Bus queue message.\n6. Azure Functions processes item customizations and saves the customized\nitems to Azure Cosmos DB.\n7. The website saves order details to Azure SQL Database.\n8. SQL Database query results are cached in Azure Cache for Redis to improve\n\n\nperformance.\nThe application consists of the following Azure services:\nRequirements -\nThe application components must meet the following requirements:\n• Azure Cosmos DB development must use a native API that receives the latest\nupdates and stores data in a document format.\n\n\n• Costs must be minimized for all Azure services.\n• Developers must test Azure Blob Storage integrations locally before\ndeployment to Azure. Testing must support the latest versions of the Azure\nStorage APIs.\nCorporate website -\n• User authentication and authorization must allow one-time passcode sign-in\nmethods and social identity providers (Google or Facebook).\n• Static web content must be stored closest to end users to reduce network\nlatency.\nInventory items -\n• Customized items read from Azure Cosmos DB must maximize throughput\nwhile ensuring data is accurate for the current user on the website.\n• Processing of inventory item updates must automatically scale and enable\nupdates across an entire Azure Cosmos DB container.\n• Inventory items must be processed in the order they were placed in the queue.\n• Inventory item images must be stored as JPEG files in their native format to\ninclude exchangeable image file format (data) stored with the blob data upon\nupload of the image file.\n• The Inventory Items API must securely access the Azure Cosmos DB data.\nOrders -\n• Orders must receive inventory item changes automatically after inventory\nitems are updated or saved.\nIssues -\n• Developers are storing the Azure Cosmos DB credentials in an insecure clear\ntext manner within the Inventory Items API code.\n• Production Azure Cache for Redis maintenance has negatively affected\napplication performance.\n\n\nYou need to support local development testing for developers.\nWhich tool should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azurite",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure Storage Emulator",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL Server Management Studio (SSMS)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Storage Explorer",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "446",
    "question": "HOTSPOT\n-\nYou have an Azure Blob Storage account named account1.\nYou plan to grant permissions for access to blobs in account1 by using a\ncombination of a shared access signature token and a stored access policy. You\nmust set the duration of the token validity by using the stored access policy.\nYou need to complete the configuration of the stored access policy and\ngenerate the shared access signature token.\nWhich token setting and token type should you use? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "447",
    "question": "You have 100 Azure virtual machines (VMs) with the system-assigned managed\nidentity enabled.\nYou need to identify the value of the object ID attribute for each of the\nidentities.\nWhich command should you use?",
    "options": [
      {
        "letter": "A",
        "text": "az ad sp credential list",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Get-AzVM",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Get-AzureADUser",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "az resource show",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "448",
    "question": "You have 100 Azure virtual machines (VMs) with the system-assigned managed\nidentity enabled.\nYou need to identify the value of the object ID attribute for each of the\nidentities.\nWhich command should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Get-AzureADUser",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Get-AzResource",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Get-AzureADUserOwnedObject",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "az ad signed-in-user list-owned-objects",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "449",
    "question": "HOTSPOT\n-\nYou are developing a web application that makes calls to the Microsoft Graph\nAPI. You register the application in the Azure portal and upload a valid X509\ncertificate.\nYou create an appsettings.json file containing the certificate name, client\nidentifier for the application, and the tenant identifier of the Microsoft Entra ID.\nYou create a method named ReadCertificate to return the X509 certificate by\nname.\nYou need to implement code that acquires a token by using the certificate.\nHow should you complete the code segment? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "450",
    "question": "You develop applications that integrate with a Microsoft Entra tenant.\nYou plan to implement a permission classification in the tenant.\nYou need to select permissions to include in your classification.\nWhich permissions should you select?",
    "options": [
      {
        "letter": "A",
        "text": "app-only access permissions that require admin consent",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "delegated permissions that require only user consent",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "app-only access permissions that require only user consent",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "delegated permissions that require admin consent\nPage 1383 of 1410\n1384 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "451",
    "question": "DRAG DROP\n-\nYou have an Azure Virtual Machine (VM) named VM1 running Windows Server\n2022 and an Azure Key Vault instance named kv1.\nYou are developing a .NET application named App1 that you plan to deploy to\nVM1.\nYou have the following requirements:\n• App1 will require access to kv1.\n• The identity used by App1 to access kv1 must be automatically deprovisioned\nwhen VM1 is deleted.\nYou need to identify the procedure that will meet the requirements.\nWhich three actions should you include in the procedure? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "452",
    "question": "HOTSPOT\n-\nYou have an Azure App Service web app named App1. App1 has Application\nInsights enabled.\nYou plan to review the configuration of telemetry sampling for Application\nInsights of App1.\nYou need to author an analytics query that will return the sampling rate.\nHow should you complete the provided query? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "453",
    "question": "HOTSPOT\n-\nYou have a web app named App1 hosted on you, on-premises web server.\nYou plan to use the Application Insights JavaScript SDK to implement client-side\nReal User Monitoring (RUM) of individual pages of App1.\nYou need to author the script element that will be added to each of the pages.\nWhat should you set for the value of src and cfg keys in the script element of\neach page? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "454",
    "question": "You develop an ASP. Net Core application by integrating the Application Insights\nSDK into your solution.\nThe application sends a very high rate of telemetry in a short time interval. You\nobserve a reduced number of events, traces, and metrics being recorded and\nincreased error rates for telemetry ingestion.\nYou need to reduce telemetry traffic, data costs, and storage costs while\npreserving a statistically correct analysis of application telemetry data. Your\nsolution must ensure that you will be able to correlate HTTP request and\nresponse data.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure a Log Analytics workspace data collection rule (DCR). Use a Kusto Query Language\n(KQL) statement to filter incoming data.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Disable adaptive sampling. Enable and configure the fixed-rate sampling module. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Set a daily cap on the Log Analytics workspace. Create an Activity log alert rule.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the TelemetryConfiguration object in the instrumented code. Increase the metric\naggregation interval to 15 minutes.\nPage 1390 of 1410\n1391 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "455",
    "question": "HOTSPOT\n-\nYou have an Azure Function app named App1 written in C# and an Application\nInsights workspace named Workspace1. App is implemented with Application\nInsights enabled and is configured to send its telemetry to Workspace1.\nYou observe that App1 telemetry collection regularly exceeds monthly quotas of\nWorkspace1.\nYou need to ensure that the telemetry volume remains within the monthly\nquotas of Workspace1.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "456",
    "question": "You have an Azure subscription that contains an Application Insights resource\nnamed AI1 and an Azure App Service web app named App1.\nYou create a Standard availability test in AI1. You set its URL to point to App1.\nYou need to ensure that any failed tests generate email notifications to the\nowners of the subscription.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create an action group.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an alert rule.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Enable the test.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable the alert.\nPage 1393 of 1410\n1394 Microsoft - AZ-204 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "457",
    "question": "You create an Azure subscription named Sub1. In Sub1, you create a custom\nAzure Event Grid topic named Topic1. Next, you create an Event Grid event\nsubscription named EventSub1. EventSub1 uses Topic1 as the event source and a\nWeb Hook as the endpoint.\nYou plan to enable dead-lettering in EventSub1.\nYou need to ensure that you can enable dead-lettering in EventSub1.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Configure delivery properties of EventSub1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an Azure Blob Storage container in Sub1.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create an Azure Storage queue in Sub1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the retry policy of EventSub1.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "458",
    "question": "HOTSPOT\n-\nYou are developing a microservices-based application by using Azure Container\nApps. The application consists of several containerized services that handle\ntasks, such as processing orders, managing inventory, and generating reports.\nProcessing orders has the following requirements:\n• Orders must be triggered by a web request.\n• Additional replicas must be added when the total number of web requests\nexceeds 300 for at least 15 seconds.\n• Processing orders must always be available based on incoming web requests.\n• Costs must be minimized for the Azure Container Apps instance.\nYou need to configure scaling.\nHow should you complete the CLI command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "459",
    "question": "DRAG DROP\n-\nYou create an Azure subscription. You then create an Azure resource group.\nYou plan to create a containerized task that will run as an Azure Container App\nin regular intervals.\nYou need to automate the process of creating the task in the resource group by\nusing Azure CLI commands.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "460",
    "question": "You are developing a microservices-based application that uses Azure Container\nApps. The application consists of several containerized services that handle\ntasks, such as processing orders, managing inventory, and generating reports.\nYou deploy a new revision of the processing orders app.\nProcessing orders must be triggered by a web request and must always be\navailable based on incoming web requests.\nYou need to validate that the replica is ready to handle incoming requests.\nWhat should you implement?",
    "options": [
      {
        "letter": "A",
        "text": "HTTP readiness probe",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "TCP readiness probe",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "HTTP startup probe",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "TCP liveness probe\nE) HTTP liveness probe",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "461",
    "question": "You are developing a microservices-based application that uses Azure Container\nApps. The application consists of several containerized services that handle\ntasks, such as processing orders, managing inventory, and generating reports.\nYou deploy two microservices named serviceA and serviceB to support\nmanaging inventory.\nYou have the following requirements:\n• serviceA and serviceB must publish events to a single Azure Event Hub by\nusing the Event Hubs SDK.\n• serviceA must publish 1,000 events per second.\n• serviceB must publish 3,000 events per second.\n• Costs must be minimized.\nYou need to support the publishing of events.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create four partitions. Update serviceA to use one partition and serviceB to use three\npartitions.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Enable and configure Azure Event Hubs Capture.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an Azure Event Hubs dedicated cluster. Configure the capacity units to one and the\nscaling units to two.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create and configure an Azure Schema Registry in Event Hubs. Update serviceA and serviceB\nto validate message schemas.\nE) Create four consumer groups. Update serviceA to use one consumer group and serviceB to\nuse three consumer groups.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "462",
    "question": "DRAG DROP\n-\nYou are developing a microservices-based application by using Azure Container\nApps. The application consists of several containerized services that handle\ntasks, such as processing orders, managing inventory, and generating reports.\nYou must deploy two microservices named serviceA and serviceB to support\nmanaging inventory. Deployment of the microservices have the following\nrequirements:\n• serviceA must create and configure all necessary Azure resources, including\nmultiple Dapr components and an Azure Blob Storage account, by using Bicep\nfiles.\n• serviceB must create and configure all necessary Azure resources without\nusing Bicep files.\n• All microservices must use the same resource group, environment, Azure\nContainer Registry, and Log Analytics workspace.\nYou need to deploy the microservices.\nWhich CLI command should you use? To answer, move the appropriate CLI\ncommands to the correct microservices. You may use each CLI command once,\nmore than once, or not at all. You may need to move the split bar between\npanes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "463",
    "question": "You are developing an Azure Function app for scalability and integration with\nAzure Blob Storage.\nYou must run the code in an Azure production environment that allows the\nfunction to scale based on demand, providing instance size selection and\nhigher concurrency control.\nThe function must connect to other Azure services secured inside a virtual\nnetwork, scale to zero instances when there are no incoming events and\nminimize costs.\nYou need to select a hosting plan to meet the requirement.\nWhich plan should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Flex Consumption",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Consumption",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Premium",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Dedicated",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "464",
    "question": "You have 100 Azure virtual machines (VMs) with the system-assigned managed\nidentity enabled.\nYou need to identify the value of the object ID attribute for each of the\nidentities.\nWhich command should you use?",
    "options": [
      {
        "letter": "A",
        "text": "az ad signed-in-user list-owned-objects",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "az ad sp credential list",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "az ad user show",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Get-AzResource",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "465",
    "question": "You have a workspace-based Azure Application Insights resource named\nInsights1 and an Azure App Service Web App named App1. Insights1 collects\ntelemetry generated by App1.\nYou plan to evaluate the alerting functionality of the availability testing that is\nenabled for App1 by taking it offline for 50 minutes.\nYou create a standard availability test for App1, set its frequency to 15 minutes,\nand set its alert status to Enabled.\nYou need to assess the number of alerts that you should expect by taking App1\noffline for 50 minutes.\nHow many alerts should you expect?",
    "options": [
      {
        "letter": "A",
        "text": "1",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "2",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "3",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "4",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "466",
    "question": "HOTSPOT\n-\nYou have an Azure Application Insights resource named AI1. AI1 monitors an\nAzure App Service web app named App1.\nYou plan to regularly analyze the usage of specific pages of App1 by a subset of\nusers. The subset will consist of users who access specific App1 pages five or\nmore times in a given month. You must be able to filter sessions and events\nbased on that subset when viewing AI1 in the Azure portal.\nYou need to configure AI1 to facilitate your analysis.\nWhat should you configure for AI1? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "467",
    "question": "HOTSPOT\n-\nYou have an on-premises, internal-only website named www-contoso.com and\nan Azure Application Insights instance named AppInsights1.\nYou need to implement availability testing of www.contoso.com by using\nAppInsights1. Your solution must minimize development effort.\nHow should you configure the test? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "468",
    "question": "You are a developer for a company that recently transitioned to using\nworkspace-based Application Insights for a C# .NET web application that runs\non Azure.\nThe application has intermittent performance issues.\nYou need to use the AI-powered code analysis feature in Application Insights to\nhelp diagnose the problem.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable and configure smart detection.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable and configure availability tests.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use the Application Map to visualize dependencies and interactions, then trace the\nperformance issues through the map.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable the profiler and snapshot debugger. \nE) Configure continuous export of telemetry data to Azure Storage.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "469",
    "question": "You have an Azure Service Bus namespace with a partitioned queue named\nqueue1.\nYou plan to send a large number of messages through queue1 over the next few\nweeks. The order of messages will be random. You must minimize the possibility\nof message transmission interruption by transient failures of individual\npartitions.\nYou need to use the optimal configuration of the partition key in the messages.\nWhich configuration should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Set the partition key of messages to the message ID value.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable sessions. Set the partition key of messages to the session ID value.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Enable sessions. Ensure that the partition key is different from the session ID value.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Leave the partition key value as null.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  }
]