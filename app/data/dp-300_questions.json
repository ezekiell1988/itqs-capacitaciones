[
  {
    "number": "1",
    "question": "You have 20 Azure SQL databases provisioned by using the vCore purchasing\nmodel.\nYou plan to create an Azure SQL Database elastic pool and add the 20\ndatabases.\nWhich three metrics should you use to size the elastic pool to meet the\ndemands of your workload? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "total size of all the databases",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "geo-replication support",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "number of concurrently peaking databases * peak CPU utilization per database (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "maximum number of concurrent sessions for all the databases\nE) total number of databases * average CPU utilization per database",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "A: Estimate the storage space needed for the pool by adding the number of bytes needed\nfor all the databases in the pool.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview\nPage 15 of 818\n16 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "CE\nCE: Estimate the vCores needed for the pool as follows:\nFor vCore-based purchasing model: MAX(<Total number of DBs X average vCore utilization\nper DB>, <Number of concurrently peaking DBs X Peak vCore utilization per DB)\nA: Estimate the storage space needed for the pool by adding the number of bytes needed\nfor all the databases in the pool.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"
  },
  {
    "number": "2",
    "question": "DRAG DROP -\nYou have SQL Server 2019 on an Azure virtual machine that contains an SSISDB\ndatabase.\nA recent failure causes the master database to be lost.\nYou discover that all Microsoft SQL Server integration Services (SSIS) packages\nfail to run on the virtual machine.\nWhich four actions should you perform in sequence to resolve the issue? To\nanswer, move the appropriate actions from the list of actions to the answer\narea and arrange them in the correct.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Attach the SSISDB database\nStep 2: Turn on the TRUSTWORTHY property and the CLR property\nIf you are restoring the SSISDB database to an SQL Server instance where the SSISDB\ncatalog was never created, enable common language runtime (clr)\nStep 3: Open the master key for the SSISDB database\nRestore the master key by this method if you have the original password that was used\nto create SSISDB. open master key decryption by password = 'LS1Setup!' --'Password used\nwhen creating SSISDB'\nAlter Master Key Add encryption by Service Master Key\nStep 4: Encrypt a copy of the master key by using the service master key\nReference:\nhttps://docs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog\nCommunity Discussion\nThe correct url iss: https://docs.microsoft.com/en-us/sql/integration-services/catalog/\nssis-catalog?view=sql-server-ver15#to-restore-the-ssis-database\n1. Attach the SSISDB database (E) 2. Open the master key for the SSISDB database (F) 3.\nEncrypt a copy of the master key by using the service master key © 4. Turn on the\nTRUSTWORTHY property and the CLR property (D) turning on the TRUSTWORTHY property\nand the CLR property (D) should be done after opening the master key for the SSISDB\ndatabase (F) and encrypting a copy of the master key by using the service master key ©.\nThis is because the TRUSTWORTHY property and CLR property are related to security and\nencryption, so it’s important to ensure that the master key is properly encrypted before\nenabling these properties.\n\n\nOdd question.\nThis is such a weird question, it doesn't address the problem at hand which is a failure\ncausing the master table being deleted. Why would that even happen? I would want to\nfigure that out\nAttaching the database and enabling CLR can be done at any order. In this MS\ndocumentation, they first enable CLR, then attach/restore the database: https://\ndocs.microsoft.com/en-us/sql/integration-services/catalog/ssis-catalog?view=sql-\nserver-ver15",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "3",
    "question": "You have an Azure SQL database that contains a table named factSales.\nFactSales contains the columns shown in the following table.\nFactSales has 6 billion rows and is loaded nightly by using a batch process. You\nmust provide the greatest reduction in space for the database and maximize\nperformance.\nWhich type of compression provides the greatest space reduction for the\ndatabase?",
    "options": [
      {
        "letter": "A",
        "text": "page compression",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "row compression",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "columnstore compression",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "columnstore archival compression",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Columnstore tables and indexes are always stored with columnstore compression. You"
  },
  {
    "number": "4",
    "question": "You have a Microsoft SQL Server 2019 database named DB1 that uses the\nfollowing database-level and instance-level features.\n✑ Clustered columnstore indexes\n✑ Automatic tuning\n✑ Change tracking\n✑ PolyBase\nYou plan to migrate DB1 to an Azure SQL database.\nWhat feature should be removed or replaced before DB1 can be migrated?",
    "options": [
      {
        "letter": "A",
        "text": "Clustered columnstore indexes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "PolyBase",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Change tracking",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Automatic tuning",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "This table lists the key features for PolyBase and the products in which they're available."
  },
  {
    "number": "5",
    "question": "You have a Microsoft SQL Server 2019 instance in an on-premises datacenter.\nThe instance contains a 4-TB database named DB1.\nYou plan to migrate DB1 to an Azure SQL Database managed instance.\nWhat should you use to minimize downtime and data loss during the migration?",
    "options": [
      {
        "letter": "A",
        "text": "distributed availability groups",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "database mirroring",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Always On Availability Group",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Database Migration Service",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Azure Database Migration Service can do online migrations with minimal downtime.\nReference:\nhttps://docs.microsoft.com/en-us/azure/dms/dms-overview"
  },
  {
    "number": "6",
    "question": "HOTSPOT -\nYou have an on-premises Microsoft SQL Server 2016 server named Server1 that\ncontains a database named DB1.\nYou need to perform an online migration of DB1 to an Azure SQL Database\nmanaged instance by using Azure Database Migration Service.\nHow should you configure the backup of DB1? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Full and log backups only\nMake sure to take every backup on a separate backup media (backup files). Azure\nDatabase Migration Service doesn't support backups that are appended to a single\nbackup file. Take full backup and log backups to separate backup files.\nBox 2: WITH CHECKSUM -\nAzure Database Migration Service uses the backup and restore method to migrate your\non-premises databases to SQL Managed Instance. Azure Database\nMigration Service only supports backups created using checksum.\nIncorrect Answers:\nNOINIT Indicates that the backup set is appended to the specified media set, preserving\nexisting backup sets. If a media password is defined for the media set, the password\nmust be supplied. NOINIT is the default.\nUNLOAD -\nSpecifies that the tape is automatically rewound and unloaded when the backup is\nfinished. UNLOAD is the default when a session begins.\nReference:\nhttps://docs.microsoft.com/en-us/azure/dms/known-issues-azure-sql-db-managed-\ninstance-online\nCommunity Discussion\nTHE ANSWER IS 100% CORRECT @KIET2131 IS WRONG\n\n\nAccording to the documentation \"Take full, differential and log backups to separate\nbackup files.\" as part of the requirements.\nBackup type: Full backup only Backup option: WITH CHECKSUM\nI cannot see any option for taking backup in DMS. It's all about setting up project, setting\nup source and destination and finally start cutover and update the connection string.\nWhere do you see the taking backups?\nHow can you make a full backup without logs, which are included automatically? The\nquestion is incorrect",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "7",
    "question": "DRAG DROP -\nYou have a resource group named App1Dev that contains an Azure SQL Database\nserver named DevServer1. DevServer1 contains an Azure SQL database named\nDB1. The schema and permissions for DB1 are saved in a Microsoft SQL Server\nData Tools (SSDT) database project.\nYou need to populate a new resource group named App1Test with the DB1\ndatabase and an Azure SQL Server named TestServer1. The resources in\nApp1Test must have the same configurations as the resources in App1Dev.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nthe answer is corrected\nRe-Write answer here FYI. 1. From the Azure Portal, export the Azure Resource Manager\ntemplates. 2. Change the server name and related variables in the templates. 3. From the\nAzure portal, deploy the templates. 4. From the database project, deply the database\nschema and permissions.\nThank you\nThank you\nthe answer is corrected",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "8",
    "question": "HOTSPOT -\nYou have an Azure Synapse Analytics dedicated SQL pool named Pool1 and an\nAzure Data Lake Storage Gen2 account named Account1.\nYou plan to access the files in Account1 by using an external table.\nYou need to create a data source in Pool1 that you can reference when you\ncreate the external table.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: dfs -\nFor Azure Data Lake Store Gen 2 used the following syntax:\nhttp[s] <storage_account>.dfs.core.windows.net/<container>/subfolders\nIncorrect:\nNot blob: blob is used for Azure Blob Storage. Syntax:\nhttp[s] <storage_account>.blob.core.windows.net/<container>/subfolders\nBox 2: TYPE = HADOOP -\nSyntax for CREATE EXTERNAL DATA SOURCE.\nExternal data sources with TYPE=HADOOP are available only in dedicated SQL pools.\nCREATE EXTERNAL DATA SOURCE <data_source_name>\nWITH -\n( LOCATION = '<prefix>://<path>'\n[, CREDENTIAL = <database scoped credential> ]\n, TYPE = HADOOP\n)\n[;]\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-\ntables\nCommunity Discussion\ndfs hadoop\n\n\nQuestion FOR DP-203 , Not For DBA (DP-300) And the answer is wrong\ndatalake gen2 is dfs. https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/\ndevelop-tables-external-tables?tabs=hadoop#create-external-data-source\ndatalake gen2 is dfs. https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/\ndevelop-tables-external-tables?tabs=hadoop#create-external-data-source\nThis question is related to the DP-203 exam.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "9",
    "question": "HOTSPOT -\nYou plan to develop a dataset named Purchases by using Azure Databricks.\nPurchases will contain the following columns:\n✑ ProductID\n✑ ItemPrice\n✑ LineTotal\n✑ Quantity\n✑ StoreID\n✑ Minute\n✑ Month\n✑ Hour\n✑ Year\n✑ Day\nYou need to store the data to support hourly incremental load pipelines that\nwill vary for each StoreID. The solution must minimize storage costs.\nHow should you complete the code? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: .partitionBy -\nExample:\ndf.write.partitionBy(\"y\",\"m\",\"d\")\n.mode(SaveMode.Append)\n.parquet(\"/data/hive/warehouse/db_name.db/\" + tableName)\nBox 2: (\"Year\",\"Month\",\"Day\",\"Hour\",\"StoreID\")\nBox 3: .parquet(\"/Purchases\")\nReference:\nhttps://intellipaat.com/community/11744/how-to-partition-and-write-dataframe-in-\nspark-without-deleting-partitions-with-no-new-data\nCommunity Discussion\nQuestion FOR DP-203 , Not For DBA (DP-300)\n.partitionBy (\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\") or (\"StoreID\", \"Hour\") .parquet(\"/\nPurchases\") // The problem is that (\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\") and\n(\"Year\",\"Month\",\"Day\",\"Hour\", \"StoreID\") are basically the same // (\"StoreID\", \"Hour\") or even\nbetter (\"StoreID\") (not on the list) are also good. The problem is that you would have to\nkeep offset of the last read // I would choose (\"StoreID\", \"Year\",\"Month\",\"Day\",\"Hour\")\nbecause it is the cleanest\n\n\nAnswer should be (\"StoreID\",\"Year\",\"Month\",\"Day\",\"Hour\") and this indeed a question from\nDP-203 (recently passed this one)\nAnswer should be (\"StoreID\",\"Year\",\"Month\",\"Day\",\"Hour\") and this indeed a question from\nDP-203 (recently passed this one)\nAnswer should be (\"StoreID\",\"Year\",\"Month\",\"Day\",\"Hour\") and this indeed a question from\nDP-203 (recently passed this one)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "10",
    "question": "You are designing a streaming data solution that will ingest variable volumes of\ndata.\nYou need to ensure that you can change the partition count after creation.\nWhich service should you use to ingest the data?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Event Hubs Standard",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Stream Analytics",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Data Factory",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Event Hubs Dedicated",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "The partition count for an event hub in a dedicated Event Hubs cluster can be increased\nafter the event hub has been created.\nIncorrect Answers:\nA: For Azure Event standard hubs, the partition count isn't changeable, so you should\nconsider long-term scale when setting partition count.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions"
  },
  {
    "number": "11",
    "question": "HOTSPOT -\nYou are building a database in an Azure Synapse Analytics serverless SQL pool.\nYou have data stored in Parquet files in an Azure Data Lake Storage Gen2\ncontainer.\nRecords are structured as shown in the following sample.\nThe records contain two applicants at most.\nYou need to build a table that includes only the address fields.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: CREATE EXTERNAL TABLE -\nAn external table points to data located in Hadoop, Azure Storage blob, or Azure Data\nLake Storage. External tables are used to read data from files or write data to files in\nAzure Storage. With Synapse SQL, you can use external tables to read external data using\ndedicated SQL pool or serverless SQL pool.\nSyntax:\nCREATE EXTERNAL TABLE { database_name.schema_name.table_name |\nschema_name.table_name | table_name }\n( <column_definition> [ ,...n ] )\nWITH (\nLOCATION = 'folder_or_filepath',\nDATA_SOURCE = external_data_source_name,\nFILE_FORMAT = external_file_format_name\nBox 2. OPENROWSET -\nWhen using serverless SQL pool, CETAS is used to create an external table and export\nquery results to Azure Storage Blob or Azure Data Lake Storage Gen2.\nExample:\nAS -\n\n\nSELECT decennialTime, stateName, SUM(population) AS population\nFROM -\nOPENROWSET(BULK 'https://azureopendatastorage.blob.core.windows.net/\ncensusdatacontainer/release/us_population_county/year=*/*.parquet',\nFORMAT='PARQUET') AS [r]\nGROUP BY decennialTime, stateName\nGO -\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-\ntables\nCommunity Discussion\nQuestion FOR DP-203 , Not For DBA (DP-300)\nThe DP-300 exam engine has been updated and some questions also cover Azure Data\nengineering side. You don't have to repeat the same statement for each question.\nThe DP-300 exam engine has been updated and some questions also cover Azure Data\nengineering side. You don't have to repeat the same statement for each question.\nI don't seen any mention of Synapse Analytics, Databricks InsightsHD or Azure Data\nFactory in the Skills Outline on the DP-300 even with the incoming update to the\nquestions Nov 24th 2021. Where have you got this information from?\nI don't seen any mention of Synapse Analytics, Databricks InsightsHD or Azure Data\nFactory in the Skills Outline on the DP-300 even with the incoming update to the\nquestions Nov 24th 2021. Where have you got this information from?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "12",
    "question": "You have an Azure Synapse Analytics Apache Spark pool named Pool1.\nYou plan to load JSON files from an Azure Data Lake Storage Gen2 container into\nthe tables in Pool1. The structure and data types vary by file.\nYou need to load the files into the tables. The solution must maintain the\nsource data types.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Load the data by using PySpark.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Load the data by using the OPENROWSET Transact-SQL command in an Azure Synapse\nAnalytics serverless SQL pool.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use a Get Metadata activity in Azure Data Factory.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use a Conditional Split transformation in an Azure Synapse data flow.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Synapse notebooks support four Apache Spark languages:\nPySpark (Python)\nSpark (Scala)\nSpark SQL -\n.NET Spark (C#)\nNote: Bring data to a notebook.\nYou can load data from Azure Blob Storage, Azure Data Lake Store Gen 2, and SQL pool as\nshown in the code samples below.\nRead a CSV from Azure Data Lake Store Gen2 as a Spark DataFrame. from pyspark.sql\nimport SparkSession from pyspark.sql.types import * account_name = \"Your account\nname\" container_name = \"Your container name\" relative_path = \"Your path\" adls_path =\n'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name,\nrelative_path) df1 = spark.read.option('header', 'true') \\\n.option('delimiter', ',') \\\n.csv(adls_path + '/Testfile.csv')\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-\ndevelopment-using-notebooks"
  },
  {
    "number": "13",
    "question": "You are designing a date dimension table in an Azure Synapse Analytics\ndedicated SQL pool. The date dimension table will be used by all the fact\ntables.\nWhich distribution type should you recommend to minimize data movement?",
    "options": [
      {
        "letter": "A",
        "text": "HASH",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "REPLICATE",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "ROUND_ROBIN",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "A replicated table has a full copy of the table available on every Compute node. Queries\nrun fast on replicated tables since joins on replicated tables don't require data\nmovement. Replication requires extra storage, though, and isn't practical for large tables.\nIncorrect Answers:\nC: A round-robin distributed table distributes table rows evenly across all distributions.\nThe assignment of rows to distributions is random. Unlike hash-distributed tables, rows\nwith equal values are not guaranteed to be assigned to the same distribution.\nAs a result, the system sometimes needs to invoke a data movement operation to better\norganize your data before it can resolve a query."
  },
  {
    "number": "14",
    "question": "HOTSPOT -\nFrom a website analytics system, you receive data extracts about user\ninteractions such as downloads, link clicks, form submissions, and video plays.\nThe data contains the following columns:\nYou need to design a star schema to support analytical queries of the data. The\nstar schema will contain four tables including a date dimension.\nTo which table should you add each column? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: DimEvent -\nBox 2: DimChannel -\nDimension tables describe business entities ג€\" the things you model. Entities can\ninclude products, people, places, and concepts including time itself. The most consistent\ntable you'll find in a star schema is a date dimension table. A dimension table contains a\nkey column (or columns) that acts as a unique identifier, and descriptive columns.\nBox 3: FactEvents -\nFact tables store observations or events, and can be sales orders, stock balances,\nexchange rates, temperatures, etc.\nReference:\nhttps://docs.microsoft.com/en-us/power-bi/guidance/star-schema\nCommunity Discussion\n\n\nDimension tables support filtering and grouping, Fact tables support summarization, so\nthe correct answer is: DimEvent, DimChannel, FactEvents.\nAnswer is incorrect. The correct answer is: DimEvent, DimChannel, FactEvents\nThis is a DP-203 question.\nDimEvent DimChannel FactEvents\nFrom my opinion 1 and 3 are opposite - Fact is number of events and Event category is\nDimEvent.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "15",
    "question": "DRAG DROP -\nYou plan to create a table in an Azure Synapse Analytics dedicated SQL pool.\nData in the table will be retained for five years. Once a year, data that is older\nthan five years will be deleted.\nYou need to ensure that the data is distributed evenly across partitions. The\nsolutions must minimize the amount of time required to delete old data.\nHow should you complete the Transact-SQL statement? To answer, drag the\nappropriate values to the correct targets. Each value may be used once, more\nthan once, or not at all.\nYou may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: HASH -\nBox 2: OrderDateKey -\nIn most cases, table partitions are created on a date column.\nA way to eliminate rollbacks is to use Metadata Only operations like partition switching\nfor data management. For example, rather than execute a DELETE statement to delete all\nrows in a table where the order_date was in October of 2001, you could partition your\ndata early. Then you can switch out the partition with data for an empty partition from\nanother table.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-\nwarehouse https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-\npractices-dedicated-sql-pool\nCommunity Discussion\nThe provided answer is correct.\nThis is a DP-203 question.\n(distribution_column_name) is only available in HASH https://docs.microsoft.com/en-us/\nsql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7\nIs this a DP-300 question?\nI agree. Here is an example from MS: CREATE TABLE myTable ( l_orderkey bigint, l_partkey\nbigint, l_suppkey bigint, l_shipinstruct char(25), l_shipmode char(10), l_comment\n\n\nvarchar(44)) WITH ( DISTRIBUTION = HASH (l_orderkey), CLUSTERED COLUMNSTORE INDEX,\nPARTITION ( l_shipdate RANGE RIGHT FOR VALUES (\n'1992-01-01','1992-02-01','1992-03-01','1992-04-01','1992-05-01',\n'1992-06-01','1992-07-01','1992-08-01','1992-09-01','1992-10-01',\n'1992-11-01','1992-12-01','1993-01-01','1993-02-01','1993-03-01' )) );",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "16",
    "question": "You have an Azure Synapse Analytics workspace named WS1 that contains an\nApache Spark pool named Pool1.\nYou plan to create a database named DB1 in Pool1.\nYou need to ensure that when tables are created in DB1, the tables are available\nautomatically as external tables to the built-in serverless SQL pool.\nWhich format should you use for the tables in DB1?",
    "options": [
      {
        "letter": "A",
        "text": "JSON",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "CSV",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Parquet",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "ORC",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Serverless SQL pool can automatically synchronize metadata from Apache Spark. A\nserverless SQL pool database will be created for each database existing in serverless\nApache Spark pools.\nFor each Spark external table based on Parquet and located in Azure Storage, an external\ntable is created in a serverless SQL pool database. As such, you can shut down your\nSpark pools and still query Spark external tables from serverless SQL pool.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-\nspark-tables"
  },
  {
    "number": "17",
    "question": "You are designing an anomaly detection solution for streaming data from an\nAzure IoT hub. The solution must meet the following requirements:\n✑ Send the output to an Azure Synapse.\n✑ Identify spikes and dips in time series data.\n✑ Minimize development and configuration effort.\nWhich should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Databricks",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Stream Analytics",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "Anomalies can be identified by routing data via IoT Hub to a built-in ML model in Azure\nStream Analytics\nReference:\nhttps://docs.microsoft.com/en-us/learn/modules/data-anomaly-detection-using-azure-\niot-hub/ https://docs.microsoft.com/en-us/azure/stream-analytics/azure-synapse-\nanalytics-output"
  },
  {
    "number": "18",
    "question": "You are creating a new notebook in Azure Databricks that will support R as the\nprimary language but will also support Scala and SQL.\nWhich switch should you use to switch between languages?",
    "options": [
      {
        "letter": "A",
        "text": "\\\\[<language>]",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "%<language>",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "\\\\[<language>]",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "@<language>",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "You can override the default language by specifying the language magic command\n%<language> at the beginning of a cell. The supported magic commands are:\n%python, %r, %scala, and %sql.\nReference:\nhttps://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-use"
  },
  {
    "number": "19",
    "question": "DRAG DROP -\nYou are creating a managed data warehouse solution on Microsoft Azure.\nYou must use PolyBase to retrieve data from Azure Blob storage that resides in\nparquet format and load the data into a large table called\nFactSalesOrderDetails.\nYou need to configure Azure Synapse Analytics to receive the data.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nTo query the data in your Hadoop data source, you must define an external table to use\nin Transact-SQL queries. The following steps describe how to configure the external table.\nStep 1: Create a master key on database.\n1. Create a master key on the database. The master key is required to encrypt the\ncredential secret.\n(Create a database scoped credential for Azure blob storage.)\nStep 2: Create an external data source for Azure Blob storage.\n2. Create an external data source with CREATE EXTERNAL DATA SOURCE..\nStep 3: Create an external file format to map the parquet files.\n3. Create an external file format with CREATE EXTERNAL FILE FORMAT.\nStep 4. Create an external table FactSalesOrderDetails\n4. Create an external table pointing to data stored in Azure storage with CREATE\nEXTERNAL TABLE.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-\nconfigure-azure-blob-storage\nCommunity Discussion\nQuestion not for DP-300 exam.\n\n\nanswer is correct. below video illustrate that: https://youtu.be/-DjOsJsIlA4?t=1266\nWhy dont you find out for yourself, and let us know.\nits correct.\nWhy dont you find out for yourself, and let us know.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "20",
    "question": "HOTSPOT -\nYou configure version control for an Azure Data Factory instance as shown in the\nfollowing exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: adf_publish -\nBy default, data factory generates the Resource Manager templates of the published\nfactory and saves them into a branch called adf_publish. To configure a custom publish\nbranch, add a publish_config.json file to the root folder in the collaboration branch.\nWhen publishing, ADF reads this file, looks for the field publishBranch, and saves all\nResource Manager templates to the specified location. If the branch doesn't exist, data\nfactory will automatically create it. And example of what this file looks like is below:\n{\n\"publishBranch\": \"factory/adf_publish\"\n}\nBox 2: /dwh_barchlet/ adf_publish/contososales\nRepositoryName: Your Azure Repos code repository name. Azure Repos projects contain\nGit repositories to manage your source code as your project grows. You can create a new\nrepository or use an existing repository that's already in your project.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/source-control\nCommunity Discussion\nYea it is, I checked the reference!\nYea it is, I checked the reference!\n\n\nAzure Data Factory is out of scope of the DP-300 exam.\nThis is a DP-203 question.\nIs it correct?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "21",
    "question": "You plan to build a structured streaming solution in Azure Databricks. The\nsolution will count new events in five-minute intervals and report only events\nthat arrive during the interval.\nThe output will be sent to a Delta Lake table.\nWhich output mode should you use?",
    "options": [
      {
        "letter": "A",
        "text": "complete",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "append",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "update",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Complete mode: You can use Structured Streaming to replace the entire table with every\nbatch.\nIncorrect Answers:\nB: By default, streams run in append mode, which adds new records to the table.\nReference:\nhttps://docs.databricks.com/delta/delta-streaming.html"
  },
  {
    "number": "22",
    "question": "HOTSPOT -\nYou are performing exploratory analysis of bus fare data in an Azure Data Lake\nStorage Gen2 account by using an Azure Synapse Analytics serverless SQL pool.\nYou execute the Transact-SQL query shown in the following exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: CSV files that have file named beginning with \"tripdata_2020\"\nBox 2: a header -\nFIRSTROW = 'first_row'\nSpecifies the number of the first row to load. The default is 1 and indicates the first row\nin the specified data file. The row numbers are determined by counting the row\nterminators. FIRSTROW is 1-based.\nExample: Option firstrow is used to skip the first row in the CSV file that represents\nheader in this case (firstrow=2). select top 10 * from openrowset( bulk 'https://\npandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/\necdc_cases.csv', format = 'csv', parser_version = '2.0', firstrow = 2 ) as rows\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file\nCommunity Discussion\nThis is a DP-203 question.\nThe provided answer is correct.\nthe query stated to start from 2nd row, so the first row should be header (and therefore\ndiscarded) and 2nd the data - just my opinion\nthe query stated to start from 2nd row, so the first row should be header (and therefore\ndiscarded) and 2nd the data - just my opinion\nExam DP-203: Data Engineering on Microsoft Azure",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "23",
    "question": "You have a SQL pool in Azure Synapse that contains a table named\ndbo.Customers. The table contains a column name Email.\nYou need to prevent nonadministrative users from seeing the full email\naddresses in the Email column. The users must see values in a format of\n[email protected] instead.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "From the Azure portal, set a mask on the Email column.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From the Azure portal, set a sensitivity classification of Confidential for the Email column.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "From Microsoft SQL Server Management Studio, set an email mask on the Email column.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "From Microsoft SQL Server Management Studio, grant the SELECT permission to the users for\nall the columns in the dbo.Customers table except Email.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The Email masking method, which exposes the first letter and replaces the domain with\nXXX.com using a constant string prefix in the form of an email address.\nExample: [email protected] -\nIncorrect:\nNot A: The Email mask feature cannot be set using portal for Azure Synapse (use\nPowerShell or REST API) or SQL Managed Instance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-\noverview"
  },
  {
    "number": "24",
    "question": "You have an Azure Databricks workspace named workspace1 in the Standard\npricing tier. Workspace1 contains an all-purpose cluster named cluster1.\nYou need to reduce the time it takes for cluster1 to start and scale up. The\nsolution must minimize costs.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Upgrade workspace1 to the Premium pricing tier.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure a global init script for workspace1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a pool in workspace1.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create a cluster policy in workspace1.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "You can use Databricks Pools to Speed up your Data Pipelines and Scale Clusters Quickly.\nDatabricks Pools, a managed cache of virtual machine instances that enables clusters to\nstart and scale 4 times faster.\nReference:\nhttps://databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html"
  },
  {
    "number": "25",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table\nnamed Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage\nGen2 container named container1.\nYou plan to insert data from the files into Table1 and transform the data. Each\nrow of data in the files will produce one row in the serving layer of Table1.\nYou need to ensure that when the source data files are loaded to container1,\nthe DateTime is stored as an additional column in Table1.\nSolution: In an Azure Synapse Analytics pipeline, you use a Get Metadata\nactivity that retrieves the DateTime of the files.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "26",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table\nnamed Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage\nGen2 container named container1.\nYou plan to insert data from the files into Table1 and transform the data. Each\nrow of data in the files will produce one row in the serving layer of Table1.\nYou need to ensure that when the source data files are loaded to container1,\nthe DateTime is stored as an additional column in Table1.\nSolution: You use an Azure Synapse Analytics serverless SQL pool to create an\nexternal table that has an additional DateTime column.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "This is not about an external table.\nInstead, in an Azure Synapse Analytics pipeline, you use a Get Metadata activity that\nretrieves the DateTime of the files.\nNote: You can use the Get Metadata activity to retrieve the metadata of any data in Azure\nData Factory or a Synapse pipeline. You can use the output from the\nGet Metadata activity in conditional expressions to perform validation, or consume the\nmetadata in subsequent activities.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"
  },
  {
    "number": "27",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Synapse Analytics dedicated SQL pool that contains a table\nnamed Table1.\nYou have files that are ingested and loaded into an Azure Data Lake Storage\nGen2 container named container1.\nYou plan to insert data from the files into Table1 and transform the data. Each\nrow of data in the files will produce one row in the serving layer of Table1.\nYou need to ensure that when the source data files are loaded to container1,\nthe DateTime is stored as an additional column in Table1.\nSolution: You use a dedicated SQL pool to create an external table that has an\nadditional DateTime column.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No \nPage 66 of 818\n67 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead, in an Azure Synapse Analytics pipeline, you use a Get Metadata activity that\nretrieves the DateTime of the files.\nNote: You can use the Get Metadata activity to retrieve the metadata of any data in Azure\nData Factory or a Synapse pipeline. You can use the output from the\nGet Metadata activity in conditional expressions to perform validation, or consume the\nmetadata in subsequent activities.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"
  },
  {
    "number": "28",
    "question": "HOTSPOT -\nYou are provisioning an Azure SQL database in the Azure portal as shown in the\nfollowing exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: no extra time -\nAuto Pause is not checked in the exhibit.\nNote: If Auto Pause is checked the correct answer is: up to one minute\nBox 2: intermittent and unpredictable\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview\nCommunity Discussion\nShould be No Extra Time, screenshot shows Enable Auto Pause is unchecked.. so there is\nno pausing at all. On idle, server will run at min vcore setting. Second option is\nintermittent load as indicated by max and min vcore settings which is only available on\nserverless compute option.\nJust done exam and auto pause delay was checked\nAuto-pause is not enabled in this case: The latency to auto-resume and auto-pause a\nserverless database is generally order of 1 minute to auto-resume and 1-10 minutes after\nthe expiration of the delay period to auto-pause.\nPls ignore. Should be No Extra Time.\nPls ignore. Should be No Extra Time.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "29",
    "question": "You plan to deploy an app that includes an Azure SQL database and an Azure\nweb app. The app has the following requirements:\n✑ The web app must be hosted on an Azure virtual network.\n✑ The Azure SQL database must be assigned a private IP address.\n✑ The Azure SQL database must allow connections only from a specific virtual\nnetwork.\nYou need to recommend a solution that meets the requirements.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Private Link",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a network security group (NSG)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a database-level firewall",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a server-level firewall",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Private Link allows you to connect to various PaaS services in Azure via a private\nendpoint.\nA private endpoint is a private IP address within a specific VNet and subnet.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/private-endpoint-overview"
  },
  {
    "number": "30",
    "question": "You are planning a solution that will use Azure SQL Database. Usage of the\nsolution will peak from October 1 to January 1 each year.\nDuring peak usage, the database will require the following:\n✑ 24 cores\n✑ 500 GB of storage\n✑ 124 GB of memory\n✑ More than 50,000 IOPS\nDuring periods of off-peak usage, the service tier of Azure SQL Database will be\nset to Standard.\nWhich service tier should you use during peak usage?",
    "options": [
      {
        "letter": "A",
        "text": "Business Critical",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Premium",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Hyperscale",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/resource-limits-vcore-\nsingle-databases#business-critical---provisioned-compute---gen4"
  },
  {
    "number": "31",
    "question": "HOTSPOT -\nYou have an Azure subscription.\nYou need to deploy an Azure SQL resource that will support cross database\nqueries by using an Azure Resource Manager (ARM) template.\nHow should you complete the ARM template? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Microsoft.Sql/managedInstances\nThe Managed Instance depends on the Virtual Network.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/create-template-\nquickstart?tabs=azure-powershell\nCommunity Discussion\nbecause virtual network has to be deployed before\nbecause virtual network has to be deployed before\nI retract the statement and have a correction : Box 2: parameters('virtualNetworkName')\n\"dependsOn\": [ \"[resourceId('Microsoft.Network/virtualNetworks',\nparameters('virtualNetworkName'))]\" The key is parameters vs variables ...\nI retract the statement and have a correction : Box 2: parameters('virtualNetworkName')\n\"dependsOn\": [ \"[resourceId('Microsoft.Network/virtualNetworks',\nparameters('virtualNetworkName'))]\" The key is parameters vs variables ...\nThe answer is correct. { \"type\": \"Microsoft.Sql/managedInstances\", \"apiVersion\":\n\"2020-02-02-preview\", \"name\": \"[parameters('managedInstanceName')]\", \"location\":\n\n\n\"[parameters('location')]\", \"dependsOn\": [ \"[resourceId('Microsoft.Network/\nvirtualNetworks', parameters('virtualNetworkName'))]\" ] }",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "32",
    "question": "HOTSPOT -\nYou have the following Azure Resource Manager template.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/purchasing-models https://\ndocs.microsoft.com/en-us/azure/azure-sql/database/single-database-create-arm-\ntemplate-quickstart\nCommunity Discussion\nAnswer is correct\nThe SKU type of Standard is only available in DTU pricing model\n\n\nthe value of \"capacity\" = 10\nthe value of \"capacity\" = 10\nall answers are correct. The first one is tricky which looks like correct but it is not. The\nkeyword is \"serverless\" which is not available in DTU purchasing option.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "33",
    "question": "HOTSPOT -\nYou have an on-premises Microsoft SQL Server 2019 instance that hosts a\ndatabase named DB1.\nYou plan to perform an online migration of DB1 to an Azure SQL managed\ninstance by using the Azure Database Migration Service.\nYou need to create a backup of DB1 that is accessible to the Azure Database\nMigration Service.\nWhat should you run for the backup and where should you store the backup? To\nanswer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: ..with CHECKSUM option -\nAzure Database Migration Service does not initiate any backups, and instead uses\nexisting backups, which you may already have as part of your disaster recovery plan, for\nthe migration. Be sure that you take backups using the WITH CHECKSUM option.\nBox 2: An SMB share -\nFor online migrations from SQL Server to SQL Managed Instance using Azure Database\nMigration Service, you must provide the full database backup and subsequent log\nbackups in the SMB network share that the service can use to migrate your databases.\nReference:\nhttps://docs.microsoft.com/en-us/azure/dms/tutorial-sql-server-managed-instance-\nonline\nCommunity Discussion\nAbsolutely right. Files must be separate.\nAnswer is correct https://docs.microsoft.com/en-us/azure/dms/tutorial-sql-server-\nmanaged-instance-online\nCorrect answer: Using Azure Database Migration Service, you must provide the full\ndatabase backup and subsequent log backups in the SMB network share that the service\ncan use to migrate your databases. Be sure that you take backups using the WITH\nCHECKSUM option\nI will go with the Blob Storage Account, since I used it quite a number of times in past\nprojects. As already quoted here, SMB share is a bit of an administrative nightmare - the\nintegration runtime does not work quite well.\nWhen doing an online migration to Managed Instance you can store the backups in blob\nstorage without needing to have an SMB share and the integration runtime running.\n\n\nhttps://learn.microsoft.com/en-us/azure/dms/tutorial-sql-server-managed-instance-\nonline-ads",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "34",
    "question": "HOTSPOT -\nYou have an Azure subscription.\nYou plan to deploy an Azure SQL database by using an Azure Resource Manager\ntemplate.\nHow should you complete the template? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: \"Microsoft.Sql/servers\"\nExample:\n\"resources\": [\n{\n\"type\": \"Microsoft.Sql/servers\",\n\"apiVersion\": \"2021-08-01-preview\",\n\"name\": \"[parameters('serverName')]\",\n\"location\": \"[parameters('location')]\",\n\"properties\": {\n\"administratorLogin\": \"[parameters('administratorLogin')]\",\n\"administratorLoginPassword\": \"[parameters('administratorLoginPassword')]\"\n}\n},\n{\n\"type\": \"Microsoft.Sql/servers/databases\",\n\"apiVersion\": \"2021-08-01-preview\",\n\"name\": \"[format('{0}/{1}', parameters('serverName'), parameters('sqlDBName'))]\",\n\"location\": \"[parameters('location')]\",\n\"sku\": {\n\"name\": \"Standard\",\n\"tier\": \"Standard\"\n},\n\"dependsOn\": [\n\"[resourceId('Microsoft.Sql/servers', parameters('serverName'))]\"\n]\n\n\n}\nBox 2: \"dependsOn\": [\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/single-database-create-\narm-template-quickstart\nCommunity Discussion\ntype\": \"Microsoft.Sql/servers/databases\"\ncorrect\ncorrect\nI think the given answer is right if we see the docs link page provided with the answer\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-create-\narm-template-quickstart?view=azuresql",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "35",
    "question": "You have an on-premises Microsoft SQL Server 2019 server that hosts a\ndatabase named DB1.\nYou have an Azure subscription that contains an Azure SQL managed instance\nnamed SQLMI1 and a virtual network named VNET1. SQLMI1 resides on VNET1.\nThe on-premises network connects to VNET1 by using an ExpressRoute\nconnection.\nYou plan to migrate DB1 to SQLMI1 by using Azure Database Migration Service.\nYou need to configure VNET1 to support the migration.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure service endpoints.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Configure virtual network peering.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Deploy an Azure firewall.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure network security groups (NSGs).\nPage 83 of 818\n84 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "During virtual network setup, if you use ExpressRoute with network peering to Microsoft,\nadd the following service endpoints to the subnet in which the service will be\nprovisioned:\n* Target database endpoint (for example, SQL endpoint, Cosmos DB endpoint, and so on)\n* Storage endpoint\n* Service bus endpoint\nThis configuration is necessary because Azure Database Migration Service lacks internet\nconnectivity.\nReference:\nhttps://docs.microsoft.com/en-us/azure/dms/tutorial-sql-server-to-managed-instance"
  },
  {
    "number": "36",
    "question": "You have an on-premises Microsoft SQL server that uses the FileTables and\nFilestream features.\nYou plan to migrate to Azure SQL.\nWhich service should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQL Server on an Azure Virtual Machine",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure SQL Managed Instance",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Database for MySQL",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "SQL Server VM alternative.\nYour business might have requirements that make SQL Server on Azure Virtual Machines\na more suitable target than Azure SQL Database.\nIf one of the following conditions applies to your business, consider moving to a SQL\nServer virtual machine (VM) instead:\n* You have strict dependency on features that are still not supported, such as\nFileStream/FileTable, PolyBase, and cross-instance transactions.\n* Etc.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/migration-guides/database/sql-\nserver-to-sql-database-overview"
  },
  {
    "number": "37",
    "question": "You need to migrate an on-premises Microsoft SQL Server database to Azure\nSQL Database. The solution must minimize downtime.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure Transaction Log Shipping.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Implement Always On availability groups.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure transactional replication.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Import a BACPAC.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Use Transactional Replication.\nWhen you can't afford to remove your SQL Server database from production while the\nmigration is occurring, you can use SQL Server transactional replication as your\nmigration solution.\nNote: There are two primary methods for migrating a SQL Server 2005 or later database\nto Azure SQL Database. The first method (database copy or BACPAC importation) is\nsimpler but requires some, possibly substantial, downtime during the migration. The\nsecond method (transactional replication) is more complex, but substantially eliminates"
  },
  {
    "number": "38",
    "question": "You have an Azure SQL database named DB1.\nYou have a table name Table1 that has 20 columns of type CHAR(400). Row\ncompression for Table1 is enabled.\nDuring a database audit, you discover that none of the fields contain more than\n150 characters.\nYou need to ensure that you can apply page compression to Table1.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure the columns as sparse.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Change the column type to NVARCHAR(MAX).",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Change the column type to VARCHAR(MAX).",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Change the column type to VARCHAR(200).",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "of non-Unicode characters.\nReference:\nhttps://www.sqlshack.com/sql-varchar-data-type-deep-dive/\nhttps://36chambers.wordpress.com/2020/06/18/nvarchar-everywhere-a-thought-\nexperiment/",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": "We reduce the max length of the column from 400 to 200.\nIncorrect:\nNot A: Sparse column is useful when there are many null columns.\nThe SQL Server Database Engine uses the SPARSE keyword in a column definition to\noptimize the storage of values in that column. Therefore, when the column value is NULL\nfor any row in the table, the values require no storage.\nNot B, Not C: SQL Server 2005 got around the limitation of 8KB storage size and provided\na workaround with varchar(max). It is a non-Unicode large variable- length character\ndata type and can store a maximum of 2^31-1 bytes (2 GB) of non-Unicode characters.\nReference:\nhttps://www.sqlshack.com/sql-varchar-data-type-deep-dive/\nhttps://36chambers.wordpress.com/2020/06/18/nvarchar-everywhere-a-thought-\nexperiment/"
  },
  {
    "number": "39",
    "question": "You have an on-premises Microsoft SQL Server named SQL1 that hosts five\ndatabases.\nYou need to migrate the databases to an Azure SQL managed instance. The\nsolution must minimize downtime and prevent data loss.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Always On availability groups",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Backup and Restore",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "log shipping",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Database Migration Assistant",
        "is_correct": true
      },
      {
        "letter": "A",
        "text": "helps you upgrade to a modern data platform by\ndetecting compatibility issues that can impact database functionality in your new version\nof SQL Server or Azure SQL Database. DMA recommends performance and reliability\nimprovements for your target environment and allows you to move your schema, data,\nand uncontained objects from your source server to your target server.\nCapabilities include:\nAssess on-premises SQL Server instance(s) migrating to Azure SQL database(s).\nNote: For large migrations (in terms of number and size of databases), we recommend\nthat you use the Azure Database Migration Service, which can migrate databases at scale.\nMigrate an on-premises SQL Server instance to a modern SQL Server instance hosted on-\npremises or on an Azure virtual machine (VM) that is accessible from your on-premises\nnetwork.\nIncorrect:\nNot B: Native RESTORE DATABASE FROM URL - uses native backups from SQL Server and\nrequires some downtime.\nNot C: What is the purpose of log shipping?\nSimilar to replication, the primary purpose of log shipping is to increase database\navailability by maintaining a backup server that can replace a production server quickly.\nReference:\nhttps://docs.microsoft.com/en-us/sql/dma/dma-overview\nPage 89 of 818\n90 Microsoft - DP-300 Practice Questions - SecExams.com\nhttps://docs.microsoft.com/en-us/azure/azure-sql/migration-guides/managed-\ninstance/sql-server-to-managed-instance-guide",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": "The Data Migration Assistant (DMA) helps you upgrade to a modern data platform by\ndetecting compatibility issues that can impact database functionality in your new version\nof SQL Server or Azure SQL Database. DMA recommends performance and reliability\nimprovements for your target environment and allows you to move your schema, data,\nand uncontained objects from your source server to your target server.\nCapabilities include:\nAssess on-premises SQL Server instance(s) migrating to Azure SQL database(s).\nNote: For large migrations (in terms of number and size of databases), we recommend\nthat you use the Azure Database Migration Service, which can migrate databases at scale.\nMigrate an on-premises SQL Server instance to a modern SQL Server instance hosted on-\npremises or on an Azure virtual machine (VM) that is accessible from your on-premises\nnetwork.\nIncorrect:\nNot B: Native RESTORE DATABASE FROM URL - uses native backups from SQL Server and\nrequires some downtime.\nNot C: What is the purpose of log shipping?\nSimilar to replication, the primary purpose of log shipping is to increase database\navailability by maintaining a backup server that can replace a production server quickly.\nReference:\nhttps://docs.microsoft.com/en-us/sql/dma/dma-overview"
  },
  {
    "number": "40",
    "question": "You have a new Azure SQL database. The database contains a column that\nstores confidential information.\nYou need to track each time values from the column are returned in a query.\nThe tracking information must be stored for 365 days from the date the query\nwas executed.\nWhich three actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Turn on auditing and write audit logs to an Azure Storage account.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Add extended properties to the column.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Turn on auditing and write audit logs to an Event Hub",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Apply sensitivity labels named Highly Confidential to the column. \nE) Turn on Azure Defender for SQL",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "DE\nD: You can apply sensitivity-classification labels persistently to columns by using new\nmetadata attributes that have been added to the SQL Server database engine. This\nmetadata can then be used for advanced, sensitivity-based auditing and protection\nscenarios.\nA: An important aspect of the information-protection paradigm is the ability to monitor\naccess to sensitive data. Azure SQL Auditing has been enhanced to include a new field in\nthe audit log called data_sensitivity_information. This field logs the sensitivity\nclassifications (labels) of the data that was returned by a query. Here's an example:\nE: Enable Microsoft Defender for Azure SQL Database at the subscription level from\nMicrosoft Defender for Cloud.\nNote: Microsoft Defender for SQL is a unified package for advanced SQL security\ncapabilities. Microsoft Defender for Cloud is available for Azure SQL Database,\nAzure SQL Managed Instance, and Azure Synapse Analytics.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-\nclassification-overview https://docs.microsoft.com/en-us/azure/azure-sql/database/\nazure-defender-for-sql"
  },
  {
    "number": "41",
    "question": "You have an Azure virtual machine named VM1 on a virtual network named\nVNet1. Outbound traffic from VM1 to the internet is blocked.\nYou have an Azure SQL database named SqlDb1 on a logical server named\nSqlSrv1.\nYou need to implement connectivity between VM1 and SqlDb1 to meet the\nfollowing requirements:\n✑ Ensure that all traffic to the public endpoint of SqlSrv1 is blocked.\n✑ Minimize the possibility of VM1 exfiltrating data stored in SqlDb1.\nWhat should you create on VNet1?",
    "options": [
      {
        "letter": "A",
        "text": "a VPN gateway",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a service endpoint",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a private link",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an ExpressRoute gateway\nPage 92 of 818\n93 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure Private Link enables you to access Azure PaaS Services (for example, Azure Storage\nand SQL Database) and Azure hosted customer-owned/partner services over a private\nendpoint in your virtual network.\nTraffic between your virtual network and the service travels the Microsoft backbone\nnetwork. Exposing your service to the public internet is no longer necessary.\nReference:\nhttps://docs.microsoft.com/en-us/azure/private-link/private-link-overview"
  },
  {
    "number": "42",
    "question": "DRAG DROP -\nYou have a new Azure SQL database named DB1 on an Azure SQL server named\nAzSQL1.\nThe only user who was created is the server administrator.\nYou need to create a contained database user in DB1 who will use Azure Active\nDirectory (Azure AD) for authentication.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Set up the Active Directory Admin for AzSQL1.\nStep 2: Connect to DB1 by using the server administrator.\nSign into your managed instance with an Azure AD login granted with the sysadmin role.\nStep 3: Create a user by using the FROM EXTERNAL PROVIDER clause.\nFROM EXTERNAL PROVIDER is available for creating server-level Azure AD logins in SQL\nDatabase managed instance. Azure AD logins allow database-level\nAzure AD principals to be mapped to server-level Azure AD logins. To create an Azure AD\nuser from an Azure AD login use the following syntax:\nCREATE USER [AAD_principal] FROM LOGIN [Azure AD login]\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-user-transact-sql\nCommunity Discussion\nanswer is correct, you have to connect using AD otherwise you will see this error.\nPrincipal 'user@xbc.com' could not be created. Only connections established with Active\nDirectory accounts can create other Active Directory users.\nStep 2 is having contradiction in answer and explanation. How should we connect to\nDB1? 'using server administrator or Azure AD authentication\"?\nA contained database user does not have a login in the master database, and maps to an\nidentity in Azure AD that is associated with the database. The Azure AD identity can be\neither an individual user account or a group.\n\n\nhttps://medium.com/@letienthanh0212/setup-azure-sql-server-to-useazure-active-\ndirectory-option-b90dccd9e277\nI would go with provided answers.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "43",
    "question": "HOTSPOT -\nYou have an Azure SQL database that contains a table named Customer.\nCustomer has the columns shown in the following table.\nYou plan to implement a dynamic data mask for the Customer_Phone column.\nThe mask must meet the following requirements:\n✑ The first six numerals of each customer's phone number must be masked.\n✑ The last four digits of each customer's phone number must be visible.\n✑ Hyphens must be preserved and displayed.\nHow should you configure the dynamic data mask? To answer, select the\nappropriate options in the answer area.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: 0 -\nCustom String : Masking method that exposes the first and last letters and adds a custom\npadding string in the middle. prefix,[padding],suffix\nBox 2: xxx-xxx -\nBox 3: 5 -\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-\nmasking\nCommunity Discussion\n\n\nThe answer is correct (0,xxx-xx,5), and here is an example of it --Create table Create Table\nCustomers (ID int Primary Key identity(1,1) Not Null, Phone Nvarchar(100)) GO --Load Data\ninsert into Customers Values ('555-555-0173'),('555-505-3124'),('555-689-4321') --Create user\nCREATE USER TestUser WITHOUT LOGIN; GRANT SELECT ON Customers TO TestUser; -- Test\nthe User in Select data EXECUTE AS USER = 'TestUser'; SELECT * FROM Customers; REVERT;\n---Now add Masking ALTER TABLE Customers ALTER COLUMN Phone nvarchar(100) MASKED\nWITH (FUNCTION= 'partial(0,\"xxx-xxx\",5)'); --Select the data again using Testuser EXECUTE\nAS USER = 'TestUser'; SELECT * FROM Customers; REVERT;\nGiven Answer is correct, it can be partial(0,\"XXX-XXX\",5) or partial(0,\"XXX-XXX-\",4)\nThank you!\nThank you!\nNote that the answer shown in the picture is correct, but the explanation is incorrect\nbecause the masking is wrong in the explantion.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "44",
    "question": "DRAG DROP -\nYou have an Azure SQL database that contains a table named Employees.\nEmployees contains a column named Salary.\nYou need to encrypt the Salary column. The solution must prevent database\nadministrators from reading the data in the Salary column and must provide the\nmost secure encryption.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Create a column master key\nCreate a column master key metadata entry before you create a column encryption key\nmetadata entry in the database and before any column in the database can be\nencrypted using Always Encrypted.\nStep 2: Create a column encryption key.\nStep 3: Encrypt the Salary column by using the randomized encryption type.\nRandomized encryption uses a method that encrypts data in a less predictable manner.\nRandomized encryption is more secure, but prevents searching, grouping, indexing, and\njoining on encrypted columns.\nNote: A column encryption key metadata object contains one or two encrypted values of\na column encryption key that is used to encrypt data in a column. Each value is\nencrypted using a column master key.\nIncorrect Answers:\nDeterministic encryption.\nDeterministic encryption always generates the same encrypted value for any given plain\ntext value. Using deterministic encryption allows point lookups, equality joins, grouping\nand indexing on encrypted columns. However, it may also allow unauthorized users to\nguess information about encrypted values by examining patterns in the encrypted\ncolumn, especially if there's a small set of possible encrypted values, such as True/False,\nor North/South/East/West region.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-\nencrypted-database-engine\nCommunity Discussion\nHere to key phrase is \"most secure\" encryption that is why the answer is correct.\nRandomized encryption uses a method that encrypts data in a less predictable manner.\n\n\nRandomized encryption is more secure, but prevents searching, grouping, indexing, and\njoining on encrypted columns.\nWorth noting that if a similar question requires that the encrypted column be able to be\nused for point lookups, equality joins, grouping or indexing of the column, then\nDeterministic Encryption would be the answer.\nI think that the provided answers are correct.\nI can't see how this will prevent Administrators from reading everything. It seems that\nsomething is missing\nAlways On Encryption is missing. In real-world, there would be Security Admin that has\naccess to the keys, and Database Admin that would have access to the database. Only\nthe business application(s) that need access to such sensitive data would be able to get\nboth the keys needed from Key Vault and the Data needed from Database to be able to\nread / update the data.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "45",
    "question": "HOTSPOT -\nYou have an Azure SQL database named DB1 that contains two tables named\nTable1 and Table2. Both tables contain a column named a Column1. Column1 is\nused for joins by an application named App1.\nYou need to protect the contents of Column1 at rest, in transit, and in use.\nHow should you protect the contents of Column1? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Column encryption Key -\nAlways Encrypted uses two types of keys: column encryption keys and column master\nkeys. A column encryption key is used to encrypt data in an encrypted column. A column\nmaster key is a key-protecting key that encrypts one or more column encryption keys.\nIncorrect Answers:\nTDE encrypts the storage of an entire database by using a symmetric key called the\nDatabase Encryption Key (DEK).\nBox 2: Deterministic -\nAlways Encrypted is a feature designed to protect sensitive data, such as credit card\nnumbers or national identification numbers (for example, U.S. social security numbers),\nstored in Azure SQL Database or SQL Server databases. Always Encrypted allows clients\nto encrypt sensitive data inside client applications and never reveal the encryption keys\nto the Database Engine (SQL Database or SQL Server).\nAlways Encrypted supports two types of encryption: randomized encryption and\ndeterministic encryption.\nDeterministic encryption always generates the same encrypted value for any given plain\ntext value. Using deterministic encryption allows point lookups, equality joins, grouping\nand indexing on encrypted columns.\nIncorrect Answers:\n✑ Randomized encryption uses a method that encrypts data in a less predictable\nmanner. Randomized encryption is more secure, but prevents searching, grouping,\nindexing, and joining on encrypted columns.\n✑ Transparent data encryption (TDE) helps protect Azure SQL Database, Azure SQL\nManaged Instance, and Azure Synapse Analytics against the threat of malicious offline\nactivity by encrypting data at rest. It performs real-time encryption and decryption of the\ndatabase, associated backups, and transaction log files at rest without requiring changes\n\n\nto the application.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-\nencrypted-database-engine\nCommunity Discussion\nCorrect answer\nThough starting SQL Server 2019 they made it possible to compare, group, search, etc. on\ncolumns with Randomized encryption via Secure enclaves https://docs.microsoft.com/\nen-us/learn/modules/protect-data-transit-rest/5-explain-object-encryption-secure-\nenclaves. So now both Deterministic and Randomized would do.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "46",
    "question": "You have 40 Azure SQL databases, each for a different customer. All the\ndatabases reside on the same Azure SQL Database server.\nYou need to ensure that each customer can only connect to and access their\nrespective database.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Implement row-level security (RLS).",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create users in each database.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Configure the database firewall.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the server firewall.\nE) Create logins in the master database. \nF) Implement Always Encrypted.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "E\nE: Generating Logins -\nLogins are server wide login and password pairs, where the login has the same password"
  },
  {
    "number": "47",
    "question": "DRAG DROP -\nYou have an Azure SQL Database instance named DatabaseA on a server named\nServer1.\nYou plan to add a new user named App1 to DatabaseA and grant App1\ndb_datareader permissions. App1 will use SQL Server Authentication.\nYou need to create App1. The solution must ensure that App1 can be given\naccess to other databases by using the same credentials.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: On the master database, run CREATE LOGIN [App1] WITH PASSWORD = 'p@aaW0rd!'\nLogins are server wide login and password pairs, where the login has the same password\nacross all databases. Here is some sample Transact-SQL that creates a login:\nCREATE LOGIN readonlylogin WITH password='1231!#ASDF!a';\nYou must be connected to the master database on SQL Azure with the administrative\nlogin (which you get from the SQL Azure portal) to execute the CREATE\nLOGIN command.\nStep 2: On DatabaseA, run CREATE USER [App1] FROM LOGIN [App1]\nUsers are created per database and are associated with logins. You must be connected to\nthe database in where you want to create the user. In most cases, this is not the master\ndatabase. Here is some sample Transact-SQL that creates a user:\nCREATE USER readonlyuser FROM LOGIN readonlylogin;\nStep 3: On DatabaseA run ALTER ROLE db_datareader ADD Member [App1]\nJust creating the user does not give them permissions to the database. You have to grant\nthem access. In the Transact-SQL example below the readonlyuser is given read only\npermissions to the database via the db_datareader role.\nEXEC sp_addrolemember 'db_datareader', 'readonlyuser';\nReference:\nhttps://azure.microsoft.com/en-us/blog/adding-users-to-your-sql-azure-database/\nCommunity Discussion\nSeems to be right\nThe answers are correct\nStep 3 should be add db_datacenter no db_datareader, 1 and 2 are steps is the answer a\nguess\n\n\ndb_datacenter is a typo. it should be a db_datareader\ndb_datacenter is a typo. it should be a db_datareader",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "48",
    "question": "You have an Azure virtual machine named VM1 on a virtual network named\nVNet1. Outbound traffic from VM1 to the internet is blocked.\nYou have an Azure SQL database named SqlDb1 on a logical server named\nSqlSrv1.\nYou need to implement connectivity between VM1 and SqlDb1 to meet the\nfollowing requirements:\n✑ Ensure that VM1 cannot connect to any Azure SQL Server other than SqlSrv1.\n✑ Restrict network connectivity to SqlSrv1.\nWhat should you create on VNet1?",
    "options": [
      {
        "letter": "A",
        "text": "a VPN gateway",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a service endpoint",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a private link",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an ExpressRoute gateway",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure Private Link enables you to access Azure PaaS Services (for example, Azure Storage\nand SQL Database) and Azure hosted customer-owned/partner services over a private\nendpoint in your virtual network.\nTraffic between your virtual network and the service travels the Microsoft backbone\nnetwork. Exposing your service to the public internet is no longer necessary.\nReference:\nhttps://docs.microsoft.com/en-us/azure/private-link/private-link-overview"
  },
  {
    "number": "49",
    "question": "You are developing an application that uses Azure Data Lake Storage Gen 2.\nYou need to recommend a solution to grant permissions to a specific\napplication for a limited time period.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "role assignments",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "account keys",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "shared access signatures (SAS)",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure Active Directory (Azure AD) identities",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Access control lists (ACL) Data Lake Storage Gen2 supports the following authorization\nmechanisms:\n✑ Shared Key authorization\n✑ Shared access signature (SAS) authorization\n✑ Role-based access control (Azure RBAC)\n✑ Access control lists (ACL)\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview\nPage 110 of 818\n111 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "A shared access signature (SAS) provides secure delegated access to resources in your\nstorage account. With a SAS, you have granular control over how a client can access your\ndata. For example:\nWhat resources the client may access.\nWhat permissions they have to those resources.\nHow long the SAS is valid.\nNote: Data Lake Storage Gen2 supports the following authorization mechanisms:\n✑ Shared Key authorization\n✑ Shared access signature (SAS) authorization\n✑ Role-based access control (Azure RBAC)\nAccess control lists (ACL) Data Lake Storage Gen2 supports the following authorization\nmechanisms:\n✑ Shared Key authorization\n✑ Shared access signature (SAS) authorization\n✑ Role-based access control (Azure RBAC)\n✑ Access control lists (ACL)\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview"
  },
  {
    "number": "50",
    "question": "You are designing an enterprise data warehouse in Azure Synapse Analytics that\nwill contain a table named Customers. Customers will contain credit card\ninformation.\nYou need to recommend a solution to provide salespeople with the ability to\nview all the entries in Customers. The solution must prevent all the salespeople\nfrom viewing or inferring the credit card information.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "row-level security",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "data masking",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Always Encrypted",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "column-level security",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics support\ndynamic data masking. Dynamic data masking limits sensitive data exposure by masking\nit to non-privileged users.\nThe Credit card masking method exposes the last four digits of the designated fields and\nadds a constant string as a prefix in the form of a credit card.\nExample:"
  },
  {
    "number": "51",
    "question": "HOTSPOT -\nYou have an Azure subscription that is linked to a hybrid Azure Active Directory\n(Azure AD) tenant. The subscription contains an Azure Synapse Analytics SQL\npool named Pool1.\nYou need to recommend an authentication solution for Pool1. The solution must\nsupport multi-factor authentication (MFA) and database-level authentication.\nWhich authentication solution or solutions should you include in the\nrecommendation? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Azure AD authentication -\nAzure Active Directory authentication supports Multi-Factor authentication through\nActive Directory Universal Authentication.\nBox 2: Contained database users -\nAzure Active Directory Uses contained database users to authenticate identities at the\ndatabase level.\nIncorrect:\nSQL authentication: To connect to dedicated SQL pool (formerly SQL DW), you must\nprovide the following information:\n✑ Fully qualified servername\n✑ Specify SQL authentication\n✑ Username\n✑ Password\nDefault database (optional)\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-\ndata-warehouse-authentication\nCommunity Discussion\nANSWER CORRECT\nyes, correct answer. Azure SQL Database, Azure SQL Managed Instance, and Azure\nSynapse Analytics support connections from SQL Server Management Studio (SSMS)\nusing Azure Active Directory - Universal with MFA authentication https://\n\n\nlearn.microsoft.com/en-us/azure/azure-sql/database/authentication-mfa-ssms-\noverview?view=azuresql\nits ok?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "52",
    "question": "You have a data warehouse in Azure Synapse Analytics.\nYou need to ensure that the data in the data warehouse is encrypted at rest.\nWhat should you enable?",
    "options": [
      {
        "letter": "A",
        "text": "Transparent Data Encryption (TDE)",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Advanced Data Security for this database",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Always Encrypted for all columns",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Secure transfer required",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Transparent data encryption (TDE) helps protect Azure SQL Database, Azure SQL Managed\nInstance, and Azure Synapse Analytics against the threat of malicious offline activity by\nencrypting data at rest.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-\nencryption-tde-overview"
  },
  {
    "number": "53",
    "question": "You are designing a security model for an Azure Synapse Analytics dedicated\nSQL pool that will support multiple companies.\nYou need to ensure that users from each company can view only the data of\ntheir respective company.\nWhich two objects should you include in the solution? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "a column encryption key",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "asymmetric keys",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a function",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "a custom role-based access control (RBAC) role\nE) a security policy",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "E\nRow-Level Security (RLS) simplifies the design and coding of security in your application.\nRLS helps you implement restrictions on data row access. For example, you can ensure\nthat workers access only those data rows that are pertinent to their department. Another\nexample is to restrict customers' data access to only the data relevant to their company.\nImplement RLS by using the CREATE SECURITY POLICYTransact-SQL statement, and\npredicates created as inline table-valued functions.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security"
  },
  {
    "number": "54",
    "question": "You have an Azure subscription that contains an Azure Data Factory version 2\n(V2) data factory named df1. DF1 contains a linked service.\nYou have an Azure Key vault named vault1 that contains an encryption kay\nnamed key1.\nYou need to encrypt df1 by using key1.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Disable purge protection on vault1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Remove the linked service from df1.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a self-hosted integration runtime.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Disable soft delete on vault1.\nPage 117 of 818\n118 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "A customer-managed key can only be configured on an empty data Factory. The data\nfactory can't contain any resources such as linked services, pipelines and data flows. It is\nrecommended to enable customer-managed key right after factory creation.\nNote: Azure Data Factory encrypts data at rest, including entity definitions and any data\ncached while runs are in progress. By default, data is encrypted with a randomly\ngenerated Microsoft-managed key that is uniquely assigned to your data factory.\nIncorrect Answers:\nA, D: Should enable Soft Delete and Do Not Purge on Azure Key Vault.\nUsing customer-managed keys with Data Factory requires two properties to be set on the\nKey Vault, Soft Delete and Do Not Purge. These properties can be enabled using either\nPowerShell or Azure CLI on a new or existing key vault.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key"
  },
  {
    "number": "55",
    "question": "You have an Azure subscription that contains a server named Server1. Server1\nhosts two Azure SQL databases named DB1 and DB2.\nYou plan to deploy a Windows app named App1 that will authenticate to DB2 by\nusing SQL authentication.\nYou need to ensure that App1 can access DB2. The solution must meet the\nfollowing requirements:\n✑ App1 must be able to view only DB2.\n✑ Administrative effort must be minimized.\nWhat should you create?",
    "options": [
      {
        "letter": "A",
        "text": "a contained database user for App1 on DB2",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a login for App1 on Server1",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a contained database user from an external provider for App1 on DB2",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a contained database user from a Windows login for App1 on DB2",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Use contained database users to authenticate SQL Server and SQL Database connections\nat the database level. A contained database is a database that is isolated from other\ndatabases and from the instance of SQL Server/ SQL Database (and the master\ndatabase) that hosts the database. SQL Server supports contained database users for\nboth Windows and SQL Server authentication.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/contained-\ndatabase-users-making-your-database-portable?view=sql-server-ver15"
  },
  {
    "number": "56",
    "question": "You create five Azure SQL Database instances on the same logical server.\nIn each database, you create a user for an Azure Active Directory (Azure AD) user\nnamed User1.\nUser1 attempts to connect to the logical server by using Azure Data Studio and\nreceives a login error.\nYou need to ensure that when User1 connects to the logical server by using\nAzure Data Studio, User1 can see all the databases.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create User1 in the master database.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Assign User1 the db_datareader role for the master database.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Assign User1 the db_datareader role for the databases that User1 creates.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Grant SELECT on sys.databases to public in the master database.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Logins and users: A user account in a database can be associated with a login that is\nstored in the master database or can be a user name that is stored in an individual\ndatabase.\nA login is an individual account in the master database, to which a user account in one\nor more databases can be linked. With a login, the credential information for the user\naccount is stored with the login.\nA user account is an individual account in any database that may be, but does not have\nto be, linked to a login. With a user account that is not linked to a login, the credential\ninformation is stored with the user account.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/logins-create-manage"
  },
  {
    "number": "57",
    "question": "You have an Azure virtual machine named VM1 on a virtual network named\nVNet1. Outbound traffic from VM1 to the internet is blocked.\nYou have an Azure SQL database named SqlDb1 on a logical server named\nSqlSrv1.\nYou need to implement connectivity between VM1 and SqlDb1 to meet the\nfollowing requirements:\n✑ Ensure that VM1 cannot connect to any Azure SQL Server other than SqlSrv1.\n✑ Restrict network connectivity to SqlSrv1.\nWhat should you create on VNet1?",
    "options": [
      {
        "letter": "A",
        "text": "a VPN gateway",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a service endpoint",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a private endpoint",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an ExpressRoute gateway",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "A private endpoint is a network interface that uses a private IP address from your virtual\nnetwork. This network interface connects you privately and securely to a service powered\nby Azure Private Link. By enabling a private endpoint, you're bringing the service into\nyour virtual network.\nThe service could be an Azure service such as:\n✑ Azure Storage\n✑ Azure Cosmos DB"
  },
  {
    "number": "58",
    "question": "HOTSPOT -\nYou have an Azure SQL database named db1 that contains an Azure Active\nDirectory (Azure AD) user named user1.\nYou need to test impersonation of user1 in db1 by running a SELECT statement\nand returning to the original execution context.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: USER -\nUsing EXECUTE AS and REVERT to switch context.\nThe following example creates a context execution stack using multiple principals. The\nREVERT statement is then used to reset the execution context to the previous caller.\n**\nEXECUTE AS USER = 'user2';\n--The following REVERT statements will reset the execution context to the previous\ncontext.\nREVERT;\n**\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/execute-as-transact-sql?\nview=sql-server-ver15 https://docs.microsoft.com/en-us/sql/t-sql/functions/suser-\nsname-transact-sql?view=sql-server-ver15\nCommunity Discussion\nThe answer is correct.\n\n\nhttps://en.dirceuresende.com/blog/sql-server-how-to-use-execute-as-to-execute-\ncommands-as-another-impersonate-user-and-how-to-prevent-it/\nyour link doesn't work anymore\nyour link doesn't work anymore",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "59",
    "question": "DRAG DROP -\nYou have an Azure SQL database named DB1. DB1 contains a table that has a\ncolumn named Col1.\nYou need to encrypt the data in Col1.\nWhich four actions should you perform for DB1 in sequence? To answer, move\nthe appropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nUse the following steps for column level encryption:\n1. Create a database master key (Step 1)\n2. Create a self-signed certificate for SQL Server (Step 2)\n3. Configure a symmetric key for encryption (Step 3)\n4. Encrypt the column data (this includes Open the symmetric key - Step 4)\n5. Query and verify the encryption\nStep 1: Create a database master key\nCreate a database master key for column level SQL Server encryption\nIn this first step, we define a database master key and provide a password to protect it. It\nis a symmetric key for protecting the private keys and asymmetric keys.\nStep 2: Create a certificate.\nCreate a self-signed certificate for Column level SQL Server encryption\nIn this step, we create a self-signed certificate using the CREATE CERTIFICATE statement.\nYou might have seen that an organization receives a certificate from a certification\nauthority and incorporates into their infrastructures. In SQL Server, we can use a self-\nsigned certificate without using a certification authority certificate.\nStep 3: Create a symmetric key.\nConfigure a symmetric key for column level SQL Server encryption.\nIn this step, we will define a symmetric key that you can see in the encryption hierarchy\nas well. The symmetric key uses a single key for encryption and decryption as well.\nStep 4: Open the symmetric key -\nData encryption.\nLet's encrypt the data in this newly added column.\nIn a query window, open the symmetric key and decrypt using the certificate. We need to\nuse the same symmetric key and certificate name that we created earlier\nEtc.\nReference:\nhttps://www.sqlshack.com/an-overview-of-the-column-level-sql-server-encryption/\n\n\nCommunity Discussion\nCreate a certificate Create a symmetric key Open the symmetric key Update Col1\nI believe Create column master key is for ALWAYS ENCRYPTED, so B is not part of this.\nLooks like provided answer is correct. 1. Create database master key 2. CREATE\nCERTIFICATE 3. CREATE SYMMETRIC KEY 4. Open symmetric key 5. Update data (have to\nopen symmetric key first, according to sample script on MS article) https://\ndocs.microsoft.com/en-us/sql/relational-databases/security/encryption/encrypt-a-\ncolumn-of-data?view=sql-server-ver16\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/\nencrypt-a-column-of-data?view=sql-server-ver16\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/\nencrypt-a-column-of-data?view=sql-server-ver16\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/\nencrypt-a-column-of-data?view=sql-server-ver16#example-encrypt-with-symmetric-\nencryption-and-authenticator",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "60",
    "question": "HOTSPOT -\nYou have a Microsoft SQL Server database named DB1 that contains a table\nnamed Table1.\nThe database role membership for a user named User1 is shown in the\nfollowing exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: delete a row from Table1 -\nMembers of the db_datawriter fixed database role can add, delete, or change data in all\nuser tables.\nBox 2: db_datareader -\nMembers of the db_datareader fixed database role can read all data from all user tables.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-\naccess/database-level-roles\n\n\nCommunity Discussion\nMembers of the db_datawriter fixed database role can add, delete, or change data in all\nuser tables. Members of the db_datareader fixed database role can read all data from all\nuser tables and views. User objects can exist in any schema except sys and\nINFORMATION_SCHEMA.\nDB_DATAWRITER: Members of the db_datawriter fixed database role can add, delete, or\nchange data in all user tables. In most use cases this role will be combined with\ndb_datareader membership to allow reading the data that is to be modified.\nDB_DATAREADER: Members of the db_datawriter fixed database role can add, delete, or\nchange data in all user tables. In most use cases this role will be combined with\ndb_datareader membership to allow reading the data that is to be modified. https://\nlearn.microsoft.com/en-us/sql/relational-databases/security/authentication-access/\ndatabase-level-roles?view=sql-server-ver16\nWithin the database, db_datawriter allows users with the role to insert, update, or delete\ndata in any user table or view in a database. The db_ddladmin role provides permissions\nfor the user to modify any the schema of a database with Data Definition Language\ncommands. The permissions for these two roles do not overlap.\nMembers of the db_datawriter fixed database role can add, delete, or change data in all\nuser tables. Members of the db_ddladmin fixed database role can run any Data\nDefinition Language (DDL) command in a database.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "61",
    "question": "You have an Azure subscription that contains a logical SQL server named\nServer1. The master database of Server1 contains a user named User1.\nYou need to ensure that User1 can create databases on Server1.\nWhich database role should you assign to User1?",
    "options": [
      {
        "letter": "A",
        "text": "db_owner",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "dbmanager",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "dbo",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "db_ddladmin\nPage 131 of 818\n132 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "dbmanager: Can create and delete databases. A member of the dbmanager role that\ncreates a database, becomes the owner of that database, which allows that user to\nconnect to that database as the dbo user. The dbo user has all database permissions in\nthe database. Members of the dbmanager role don't necessarily have permission to\naccess databases that they don't own.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-\naccess/database-level-roles"
  },
  {
    "number": "62",
    "question": "You have an on-premises Microsoft SQL Server 2019 instance named SQL1 that\nhosts a database named db1. You have an Azure subscription that contains an\nAzure SQL managed instance named MI1 and an Azure Storage account named\nstorage1.\nYou plan to migrate db1 to MI1 by using the backup and restore process.\nYou need to ensure that you can back up db1 to storage1. The solution must\nmeet the following requirements:\n✑ Use block blob storage.\n✑ Maximize security.\nWhat should you do on storage1?",
    "options": [
      {
        "letter": "A",
        "text": "Generate a shared access signature (SAS).",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create an access policy.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Rotate the storage keys.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable infrastructure encryption.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "If your database contains sensitive data that is protected by Always Encrypted, migration\nprocess using Azure Data Studio with DMS will automatically migrate your Always\nEncrypted keys to your target SQL Server on Azure Virtual Machine.\nReference:\nhttps://docs.microsoft.com/en-us/azure/dms/tutorial-sql-server-to-virtual-machine-\nonline-ads"
  },
  {
    "number": "63",
    "question": "You have an Azure SQL database named DB1.\nA user named User1 has an Azure Active Directory (Azure AD) account.\nYou need to provide User1 with the ability to add and remove columns from the\ntables in DB1. The solution must use the principle of least privilege.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Assign the database user the db_owner role.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a contained database user.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a login and an associated database user. \nPage 134 of 818\n135 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Assign the database user the db_ddladmin role.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "account. So you could create a contained database user with the\nAzure AD account, or you could create a Login and User with the Azure AD account. Since\nprincipal of least privilege is mentioned, wouldnt a contained Azure AD User be the\nbetter option?\nSelected Answer: BD\nAs the solution must use the principle of least privilege then we should create a\ncontained DB user (we can do it using an AD account or using SQL Server authentication\ndue to both are allowed to create contained DB users)\nAgreed @lukelin08\nAgreed @lukelin08\nPage 135 of 818\n136 Microsoft - DP-300 Practice Questions - SecExams.com\nB. Create a contained database user. D. Assign the database user the db_ddladmin role.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "D\nC: Logins and users: A user account in a database can be associated with a login that is\nstored in the master database or can be a user name that is stored in an individual\ndatabase.\nA login is an individual account in the master database, to which a user account in one\nor more databases can be linked. With a login, the credential information for the user\naccount is stored with the login.\nA user account is an individual account in any database that may be, but does not have\nto be, linked to a login. With a user account that is not linked to a login, the credential\ninformation is stored with the user account.\nD: db_ddladmin: Members of the db_ddladmin fixed database role can run any Data\nDefinition Language (DDL) command (such as adding and removing columns) in a\ndatabase.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/logins-create-manage\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-\naccess/database-level-roles?view=sql-server-ver15"
  },
  {
    "number": "64",
    "question": "You have an Azure SQL database named sqldb1.\nYou need to minimize the possibility of Query Store transitioning to a read-only\nstate.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Double the value of Data Flush interval",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Decrease by half the value of Data Flush Interval",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Double the value of Statistics Collection Interval",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Decrease by half the value of Statistics Collection interval",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "limit isn't strictly enforced. Storage size is checked only when Query\nStore writes data to disk. This interval is set by the Data Flush Interval\n(Minutes) option. If Query Store has breached the maximum size limit between storage\nsize checks, it transitions to read-only mode.\nIncorrect Answers:\nC: Statistics Collection Interval: Defines the level of granularity for the collected runtime\nstatistic, expressed in minutes. The default is 60 minutes. Consider using a lower value if\nyou require finer granularity or less time to detect and mitigate issues. Keep in mind that\nthe value directly affects the size of Query Store data.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-\nwith-the-query-store",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "The Max Size (MB) limit isn't strictly enforced. Storage size is checked only when Query\nStore writes data to disk. This interval is set by the Data Flush Interval\n(Minutes) option. If Query Store has breached the maximum size limit between storage\nsize checks, it transitions to read-only mode.\nIncorrect Answers:\nC: Statistics Collection Interval: Defines the level of granularity for the collected runtime\nstatistic, expressed in minutes. The default is 60 minutes. Consider using a lower value if\nyou require finer granularity or less time to detect and mitigate issues. Keep in mind that\nthe value directly affects the size of Query Store data.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-\nwith-the-query-store"
  },
  {
    "number": "65",
    "question": "You have SQL Server 2019 on an Azure virtual machine that runs Windows Server\n2019. The virtual machine has 4 vCPUs and 28 GB of memory.\nYou scale up the virtual machine to 16 vCPUSs and 64 GB of memory.\nYou need to provide the lowest latency for tempdb.\nWhat is the total number of data files that tempdb should contain?",
    "options": [
      {
        "letter": "A",
        "text": "2",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "4",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "8",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "64\nPage 137 of 818\n138 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The number of files depends on the number of (logical) processors on the machine. As a\ngeneral rule, if the number of logical processors is less than or equal to eight, use the\nsame number of data files as logical processors. If the number of logical processors is\ngreater than eight, use eight data files and then if contention continues, increase the\nnumber of data files by multiples of 4 until the contention is reduced to acceptable\nlevels or make changes to the workload/code.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/databases/tempdb-\ndatabase"
  },
  {
    "number": "66",
    "question": "HOTSPOT -\nYou have an Azure SQL database named db1.\nYou need to retrieve the resource usage of db1 from the last week.\nHow should you complete the statement? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: sys.resource_stats -\nsys.resource_stats returns CPU usage and storage data for an Azure SQL Database. It has\ndatabase_name and start_time columns.\nBox 2: DateAdd -\nThe following example returns all databases that are averaging at least 80% of compute\nutilization over the last one week.\nDECLARE @s datetime;\nDECLARE @e datetime;\nSET @s= DateAdd(d,-7,GetUTCDate());\nSET @e= GETUTCDATE();\nSELECT database_name, AVG(avg_cpu_percent) AS Average_Compute_Utilization\nFROM sys.resource_stats -\nWHERE start_time BETWEEN @s AND @e\nGROUP BY database_name -\nHAVING AVG(avg_cpu_percent) >= 80\nIncorrect Answers:\nsys.dm_exec_requests:\nsys.dm_exec_requests returns information about each request that is executing in SQL\nServer. It does not have a column named database_name. sys.dm_db_resource_stats:\nsys.dm_db_resource_stats does not have any start_time column.\nNote: sys.dm_db_resource_stats returns CPU, I/O, and memory consumption for an Azure\nSQL Database database. One row exists for every 15 seconds, even if there is no activity\nin the database. Historical data is maintained for approximately one hour.\nSys.dm_user_db_resource_governance returns actual configuration and capacity settings\n\n\nused by resource governance mechanisms in the current database or elastic pool. It does\nnot have any start_time column.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-\nresource-stats-azure-sql-database\nCommunity Discussion\nsys.dm_db_resource_stats: This DMV records a snapshot of resource usage for the\ndatabase every 15 seconds (kept for 1 hour). sys.resource_stats: This can be run in the\ncontext of the master database of the Azure SQL Database server to see resource usage\nfor all Azure SQL Database databases associated with the server. This view is less\ngranular and shows resource usage every 5 minutes (kept for 14 days). sys.resource_stats\nis the correct answer due to two reason 1. it holds data for 14 days while the other one\nholds data for 1 hour only. 2.. It has start_time column while the other one does not have\nit.\nAnswers are correct. 1. sys.dm_db_resource_stats can't be becuase only keep data by 1h.\nsys.resource_stats can keep by 14days. 2.dateadd is correct\nAnswer is given correct https://docs.microsoft.com/en-us/sql/relational-databases/\nsystem-catalog-views/sys-resource-stats-azure-sql-database?view=azuresqldb-current\nsee the example section yo will understand\nAnswer is correct\nSys.dm_db_resource_stats has not got 'start time' column answer is sys.resource_stats",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "67",
    "question": "You have 50 Azure SQL databases.\nYou need to notify the database owner when the database settings, such as the\ndatabase size and pricing tier, are modified in Azure.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a diagnostic setting for the activity log that has the Security log enabled.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "For the database, create a diagnostic setting that has the InstanceAndAppAdvanced metric\nenabled.\nPage 141 of 818\n142 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an alert rule that uses a Metric signal type.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create an alert rule that uses an Activity Log signal type.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Activity log events - An alert can trigger on every event, or, only when a certain number\nof events occur.\nThe activity log of a database logs the change for the SKU (Stock-keeping-Unit) change.\nIncorrect Answers:\nC: Metric values - The alert triggers when the value of a specified metric crosses a\nthreshold you assign in either direction. That is, it triggers both when the condition is\nfirst met and then afterwards when that condition is no longer being met.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/alerts-insights-configure-\nportal"
  },
  {
    "number": "68",
    "question": "You have several Azure SQL databases on the same Azure SQL Database server\nin a resource group named ResourceGroup1.\nYou must be alerted when CPU usage exceeds 80 percent for any database. The\nsolution must apply to any additional databases that are created on the Azure\nSQL server.\nWhich resource type should you use to create the alert?",
    "options": [
      {
        "letter": "A",
        "text": "Resource Groups",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQL Servers",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL Databases",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "SQL Virtual Machines",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "There are resource types related to application code, compute infrastructure, networking,\nstorage + databases.\nYou can deploy up to 800 instances of a resource type in each resource group.\nSome resources can exist outside of a resource group. These resources are deployed to\nthe subscription, management group, or tenant. Only specific resource types are\nsupported at these scopes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/management/\nresource-providers-and-types"
  },
  {
    "number": "69",
    "question": "You have SQL Server 2019 on an Azure virtual machine that runs Windows Server\n2019. The virtual machine has 4 vCPUs and 28 GB of memory.\nYou scale up the virtual machine to 8 vCPUSs and 64 GB of memory.\nYou need to provide the lowest latency for tempdb.\nWhat is the total number of data files that tempdb should contain?",
    "options": [
      {
        "letter": "A",
        "text": "2",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "4",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "8",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "64",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The number of files depends on the number of (logical) processors on the machine. As a\ngeneral rule, if the number of logical processors is less than or equal to eight, use the\nsame number of data files as logical processors. If the number of logical processors is\ngreater than eight, use eight data files and then if contention continues, increase the\nnumber of data files by multiples of 4 until the contention is reduced to acceptable\nlevels or make changes to the workload/code.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/databases/tempdb-\ndatabase"
  },
  {
    "number": "70",
    "question": "You have SQL Server on an Azure virtual machine that contains a database\nnamed DB1. DB1 contains a table named CustomerPII.\nYou need to record whenever users query the CustomerPII table.\nWhich two options should you enable? Each correct answer presents part of the\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "server audit specification",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQL Server audit",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "database audit specification",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "a server principal",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nAuditing an instance of SQL Server or a SQL Server database involves tracking and\nlogging events that occur on the system. The SQL Server Audit object collects a single\ninstance of server-level or database-level actions and groups of actions to monitor. The\naudit is at the SQL Server instance level. You can have multiple audits per SQL Server\ninstance. The Database-Level Audit Specification object belongs to an audit. You can\ncreate one database audit specification per\nSQL Server database per audit.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/auditing/create-a-\nserver-audit-and-database-audit-specification"
  },
  {
    "number": "71",
    "question": "You have an Azure virtual machine based on a custom image named VM1.\nVM1 hosts an instance of Microsoft SQL Server 2019 Standard.\nYou need to automate the maintenance of VM1 to meet the following\nrequirements:\n✑ Automate the patching of SQL Server and Windows Server.\n✑ Automate full database backups and transaction log backups of the\ndatabases on VM1.\n✑ Minimize administrative effort.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Enable a system-assigned managed identity for VM1",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Register the Azure subscription to the Microsoft.Sql resource provider",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Install an Azure virtual machine Desired State Configuration (DSC) extension on VM1",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Register the Azure subscription to the Microsoft.SqlVirtualMachine resource provider",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Automated Patching depends on the SQL Server infrastructure as a service (IaaS) Agent\nExtension. The SQL Server IaaS Agent Extension (SqlIaasExtension) runs on Azure virtual\nmachines to automate administration tasks. The SQL Server IaaS extension is installed\nwhen you register your SQL Server VM with the SQL\nServer VM resource provider.\nTo utilize the SQL IaaS Agent extension, you must first register your subscription with the\nMicrosoft.SqlVirtualMachine provider, which gives the SQL IaaS extension the ability to\ncreate resources within that specific subscription.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/sql-\nserver-iaas-agent-extension-automate-management https://docs.microsoft.com/en-us/\nazure/azure-sql/virtual-machines/windows/sql-agent-extension-manually-register-\nsingle-vm?tabs=bash%2Cazure-cli"
  },
  {
    "number": "72",
    "question": "HOTSPOT -\nYou are building an Azure Stream Analytics job to retrieve game data.\nYou need to ensure that the job returns the highest scoring record for each five-\nminute time interval of each game.\nHow should you complete the Stream Analytics query? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: TopOne() OVER(PARTITION BY Game ORDER BY Score Desc)\nTopOne returns the top-rank record, where rank defines the ranking position of the event\nin the window according to the specified ordering. Ordering/ranking is based on event\ncolumns and can be specified in ORDER BY clause.\nAnalytic Function Syntax:\nTopOne() OVER ([<PARTITION BY clause>] ORDER BY (<column name> [ASC |DESC])+ <LIMIT\nDURATION clause> [<WHEN clause>])\nBox 2: Tumbling(minute 5)\nTumbling window functions are used to segment a data stream into distinct time\nsegments and perform a function against them, such as the example below. The key\ndifferentiators of a Tumbling window are that they repeat, do not overlap, and an event\ncannot belong to more than one tumbling window.\nReference:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/topone-azure-stream-\n\n\nanalytics https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/stream-\nanalytics/stream-analytics-window-functions.md\nCommunity Discussion\nI got a similar question while taking a DP-203 exam. Don't think this is 100% question for\nDP-300\nThe answer looks right\nThat looks like your answer to most questions!\nThat looks like your answer to most questions!\nI believe this question is outside the scope of DP-300 and for DP-203.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "73",
    "question": "A company plans to use Apache Spark analytics to analyze intrusion detection\ndata.\nYou need to recommend a solution to analyze network and system activity data\nfor malicious activities and policy violations. The solution must minimize\nadministrative efforts.\nWhat should you recommend?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Data Lake Storage",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Databricks",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure HDInsight",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Data Factory",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure DataBricks does have integration with Azure Monitor. Application logs and metrics\nfrom Azure Databricks can be send to a Log Analytics workspace.\nReference:"
  },
  {
    "number": "74",
    "question": "DRAG DROP -\nYour company analyzes images from security cameras and sends alerts to\nsecurity teams that respond to unusual activity. The solution uses Azure\nDatabricks.\nYou need to send Apache Spark level events, Spark Structured Streaming\nmetrics, and application metrics to Azure Monitor.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions in the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nSend application metrics using Dropwizard.\n\n\nSpark uses a configurable metrics system based on the Dropwizard Metrics Library.\nTo send application metrics from Azure Databricks application code to Azure Monitor,\nfollow these steps:\nStep 1: Configure your Azure Databricks cluster to use the Databricksmonitoring library.\nPrerequisite: Configure your Azure Databricks cluster to use the monitoring library.\nStep 2: Build the spark-listeners-loganalytics-1.0-SNAPSHOT.jar JAR file\nStep 3: Create Dropwizard counters in your application code\nCreate Dropwizard gauges or counters in your application code\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/\napplication-logs\nCommunity Discussion\nis his related to DP-300?\nI believe this is outside the scope of DP-300 and belongs to DP-203\nI believe this is outside the scope of DP-300 and belongs to DP-203\n-> Configure the Databricks cluster to use the Databricks monitoring library. -> Build a\nspark-listner-lognalytics-1.0-SNAPSHOT.jar JAR file. -> Create Dropwizard counters in the\napplication code. https://docs.microsoft.com/en-us/azure/architecture/databricks-\nmonitoring/application-logs\nlooks DP-300, what do you think?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "75",
    "question": "You have an Azure data solution that contains an enterprise data warehouse in\nAzure Synapse Analytics named DW1.\nSeveral users execute adhoc queries to DW1 concurrently.\nYou regularly perform automated data loads to DW1.\nYou need to ensure that the automated data loads have enough memory\navailable to complete quickly and successfully when the adhoc queries run.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Assign a smaller resource class to the automated data load queries.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create sampled statistics to every column in each table of DW1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Assign a larger resource class to the automated data load queries.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Hash distribute the large fact tables in DW1 before performing the automated data loads.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The performance capacity of a query is determined by the user's resource class.\nSmaller resource classes reduce the maximum memory per query, but increase\nconcurrency.\nLarger resource classes increase the maximum memory per query, but reduce\nconcurrency.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/\nresource-classes-for-workload-management"
  },
  {
    "number": "76",
    "question": "You are monitoring an Azure Stream Analytics job.\nYou discover that the Backlogged input Events metric is increasing slowly and is\nconsistently non-zero.\nYou need to ensure that the job can handle all the events.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Remove any named consumer groups from the connection and use $default.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Change the compatibility level of the Stream Analytics job.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create an additional output stream for the existing input stream.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Increase the number of streaming units (SUs).",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Backlogged Input Events: Number of input events that are backlogged. A non-zero value\nfor this metric implies that your job isn't able to keep up with the number of incoming\nevents. If this value is slowly increasing or consistently non-zero, you should scale out\nyour job, by increasing the SUs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring"
  },
  {
    "number": "77",
    "question": "You have an Azure Stream Analytics job.\nYou need to ensure that the job has enough streaming units provisioned.\nYou configure monitoring of the SU % Utilization metric.\nWhich two additional metrics should you monitor? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Late Input Events",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Out of order Events",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Backlogged Input Events",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Watermark Delay \nE) Function Events",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "D\nTo react to increased workloads and increase streaming units, consider setting an alert\nof 80% on the SU Utilization metric. Also, you can use watermark delay and backlogged\nevents metrics to see if there is an impact.\nNote: Backlogged Input Events: Number of input events that are backlogged. A non-zero\nvalue for this metric implies that your job isn't able to keep up with the number of\nincoming events. If this value is slowly increasing or consistently non-zero, you should\nscale out your job, by increasing the SUs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring"
  },
  {
    "number": "78",
    "question": "You have an Azure Databricks resource.\nYou need to log actions that relate to changes in compute for the Databricks\nresource.\nWhich Databricks services should you log?",
    "options": [
      {
        "letter": "A",
        "text": "clusters",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "jobs",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "DBFS",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SSH\nE) workspace",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Clusters logs include information regarding changes in compute.\nIncorrect:\nNot E: Workspace logs do not include information related to changes in compute.\nReference:\nhttps://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-\nsettings/azure-diagnostic-logs#configure-diagnostic-log-delivery"
  },
  {
    "number": "79",
    "question": "Your company uses Azure Stream Analytics to monitor devices.\nThe company plans to double the number of devices that are monitored.\nYou need to monitor a Stream Analytics job to ensure that there are enough\nprocessing resources to handle the additional load.\nWhich metric should you monitor?",
    "options": [
      {
        "letter": "A",
        "text": "Input Deserialization Errors",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Late Input Events",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Early Input Events",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Watermark delay",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "The Watermark delay metric is computed as the wall clock time of the processing node\nminus the largest watermark it has seen so far.\nThe watermark delay metric can rise due to:\n1. Not enough processing resources in Stream Analytics to handle the volume of input\nevents.\n2. Not enough throughput within the input event brokers, so they are throttled.\n3. Output sinks are not provisioned with enough capacity, so they are throttled.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-time-\nhandling"
  },
  {
    "number": "80",
    "question": "You manage an enterprise data warehouse in Azure Synapse Analytics.\nUsers report slow performance when they run commonly used queries. Users do\nnot report performance changes for infrequently used queries.\nYou need to monitor resource utilization to determine the source of the\nperformance issues.\nWhich metric should you monitor?",
    "options": [
      {
        "letter": "A",
        "text": "Local tempdb percentage",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "DWU percentage",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Data Warehouse Units (DWU) used",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Cache hit percentage",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "You can use Azure Monitor to view cache metrics to troubleshoot query performance.\nThe key metrics for troubleshooting the cache are Cache hit percentage and Cache used\npercentage.\nPossible scenario: Your current working data set cannot fit into the cache which causes a\nlow cache hit percentage due to physical reads. Consider scaling up your performance\nlevel and rerun your workload to populate the cache.\nReference:\nhttps://docs.microsoft.com/da-dk/azure/synapse-analytics/sql-data-warehouse/sql-\ndata-warehouse-how-to-monitor-cache"
  },
  {
    "number": "81",
    "question": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a\ndatabase named DB1. DB1 contains a fact table named Table.\nYou need to identify the extent of the data skew in Table1.\nWhat should you do in Synapse Studio?",
    "options": [
      {
        "letter": "A",
        "text": "Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Connect to the built-in pool and run DBCC CHECKALLOC.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Connect to Pool1 and run DBCC CHECKALLOC.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Connect to the built-in pool and query sys.dm_pdw_nodes_db_partition_stats.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "First connect to Pool1, not the built-in serverless pool, then use\nsys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data.\nReference:"
  },
  {
    "number": "82",
    "question": "You have an Azure Synapse Analytics dedicated SQL pool.\nYou run PDW_SHOWSPACEUSED('dbo.FactInternetSales'); and get the results\nshown in the following table.\nWhich statement accurately describes the dbo.FactInternetSales table?",
    "options": [
      {
        "letter": "A",
        "text": "The table contains less than 10,000 rows.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "All distributions contain data.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "The table uses round-robin distribution",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "The table is skewed.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "The rows per distribution can vary up to 10% without a noticeable impact on\nperformance. Here the distribution varies more than 10%. It is skewed.\nNote: SHOWSPACEUSED displays the number of rows, disk space reserved, and disk space\nused for a specific table, or for all tables in a Azure Synapse\nAnalytics or Parallel Data Warehouse database.\nThis is a very quick and simple way to see the number of table rows that are stored in\neach of the 60 distributions of your database. Remember that for the most balanced\nperformance, the rows in your distributed table should be spread evenly across all the"
  },
  {
    "number": "83",
    "question": "DRAG DROP -\nYou have an Azure SQL managed instance named SQLMI1 that has Resource\nGovernor enabled and is used by two apps named App1 and App2.\nYou need to configure SQLMI1 to limit the CPU and memory resources that can\nbe allocated to App1.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/resource-governor/\nresource-governor?view=sql-server-ver15 https://docs.microsoft.com/en-us/sql/\n\n\nrelational-databases/resource-governor/create-and-test-a-classifier-user-defined-\nfunction?view=sql-server-ver15\nCommunity Discussion\nCorrect order: 1 The resource pool provides and limits the resources required by the\napplication 2 The workload group uses the resource pool it is associated with 3 The\nclassifier function assign the incoming session to the workload group 4 Modify resource\ngovernor to apply configuration changes\nlooks good, what do you think?\nlooks good, my G.\nlooks good, my G.\nhttps://learn.microsoft.com/en-us/sql/-databases/resource-governor/create-and-test-a-\nclassifier-user-defined-function?view=sql-server-ver16 CREATE RESOURCE POOL CREATE\nWORKLOAD GROUP Create the classifier function ALTER RESOURCE GOVERNOR\nRECONFIGURE; GO",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "84",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have SQL Server 2019 on an Azure virtual machine.\nYou are troubleshooting performance issues for a query in a SQL Server\ninstance.\nTo gather more information, you query sys.dm_exec_requests and discover that\nthe wait type is PAGELATCH_UP and the wait_resource is 2:3:905856.\nYou need to improve system performance.\nSolution: You shrink the transaction log file.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "You should instead reduce the use of table variables and temporary tables.\nOr you could create additional tempdb files.\nNote: The following operations use tempdb extensively:\n* Repetitive create-and-drop operation of temporary tables (local or global).\n* Table variables that use tempdb for storage.\n* Etc.\nReference:\nhttps://docs.microsoft.com/en-US/troubleshoot/sql/performance/recommendations-\nreduce-allocation-contention"
  },
  {
    "number": "85",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have SQL Server 2019 on an Azure virtual machine.\nYou are troubleshooting performance issues for a query in a SQL Server\ninstance.\nTo gather more information, you query sys.dm_exec_requests and discover that\nthe wait type is PAGELATCH_UP and the wait_resource is 2:3:905856.\nYou need to improve system performance.\nSolution: You change the data file for the master database to autogrow by 10\npercent.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "You should instead reduce the use of table variables and temporary tables.\nOr you could create additional tempdb files\nNote: The following operations use tempdb extensively:"
  },
  {
    "number": "86",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have SQL Server 2019 on an Azure virtual machine.\nYou are troubleshooting performance issues for a query in a SQL Server\ninstance.\nTo gather more information, you query sys.dm_exec_requests and discover that\nthe wait type is PAGELATCH_UP and the wait_resource is 2:3:905856.\nYou need to improve system performance.\nSolution: You reduce the use of table variables and temporary tables.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The following operations use tempdb extensively:\n* Repetitive create-and-drop operation of temporary tables (local or global).\n* Table variables that use tempdb for storage.\n* Etc.\nReference:\nhttps://docs.microsoft.com/en-US/troubleshoot/sql/performance/recommendations-\nreduce-allocation-contention"
  },
  {
    "number": "87",
    "question": "You have an Azure SQL database named db1 on a server named server1.\nYou need to modify the MAXDOP settings for db1.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Connect to db1 and run the sp_configure command.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Connect to the master database of server1 and run the sp_configure command.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure the extended properties of db1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Modify the database scoped configuration of db1.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "If you determine that a MAXDOP setting different from the default is optimal for your\nAzure SQL Database workload, you can use the ALTER DATABASE\nSCOPED CONFIGURATION T-SQL statement.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/configure-max-degree-of-\nparallelism"
  },
  {
    "number": "88",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have SQL Server 2019 on an Azure virtual machine.\nYou are troubleshooting performance issues for a query in a SQL Server\ninstance.\nTo gather more information, you query sys.dm_exec_requests and discover that\nthe wait type is PAGELATCH_UP and the wait_resource is 2:3:905856.\nYou need to improve system performance.\nSolution: You create additional tempdb files.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 172 of 818\n173 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "To improve the concurrency of tempdb, try the following methods:\n* Increase the number of data files in tempdb to maximize disk bandwidth and reduce\ncontention in allocation structures.\n* Etc.\nNote: Symptoms -\nOn a server that is running Microsoft SQL Server, you notice severe blocking when the\nserver is experiencing a heavy load. Dynamic Management Views\n[sys.dm_exec_request or sys.dm_os_waiting_tasks] indicates that these requests or tasks\nare waiting for tempdb resources. Additionally, the wait type is\nPAGELATCH_UP, and the wait resource points to pages in Tempdb.\nReference:\nhttps://docs.microsoft.com/en-US/troubleshoot/sql/performance/recommendations-\nreduce-allocation-contention"
  },
  {
    "number": "89",
    "question": "You have SQL Server on an Azure virtual machine.\nYou need to add a 4-TB volume that meets the following requirements:\n✑ Maximizes IOPs\n✑ Uses premium solid state drives (SSDs)\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Attach two mirrored 4-TB SSDs.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Attach a stripe set that contains four 1-TB SSDs.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Attach a RAID-5 array that contains five 1-TB SSDs.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Attach a single 4-TB SSD.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "For more throughput, you can add additional data disks and use disk striping.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/storage-\nconfiguration?tabs=windows2016"
  },
  {
    "number": "90",
    "question": "You have an Azure SQL database named db1 on a server named server1.\nThe Intelligent Insights diagnostics log identifies that several tables are missing\nindexes.\nYou need to ensure that indexes are created for the tables.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Run the DBCC SQLPERF command.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Run the DBCC DBREINDEX command.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Modify the automatic tuning settings for db1.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Modify the Query Store settings for db1.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Automatic tuning is a fully managed intelligent performance service that uses built-in\nintelligence to continuously monitor queries executed on a database, and it\nautomatically improves their performance.\nAutomatic tuning for Azure SQL Database uses the CREATE INDEX, DROP INDEX, and FORCE\nLAST GOOD PLAN database advisor recommendations to optimize your database\nperformance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/automatic-tuning-overview"
  },
  {
    "number": "91",
    "question": "You have an Azure SQL managed instance named SQL1 and two Azure web apps\nnamed App1 and App2.\nYou need to limit the number of IOPs that App2 queries generate on SQL1.\nWhich two actions should you perform on SQL1? Each correct answer presents\npart of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Enable query optimizer fixes.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable Resource Governor.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Enable parameter sniffing.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a workload group. \nE) Configure In-memory OLTP.\nF) Run the Database Engine Tuning Advisor.\nG) Reduce the Max Degree of Parallelism value.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nSQL Server Resource Governor is a feature that you can use to manage SQL Server\nworkload and system resource consumption. Resource Governor enables you to specify\nlimits on the amount of CPU, physical I/O, and memory that incoming application\nrequests can use.\nThe following concept is fundamental to understanding and using Resource Governor:\n* Workload groups. A workload group serves as a container for session requests that\nhave similar classification criteria. A workload allows for aggregate monitoring of the\nsessions, and defines policies for the sessions. Each workload group is in a resource\npool. Two workload groups (internal and default) are created and mapped to their\ncorresponding resource pools when SQL Server is installed. Resource Governor also\nsupports user-defined workload groups.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/resource-governor/\nresource-governor?view=sql-server-ver15"
  },
  {
    "number": "92",
    "question": "You have an Azure SQL database named db1 on a server named server1.\nThe Intelligent Insights diagnostics log identifies queries that cause\nperformance issues due to tempDB contention.\nYou need to resolve the performance issues.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Implement memory-optimized tables.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Run the DBCC FLUSHPROCINDB command.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Replace the sequential index keys with nonsequential keys.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Run the DBCC DBREINDEX command.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "93",
    "question": "HOTSPOT -\nYou have an Azure subscription that contains an Azure SQL database.\nThe database fails to respond to queries in a timely manner.\nYou need to identify whether the issue relates to resource_semaphore waits.\nHow should you complete the Transact-SQL query? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nIf your top wait type is RESOURCE_SEMAHPORE and you don't have a high CPU usage\nissue, you may have a memory grant waiting issue.\nDetermine if a RESOURCE_SEMAHPORE wait is a top wait\nUse the following query to determine if a RESOURCE_SEMAHPORE wait is a top wait\nSELECT wait_type,\nSUM(wait_time) AS total_wait_time_ms\nFROM sys.dm_exec_requests AS req\nJOIN sys.dm_exec_sessions AS sess\nON req.session_id = sess.session_id\nWHERE is_user_process = 1 -\nGROUP BY wait_type -\nORDER BY SUM(wait_time) DESC;\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/monitoring-with-dmvs\nCommunity Discussion\n\n\nhttps://www.sqlshack.com/sql-server-performance-tuning-resource_semaphore-waits/\nAnswer is correct In addition, sys.query_store_query table does not have column\nsession_id required for the join in the query\nAnswer is correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "94",
    "question": "You have SQL Server 2019 on an Azure virtual machine that runs Windows Server\n2019. The virtual machine has 4 vCPUs and 28 GB of memory.\nYou scale up the virtual machine to 8 vCPUs and 64 GB of memory.\nYou need to reduce tempdb contention without negatively affecting server\nperformance.\nWhat is the number of secondary data files that you should configure for\ntempdb?",
    "options": [
      {
        "letter": "A",
        "text": "2",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "4",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "8",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "64",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The number of secondary data files depends on the number of (logical) processors on\nthe machine. As a general rule, if the number of logical processors is less than or equal\nto eight, use the same number of data files as logical processors. If the number of logical\nprocessors is greater than eight, use eight data files. Then if contention continues,\nincrease the number of data files by multiples of four until the contention decreases to\nacceptable levels, or make changes to the workload/code.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/databases/tempdb-\ndatabase"
  },
  {
    "number": "95",
    "question": "You receive numerous alerts from Azure Monitor for an Azure SQL Database\ninstance.\nYou need to reduce the number of alerts. You must only receive alerts if there is\na significant change in usage patterns for an extended period.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Set Threshold Sensitivity to High",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Set the Alert logic threshold to Dynamic",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Set the Alert logic threshold to Static",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Set Threshold Sensitivity to Low \nE) Set Force Plan to On\nPage 182 of 818\n183 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nB: Dynamic Thresholds continuously learns the data of the metric series and tries to\nmodel it using a set of algorithms and methods. It detects patterns in the data such as\nseasonality (Hourly / Daily / Weekly), and is able to handle noisy metrics (such as\nmachine CPU or memory) as well as metrics with low dispersion (such as availability and\nerror rate).\nD: Alert threshold sensitivity is a high-level concept that controls the amount of\ndeviation from metric behavior required to trigger an alert.\nLow ג€\" The thresholds will be loose with more distance from metric series pattern. An\nalert rule will only trigger on large deviations, resulting in fewer alerts.\nIncorrect Answers:\nA: High ג€\" The thresholds will be tight and close to the metric series pattern. An alert\nrule will be triggered on the smallest deviation, resulting in more alerts.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/platform/alerts-dynamic-\nthresholds"
  },
  {
    "number": "96",
    "question": "You have an Azure SQL database named sqldb1.\nYou need to minimize the amount of space by the data and log files of sqldb1.\nWhat should you run?",
    "options": [
      {
        "letter": "A",
        "text": "DBCC SHRINKDATABASE",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "sp_clean_db_free_space",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "sp_clean_db_file_free_space",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "DBCC SHRINKFILE",
        "is_correct": false
      },
      {
        "letter": "A",
        "text": "as it can cause performance issues and should only be used\nas a last resort. Instead, DBCC SHRINKFILE (option D) is the preferred option to minimize\nthe amount of space used by the data and log files of an Azure SQL database.\nPage 184 of 818\n185 Microsoft - DP-300 Practice Questions - SecExams.com\nDBCC SHRINKDATABASE shrinks all data and log files in a database using a single\ncommand. The command shrinks one data file at a time, which can take a long time for\nlarger databases. It also shrinks the log file, which is usually unnecessary because Azure\nSQL Database shrinks log files automatically as needed.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "DBCC SHRINKDATABASE shrinks the size of the data and log files in the specified\ndatabase.\nIncorrect Answers:\nD: To shrink one data or log file at a time for a specific database, execute the DBCC\nSHRINKFILE command.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-\nshrinkdatabase-transact-sql"
  },
  {
    "number": "97",
    "question": "You have an Azure SQL Database server named sqlsrv1 that hosts 10 Azure SQL\ndatabases.\nThe databases perform slower than expected.\nYou need to identify whether the performance issue relates to the use of\ntempdb by Azure SQL databases in sqlsrv1.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Run Query Store-based queries",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Review information provided by SQL Server Profiler-based traces",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Review information provided by Query Performance Insight",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Run dynamic management view-based queries",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Microsoft Azure SQL Database and Azure SQL Managed Instance enable a subset of\ndynamic management views to diagnose performance problems, which might be caused\nby blocked or long-running queries, resource bottlenecks, poor query plans, and so on.\nThis include edentifying tempdb performance issues:\nWhen identifying IO performance issues, the top wait types associated with tempdb\nissues is PAGELATCH_* (not PAGEIOLATCH_*).\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/monitoring-with-dmvs"
  },
  {
    "number": "98",
    "question": "DRAG DROP -\nYou are building an Azure virtual machine.\nYou allocate two 1-TiB, P30 premium storage disks to the virtual machine. Each\ndisk provides 5,000 IOPS.\nYou plan to migrate an on-premises instance of Microsoft SQL Server to the\nvirtual machine. The instance has a database that contains a 1.2-TiB data file.\nThe database requires 10,000 IOPS.\nYou need to configure storage for the virtual machine to support the database.\nWhich three objects should you create in sequence? To answer, move the\nappropriate objects from the list of objects to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nFollow these same steps to create striped virtual disk:\n✑ Create Log Storage Pool.\n✑ Create Virtual Disk\n✑ Create Volume\nBox 1: a storage pool -\nBox 2: a virtual disk that uses stripe layout\nDisk Striping: Use multiple disks and stripe them together to get a combined higher IOPS\nand Throughput limit. The combined limit per VM should be higher than the combined\nlimits of attached premium disks.\nBox 3: a volume -\nReference:\nhttps://hanu.com/hanu-how-to-striping-of-disks-for-azure-sql-server/\nCommunity Discussion\nStorage Layout options Simple, Mirror Or Partity. Create Virtual Disk with simple layout is\ncorrect.\nAnswer given is correct. You can stripe 2 premium data disk to get 10000 IOPS. https://\ntechcommunity.microsoft.com/t5/sql-server/optimize-oltp-performance-with-sql-server-\non-azure-vm/ba-p/916794\n\n\n-> a storage pool -> a virtual disk that uses the simple layout -> a volume There are only\nthree layouts while creating virtual disks on a storage pool: Simple - Data is striped\nacross physical disks, maximizing capacity and throughput Mirror - Data is striped across\nphysical disks, creating two or three copies of data Parity - Data and parity information is\nstriped across physical disks, increasing reliability\nThis stripe is not about creating redundancy like RAID 5. This is about combining the\ntotal IOPS of the two virtual disk into a single volume with 10,000 IOPS. Redundancy is\nalready provided by the Azure infrastructure hosting the virtual disk. It runs at 11 9s.\nThis stripe is not about creating redundancy like RAID 5. This is about combining the\ntotal IOPS of the two virtual disk into a single volume with 10,000 IOPS. Redundancy is\nalready provided by the Azure infrastructure hosting the virtual disk. It runs at 11 9s.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "99",
    "question": "You have an Azure subscription that contains an Azure SQL database. The\ndatabase contains a table named tablet that uses partitioned columnstores.\nYou need to configure table1 to meet the following requirements:\n• Each partition must be compressed.\n• The compression ratio must be maximized.\n• You must be able to index the compressed data.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "page compression",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "columnstore compression",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "GZIP compression",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "columnstore archival compression",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "100",
    "question": "You have an Azure subscription linked to an Azure Active Directory (Azure AD)\ntenant. The subscription contains 10 virtual machines that run Windows Server\n2019 and host Microsoft SQL Server 2019 instances.\nYou need to ensure that you can manage the SQL Server instances by using a\nsingle user account.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Enable a user-assigned managed identity on each virtual machine.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Deploy an Azure Active Directory Domain Services (Azure AD DS) domain and join the virtual\nmachines to the domain.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Enable a system-assigned managed identity on each virtual machine.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Join the virtual machines to the Azure AD tenant.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "101",
    "question": "DRAG DROP\n-\nYou have an Azure subscription.\nYou plan to deploy a new Azure virtual machine that will host a Microsoft SQL\nServer instance.\nYou need to configure the disks on the virtual machine. The solution must meet\nthe following requirements:\n• Minimize latency for transaction logs.\n• Minimize the impact on IO throughput of the virtual machine.\nWhich type of disk should you use for each workload? To answer, drag the\nappropriate disk types to the correct workloads. Each disk type may be used\nonce, more than once, or not at all. You may need to drag the split bar between\npanes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nAccording this article: https://learn.microsoft.com/en-us/azure/azure-sql/virtual-\nmachines/windows/performance-guidelines-best-practices-storage?view=azuresql \"For\nthe log drive ... If submillisecond storage latency is required, use Azure ultra disk\" - so\nthe answer is correct \"Place tempdb on the local ephemeral SSD (default D:\\) drive for\nmost SQL Server workloads\" - thus the answer should be perhaps \"local\" ?\nYes I agree, the tempdb should be on a local drive\ncorrect If possible, use Write Acceleration over ultra disks for the transaction log disk. For\nVMs that don't support Write Acceleration but require low latency to the transaction log,\nuse Azure ultra disks.\nwhich one is correct option? I think, Tempdb should be local and transaction log should\nbe premium\nTempdb should be local. Not sure about the other one, but I guess if we are minimising\nlatency, it must be Ultra Disk?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "102",
    "question": "HOTSPOT\n-\nYou have an Azure SQL database named DB1 that contains a table named\nOrders. The Orders table contains a row for each sales order. Each sales order\nincludes the name of the user who placed the order.\nYou need to implement row-level security (RLS). The solution must ensure that\nthe users can view only their respective sales orders.\nWhat should you include in the solution? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is correct. Row-level security is implemented via CREATE SECURITY POLICY and\nusing in-table function\nAnswer is correct. Implement RLS by using the CREATE SECURITY POLICY Transact-SQL\nstatement, and predicates created as inline table-valued functions. https://\nlearn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?\nview=sql-server-ver16",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "103",
    "question": "DRAG DROP\n-\nYou have an Azure subscription that contains an Azure SQL database named\nSQLDb1. SQLDb1 contains a table named Table1.\nYou plan to deploy an Azure web app named webapp1 that will export the rows\nin Table1 that have changed.\nYou need to ensure that webapp1 can identify the changes to Table1. The\nsolution must meet the following requirements:\n• Minimize compute times.\n• Minimize storage.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nThe answer is correct. To track changes, change tracking must first be enabled for the\ndatabase and then enabled for the tables that you want to track within that database. 1)\nALTER DATABASE Sandbox1 SET CHANGE_TRACKING = ON 2) ALTER TABLE Persons ENABLE\nCHANGE_TRACKING 3) run the CHANGETABLE() function\ndid a small lab, got error msg when enabling Change_Tracking on table before enabling\non database level. ====Change tracking must be enabled on database 'Sandbox1' before\nit can be enabled on table 'Persons'.====\ndid a small lab, got error msg when enabling Change_Tracking on table before enabling\non database level. ====Change tracking must be enabled on database 'Sandbox1' before\nit can be enabled on table 'Persons'.====\ndid a small lab, got error msg when enabling Change_Tracking on table before enabling\non database level. ====Change tracking must be enabled on database 'Sandbox1' before\nit can be enabled on table 'Persons'.====\ndid a small lab, got error msg when enabling Change_Tracking on table before enabling\non database level. ====Change tracking must be enabled on database 'Sandbox1' before\nit can be enabled on table 'Persons'.====",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "104",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that is linked to an Azure Active Directory (Azure\nAD) tenant named contoso.com The subscription contains an Azure SQL\ndatabase named SQL1 and an Azure web app named app1. App1 has the\nmanaged identity feature enabled.\nYou need to create a new database user for app1.\nHow should you complete the Transact-SQL statement? To answer select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is correct. In T-SQL, just point to App name not managed identity. https://\nlearn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/\ntutorial-windows-vm-access-sql\nThe syntax: CREATE USER [App1] FROM EXTERNAL PROVIDER\nThe syntax: CREATE USER [App1] FROM EXTERNAL PROVIDER\nhussam\nThanks for clarification.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "105",
    "question": "DRAG DROP\n-\nYou have an Azure subscription that contains an Azure SQL database named\nDB1.\nYou plan to perform a classification scan of DB1 by using Azure Purview.\nYou need to ensure that you can register DB1.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\nCommunity Discussion\nCreate a Purview account. Modify the Access control (IAM) settings for DB1. From the\nPurview Studio, create a scan rule set.\nafter some further reading, i think the provided answer seems right. the question is not\nhow to perform scan. it is about what need to be done to register for the scanning. If you\n\n\ncarefully read the content in this link, you will see the answer make more sense. https://\nlearn.microsoft.com/en-us/azure/purview/register-scan-azure-sql-database?tabs=sql-\nauthentication\nSeem you are right and I was wrong. Only I would change the sequence of the steps 2\nand 3. First to create a managed identity => then to modify IAM of the database\nsystem managed identity CANNOT be created by the user, so it's not valid.\nsystem managed identity CANNOT be created by the user, so it's not valid.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "106",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains a group named Group1 and an\nAzure SQL managed instance that hosts a database named DB1.\nYou need to ensure that Group1 has read access to new tables created in DB1.\nThe solution must use the principle of least privilege.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nIt's an incorrect syntax to use [database]::[table]. The correct answer is [schema]::[table].\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/grant-schema-permissions-\ntransact-sql?view=sql-server-ver16\nIt's an incorrect syntax to use [database]::[table]. The correct answer is [schema]::[table].\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/grant-schema-permissions-\ntransact-sql?view=sql-server-ver16\nThe question is confusing. It states \"You need to ensure that Group1 has read access to\nnew tables created in DB1. The solution must use the principle of least privilege.\" If you\ngrant to schema, then it will be specific to schema only. How about if there are two\nschema in the database like for example dbo and user1? So, it should be GRANT SELECT\nON DATABASE::TABLE...",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "107",
    "question": "You have an instance of SQL Server on Azure Virtual Machines named VM1.\nYou plan to schedule a SQL Server Agent job that will rebuild indexes of the\ndatabases hosted on VM1.\nYou need to configure the account that will be used by the agent. The solution\nmust use the principle of least privilege.\nWhich operating system user right should you assign to the account?",
    "options": [
      {
        "letter": "A",
        "text": "Increase scheduling priority",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Log on as a service",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Profile system performance",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Log on as a batch job",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "108",
    "question": "You have an Azure subscription that contains an instance of SQL Server on an\nAzure virtual machine named SQLVM1 and a user named User1. SQLVM1 hosts a\ndatabase named DB1.\nYou need to ensure that User1 can create a scheduled task to perform a full\nbackup of DB1. The solution must use the principle of least privilege.\nWhich built-in database role should you assign to User1?",
    "options": [
      {
        "letter": "A",
        "text": "db_owner",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQLAgentReaderRole",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQLAgentUserRole",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQLAgentOperatorRole",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "109",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains a logical SQL server. The server\nhosts two databases named db1 and db2 and an Azure AD service principal\nnamed app1.\nYou need to ensure that app1 can access db1. The solution must use the\nprinciple of least privilege.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nActually, rereading the question it says that the logical SQL server hosts the user app1,\nthis means tha app1 has a login on the server. This in turn means that the best way to\ngive app1 permissions to db1 is to create user app1 from login app1\n\"Create the service principal user in Azure SQL Database - Create the user AppSP in the\nSQL Database using the following T-SQL command: CREATE USER [AppSP] FROM EXTERNAL\nPROVIDER GO \" Logical server is a default container for Azure SQL Database. https://\nlearn.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-service-\nprincipal-tutorial?view=azuresql\nAnswer correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "110",
    "question": "Your on-premises network contains a server that hosts a 60-TB database named\nDB1. The network has a 10-Mbps internet connection.\nYou need to migrate DB1 to Azure. The solution must minimize how long it takes\nto migrate the database.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Migrate",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Data Box",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Database Migration Service\nPage 208 of 818\n209 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Data Migration Assistant (DMA)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "looks right. Database bigger than 40TB, better to use Azure Data Box\nSelected Answer: B\nThe Microsoft Azure Data Box cloud solution lets you send terabytes of data into and out\nof Azure in a quick, inexpensive, and reliable way. The secure data transfer is accelerated\nby shipping you a proprietary Data Box storage device. Each storage device has a\nmaximum usable storage capacity of 80 TB and is transported to your datacenter\nthrough a regional carrier. The device has a rugged casing to protect and secure data\nduring the transit.\nSelected Answer: B\nYes The only practical solution for a db that size\nhttps://learn.microsoft.com/en-us/azure/databox/data-box-overview\nhttps://learn.microsoft.com/en-us/azure/databox/data-box-overview\nPage 209 of 818\n210 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "111",
    "question": "HOTSPOT -\nYou have an Azure SQL logical server.\nYou run the following script.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI think it's NO - as we are creating so called updatable ledger table (i.e. not append-only)\n, we can also delete and update rows. NO - the CREATE DATABASE statement does not\ncontain \"WITH LEDGER = ON\" clause thus it can contain non-ledger tables NO - when the\nGENERATED ALWAYS columns are not specified in the CREATE statement they'll be created\nautomatically\nI think answer is NNN:If any of the required generated always columns isn't defined in\nthe CREATE TABLE statement and the statement includes LEDGER = ON, the system will\nautomatically attempt to add the column... https://learn.microsoft.com/en-us/sql/t-sql/\nstatements/create-table-transact-sql?view=sql-server-ver16#generate-always-columns\nAnswer is :NNN\nanswer not readable, please fix\nAnswer is : NO NO Yes",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "112",
    "question": "HOTSPOT\n-\nYou need to use an Azure Resource Manager (ARM) template to deploy an Azure\nvirtual machine that will host a Microsoft SQL Server instance. The solution\nmust maximize disk I/O performance for the SQL Server database and log files.\nHow should you complete the template? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nI think it should be opposite, i.e. Read-Only and None. See https://learn.microsoft.com/\nen-us/azure/azure-sql/virtual-machines/windows/performance-guidelines-best-\npractices-storage?view=azuresql#caching We need to set 'Read-Only' caching for data\ndisks and 'None' for the log disk. There is a loop (copy) in the ARM template, it evaluates\nwhether current disk ID is greater than sum of data disks. If yes then it's the log disk, so\nassign None, otherwise the value in the variable (i.e. Read-Only). Similar example is here:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/create-\nsql-vm-resource-manager-template\nlicna is correct\nlicna is correct\nNone ReadOnly\n\"caching\": \"[if(greaterOrEquals(range(0, add(parameters('sqlDataDisksCount'),\nparameters('sqlLogDisksCount')))[range(0, length(range(0,\nadd(parameters('sqlDataDisksCount'), parameters('sqlLogDisksCount')))))\n[copyIndex('dataDisks')]], parameters('sqlDataDisksCount')), 'None',\nvariables('dataDisks').caching)]\", The expression is checking if the current disk being\nprocessed (indicated by copyIndex('dataDisks')) is a log disk or a data disk. Here's how it\n\n\nworks: Generate a Range: range(0, add(parameters('sqlDataDisksCount'),\nparameters('sqlLogDisksCount'))) creates an array from 0 to the total number of disks\n(data + log disks). Get Current Disk Index: range(0, length(range(0,\nadd(parameters('sqlDataDisksCount'), parameters('sqlLogDisksCount')))))\n[copyIndex('dataDisks')] retrieves the current disk index in the loop. Check Disk Type:\ngreaterOrEquals(..., parameters('sqlDataDisksCount')) checks if the current disk index is\ngreater than or equal to the number of data disks. If true, it means the current disk is a\nlog disk. Set Caching Mode: If the current disk is a log disk, set caching to None. If the\ncurrent disk is a data disk, use the caching setting from variables('dataDisks').caching. So\nvariable at the top of image should be READ-ONLY and the missing bit from the\nstatement is NONE",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "113",
    "question": "You have an Azure SQL managed instance named MI1.\nYou need to implement automatic tuning for the databases of MI1.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use The REST API to call the patch operation and modify the AutomaticTuningServerMode\nproperty.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From the Azure portal, configure automatic tuning.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use Transact-SQL to enable the FORCE_LAST_GOOD_PLAN option.",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "114",
    "question": "You have an Azure subscription that contains an Azure SQL database named\ndb1.\nYou need to implement SQL insights for db1.\nWhich two resources should you create first? Each correct answer presents part\nof the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "a storage account",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a virtual machine",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "an Azure logic app",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "an Azure function\nE) a Log Analytics workspace",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "E"
  },
  {
    "number": "115",
    "question": "You have an Azure subscription that contains the resources shown in the\nfollowing table.\nApp1 experiences transient connection errors and timeouts when it attempts to\naccess db1 after extended periods of inactivity.\nYou need to modify db1 to resolve the issues experienced by App1 as soon as\npossible, without considering immediate costs.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable automatic tuning for db1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Increase the number of vCores allocated to db1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Decrease the auto-pause delay for db1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Disable auto-pause delay for db1.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "116",
    "question": "HOTSPOT\n-\nYou have an Azure SQL database named DB1 that contains a table named Table1.\nYou run a query to load data into Table1.\nThe performance metrics of Table1 during the load operation are shown in the\nfollowing exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nYou are loading data into Table1, which means there is some inserts going on. Note that\ninsert statements is also considered a query. Not sure why the first answer is perform\nquery tuning. It should be scale resource in order to reduce the time insert data into\nTable1.\nI was thinking the same, In my opinion also can be scale resources for the first one.\n\n\nI was thinking the same, In my opinion also can be scale resources for the first one.\nQuery tuning is correct, make sure insert is efficient i.e. load in batches from In-Memory\nnon durable table is faster.\ncorrect.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "117",
    "question": "DRAG DROP\n-\nYou have a database named db1.\nThe log for db1 contains the following entry.\nYou need to ensure that db1 can process transactions.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nmy take would be: 1.remove db1 from ag, 2. backup t.log , 3 re-add db1 to ag . shrinking\nlog shouldn't be necessary since log backup will reduce its size\nBack up the transaction log file. Remove db1 from the availability group. Shrink the\ntransaction log file.\nBased on amazonalex with https://www.sqlshack.com/sql-server-transaction-log-\nbackup-truncate-and-shrink-operations/, my answer is Remove db1 from the Availability\nGroup Backup the Transaction Log File Add db1 from the Availability Group\nFirst need to remove DB from AG then sometime its log file auto clean from full to 90%\nspace then shrink log file and readd into AG\nFirst need to remove DB from AG then sometime its log file auto clean from full to 90%\nspace then shrink log file and readd into AG",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "118",
    "question": "You have the following resources:\n• 15 SQL Server on Azure Virtual Machines instances\n• 20 Azure SQL databases\nYou need to recommend a solution to centrally monitor the resources for\nsecurity vulnerabilities.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "database audits",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Microsoft Defender",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL insights",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure SQL Auditing",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "119",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains an instance of SQL Server on\nAzure Virtual Machines named SQLVM1 and a user named User1. SQLVM1 hosts a\ndatabase named DB1.\nYou need to ensure that User1 can perform the following tasks on DB1:\n• Create jobs.\n• View all jobs.\n• Modify, delete, and disable the jobs the user created.\nThe solution must use the principle of least privilege.\nWhich built-in database role should you assign to User1, and where is the role\ndefined? To answer, select the appropriate options in the answer area.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nLocation: msdb Role: SQLAgentUesrRole\nLocation: msdb Role: SQLAgentOperatorRole • Create jobs. • View all jobs. • Modify, delete,\nand disable the jobs the user created.\nBased on the table here https://www.youtube.com/watch?v=rodn5MSeiuY at 0m:40s and\nusing the principal of least privilege, the answer should be SQLAgentReaderRole.\nSQLAgentUserRole cannot view all jobs, it can only view own jobs. Although both\nSQLAgentReaderRole and SQLAgentOperatorRole can create/modify/delete own jobs, the\nnext thing to consider here is that the user must be able to disable its own job.\nSQLAgentReaderRole can disable own job, where SQLAgentOperatorRole can disable all\njobs. Using the principal of least privilege, the answer is SQLAgentReaderRole.\nThe answer is correct. SQLAgentUesrRole can not view all jobs. Gosan is right!\nLocaltion:msdb Role:SQLAgentReaderRole (Because question request \"view all jobs\".\nSQLAgentUserRole only view owned jobs)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "120",
    "question": "You have an Azure subscription that contains an Azure SQL managed instance\nnamed SQLMI1 and a Log Analytics workspace named Workspace1.\nYou need to collect performance metrics for SQLMI1 and stream the metrics to\nWorkspace.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Create a private endpoint connection on SQLMI1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure Azure SQL Analytics to use Workspace1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Modify the Compute + storage settings for SQLMI1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Modify the diagnostic settings for SQLMI1.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "121",
    "question": "You have an Azure SQL Database elastic pool that contains 10 databases.\nYou receive the following alert.\nMsg 1132, Level 16, state 1, Line 1\nThe elastic pool has reached its storage limit. The storage used for the elastic\npool cannot exceed (76800) MBs.\nYou need to resolve the alert. The solution must minimize administrative effort.\nWhich three actions can you perform? Each correct answer presents a complete\nsolution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Increase the maximum storage of the elastic pool.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Delete data from a database.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Remove a database from the pool.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Enable data compression.\nE) Shrink individual databases.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "CE"
  },
  {
    "number": "122",
    "question": "You have an Azure subscription.\nYou need to deploy a new Azure SQL database by using Azure Command-Line\nInterface (CLI).\nWhich three parameters are required?",
    "options": [
      {
        "letter": "A",
        "text": "--name, --edition, and --capacity",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "--name, --tier, and --min-capacity",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "--name, --resource-group, and --server",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "--name, --licence-type, and --capacity",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "123",
    "question": "HOTSPOT\n-\nYou have an Azure subscription.\nYou plan to migrate 10 on-premises Microsoft SQL Server instances to Azure.\nYou need to ensure that the migrated environment can be managed by using\nmultiserver administration and supports master/target (MSX/TSX) jobs.\nThe solution must minimize administrative effort.\nWhich SQL deployment options should you select as the master server (MSX)\nand the target server (TSX)? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nI will say \"Virtual Machines\" for both, as said here https://learn.microsoft.com/en-us/\nazure/azure-sql/managed-instance/transact-sql-tsql-differences-sql-server?\nview=azuresql-mi#sql-server-agent : \"The Multi Server Administration feature for master/\ntarget (MSX/TSX) jobs are not supported.\" So, which answer for the test ? :(\nBut what I see is \"The Multi Server Administration feature is not supported on Azure SQL\nManaged Instance.\"\nBut what I see is \"The Multi Server Administration feature is not supported on Azure SQL\nManaged Instance.\"\nOn Azure SQL Managed Instance, The Multi Server Administration feature for master/\ntarget (MSX/TSX) jobs are not supported. as per info from below Microsoft URL. SO, I\nthink answer is Azure Virtual machines for MSX and TSX setup. https://\nlearn.microsoft.com/en-us/sql/ssms/agent/create-a-multiserver-environment?view=sql-\nserver-ver16 https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/\ntransact-sql-tsql-differences-sql-server?view=azuresql\nThe Master Server (MSX) and Target Server (TSX) are SQL Server concepts used for\nmanaging multiple servers at once. The Master Server (MSX) is the central server that\nmanages one or more Target Servers (TSX). By using MSX, you can manage multiple SQL\nServer instances as if they were a single entity. MSX simplifies the administration tasks of\n\n\nmanaging multiple servers by allowing you to execute tasks on multiple servers at once,\nsuch as backing up databases or running queries. The Target Server (TSX) is a SQL Server\ninstance that is managed by the Master Server (MSX). TSX instances can be located on\nthe same physical server as the MSX instance or on a different server. MSX/TSX can also\nbe used for managing SQL Server Agent jobs, where you can define jobs at the master\nserver and then distribute them to the target servers. This feature helps in centrally\nmanaging and scheduling SQL Server Agent jobs across multiple servers.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "124",
    "question": "You have two on-premises Microsoft SQL Server 2019 instances named SQL1 and\nSQL2.\nYou need to migrate the databases hosted on SQL1 to Azure. The solution must\nmeet the following requirements:\n• The service that hosts the migrated databases must be able to communicate\nwith SQL2 by using linked server connections.\n• Administrative effort must be minimized.\nWhat should you use to host the databases?",
    "options": [
      {
        "letter": "A",
        "text": "a single Azure SQL database",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQL Server on Azure Virtual Machines",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Managed Instance",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an Azure SQL Database elastic pool",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "125",
    "question": "HOTSPOT\n-\nYou have an on-premises Microsoft SQL Server 2016 instance that hosts a\ndatabase named db1. You have an Azure subscription that contains an Azure\nSQL managed instance named MI1.\nYou plan to perform an online migration of db1 to MI1 by using Azure Database\nMigration Service.\nYou need to create the backups for the migration. The solution must minimize\nthe number of backup files created.\nWhich type of backups should you create, and how should you store the\nbackups? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nThe given answer is correct as Azure Database Migration Service cannot use backups\nappended to a single file. Reference: https://www.secexams.com/exams/microsoft/\ndp-300/view/10/\nCorrect reference link: https://learn.microsoft.com/en-us/azure/dms/known-issues-\nazure-sql-db-managed-instance-online\nCorrect reference link: https://learn.microsoft.com/en-us/azure/dms/known-issues-\nazure-sql-db-managed-instance-online",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "126",
    "question": "You have a SQL Server on Azure Virtual Machines instance named SQLVM1 that\nwas deployed by using an Azure Marketplace SQL Server 2019 Enterprise image.\nYou need to change the Microsoft SQL Server instance on SQLVM1 to the\nStandard edition. The solution must ensure licensing compliance.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "From the SQL Server Installation Center on SQLVM1, run the Edition Upgrade wizard.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From SQLVM1, uninstall the SQL Server instance.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "From the SQL Server Installation Center on SQLVM1, run the Repair wizard.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "From the Azure portal, reconfigure SQLVM1.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "127",
    "question": "Your on-premises network contains a Microsoft SQL Server 2016 server that\nhosts a database named db1.\nYou have an Azure subscription.\nYou plan to migrate db1 to an Azure SQL managed instance.\nYou need to create the SQL managed instance. The solution must minimize the\ndisk latency of the instance.\nWhich service tier should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Business Critical",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Hyperscale",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "General Purpose",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Premium",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "128",
    "question": "You have an Azure subscription.\nYou need to deploy an Azure SQL database. The solution must meet the\nfollowing requirements:\n• Dynamically scale CPU resources.\n• Ensure that the database can be paused to reduce costs.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "the Business Critical service tier",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "the serverless compute tier",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "an elastic pool",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "the General Purpose service tier",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "129",
    "question": "HOTSPOT\n-\nYou have an Azure subscription.\nYou need to deploy an Azure SQL managed instance that meets the following\nrequirements:\n• Optimize latency.\n• Maximize the memory-to-vCore ratio.\nWhich service tier and hardware generation should you use? To answer, select\nthe appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nthe Answer given is correct Business Critical service tier on Premium Series Memory-\nOptimized The Business Critical service tier is built for applications with high I/O\nrequirements. It offers the highest resilience to failures using several isolated replicas.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-\ninstance-paas-overview?view=azuresql#business-critical-service-tier\nGiven Answer is correct Business critical optimises latency\nService tier: Business Critical Hardware generation: Premium-series - memory optimized\nHere's why these selections meet the requirements: Business Critical tier: This service\ntier is optimized for latency-sensitive workloads and provides the lowest latency due to\nits use of local SSD storage. It also offers a higher memory-to-vCore ratio compared to\nother tiers. Premium-series - memory optimized: This hardware generation provides the\nmost memory per vCore, which maximizes the memory-to-vCore ratio. This is suitable for\nhigh-performance database workloads that require more memory for in-memory\noperations.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "130",
    "question": "You have an Azure SQL database named DB1.\nYou need to encrypt DB1. The solution must meet the following requirements:\n• Encrypt data in motion.\n• Support comparison operators.\n• Provide randomized encryption.\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Always Encrypted with secure enclaves",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Always Encrypted",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "column-level encryption",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Transparent Data Encryption (TDE)",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "131",
    "question": "You have an instance of SQL Server on Azure Virtual Machines.\nYou need to ensure that a user named User1 can configure proxy accounts for\nSQL Server Agent jobs. The solution must use the principle of least privilege.\nWhich role should you assign to User1?",
    "options": [
      {
        "letter": "A",
        "text": "sysadmin",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "SQLAgentUserRole",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQLAgentReaderRole",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQLAgentOperatorRole",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "132",
    "question": "You have an Azure SQL database named DB1. DB1 has a table named Table1 that\ncontains the following columns.\nYou plan to enable Always Encrypted for Table1.\nWhich two columns support encryption? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Column1",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Column2",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Column3",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Column4 \nPage 247 of 818\n248 Microsoft - DP-300 Practice Questions - SecExams.com\nE) Column5",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "D"
  },
  {
    "number": "133",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains an Azure SQL managed instance\nnamed SQL1.\nYou use the Microsoft Power BI service.\nYou need to configure connectivity from the Power BI service to SQL1. The\nsolution must ensure that only the Power BI service can initiate connections to\nSQL1.\nWhich type of endpoint should you use for SQL1, and what should you use to\ncontrol the connections to SQL1? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nTo configure connectivity from the Power BI service to an Azure SQL managed instance,\nyou should use a Public endpoint for SQL1 1. You can control the connections to SQL1 by\nusing a Network Security Group (NSG) 1. Therefore, the correct answer is Endpoint type:\nPublic and Control connections by using: A network security group (NSG).\nThe answer is correct\nreference: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-\nservice-tags?source=recommendations The following configurations are necessary to\nsuccessfully enable the endpoints for use in the Power BI service: 1. Enable a public\nendpoint in the SQL Managed Instance. 2. Create a Network Security Group rule to allow\ninbound traffic. 3. Enter the credentials in Power BI. So the answer is correct.\nhttps://www.datahai.co.uk/power-bi/connecting-power-bi-to-azure-sql-database-using-\nprivate-endpoints/ https://community.fabric.microsoft.com/t5/Power-Query/Azure-SQL-\nServer-Managed-Instance/m-p/377418#M16216 https://community.powerbi.com/t5/\nPower-Query/Azure-SQL-Server-Managed-Instance/m-p/376246 https://\nwww.mssqltips.com/sqlservertip/5953/create-power-bi-connection-to-azure-sql-\ndatabase/",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "134",
    "question": "HOTSPOT\n-\nYou have an Azure SQL database named DB1 in the General Purpose service tier.\nYou need to monitor DB1 by using SQL Insights.\nWhat should you include in the solution? To answer, select the appropriate\noptions in the\nanswer area.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nA virtual machine and Log Analytics workspace\nseem to be correct answer, SQL Insights stores its data in one or more Log Analytics\nworkspaces. https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-insights-\noverview?view=azuresql https://learn.microsoft.com/en-us/azure/azure-sql/database/\nsql-insights-enable?view=azuresql#create-log-analytics-workspace\nSQL Insights uses dedicated monitoring virtual machines to remotely collect data from\nyour SQL resources. Each monitoring virtual machine has the Azure Monitor agent and\nthe Workload Insights (WLI) extension installed. The given answer is correct\nA VM cannot collect the data, the Azure Monitor agent does. Indeed a poorly worded\nquestion.\nThe store part is correct like Pravana said. but the collect is wrong From the same link\nPravana provide : You will need to create one or more Azure virtual machines that will be\nused to collect data to monitor SQL.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "135",
    "question": "You have a Microsoft SQL Server 2017 server.\nYou need to migrate the server to Azure. The solution must meet the following\nrequirements:\n• Ensure that the latest version of SQL Server is used.\n• Support the SQL Server Agent service.\n• Minimize administrative effort.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "an Azure SQL Database elastic pool",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL Database",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL Server on Azure Virtual Machines",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure SQL Managed Instance",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "136",
    "question": "HOTSPOT\n-\nYou have a Microsoft SQL Server 2017 server that hosts five databases.\nYou plan to migrate the databases to Azure.\nYou need to recommend a solution that meets the following requirements:\n• Automatically scales compute based on the workload demand\n• Provides per-second billing\nWhat should you include in the recommendation? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nWe have five bds, SQL Database in serverless compute tier is for single databases.... so\nwe would need no a single serverless SQL Databases... but five. We need to automatically\nscale compute based on workload, and be billed by the second...For five databses we\nwould create an elastic pool ( with fixed min and max resources for automatic compute\nscaling of the bds), using vCore model, with provisioned compute tier so we are charged\nby the second(DTU model charges by hour). vCore model has General Purpose, Business\nCritical, and Hyperscale.... we would use GEneral purpose. Serverless compute tier es for\nsingle databases... so we would need five servers SQL Databases.... not \"a single Azure\nSQL databases in the serverless compute tier\".\nhttps://azure.microsoft.com/en-us/pricing/details/azure-sql-database/single/\nServerless compute tier also mentioned here.\nIn the answer area, you should choose: Azure service: A single Azure SQL database in the\nserverless compute tier Service tier: General Purpose Here’s the explanation: A single\nAzure SQL database in the serverless compute tier: This service automatically scales\ncompute based on the workload demand, which is one of your requirements.\nAdditionally, the serverless tier offers per-second billing, allowing you to pay only for the\ncompute you use. General Purpose service tier: This tier is typically available with the\nserverless compute model and offers a balanced and scalable compute and storage\noptions suitable for most business workloads.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?\nview=azuresql-db&tabs=general-purpose Serverless is a compute tier for single\ndatabases in Azure SQL Database that automatically scales compute based on workload\ndemand and bills for the amount of compute used per second.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "137",
    "question": "You have an on-premises Microsoft SQL Server 2019 database named SQL1 that\nuses merge replication.\nYou need to migrate SQL1 to Azure.\nWhich service should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Edge",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL Database",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL Server on Azure Virtual Machines",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure SQL Managed Instance",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "138",
    "question": "You have an on-premises datacenter that contains a 2-TB Microsoft SQL Server\n2019 database named DB1.\nYou need to recommend a solution to migrate DB1 to an Azure SQL managed\ninstance. The solution must minimize downtime and administrative effort.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "Log Replay Service (LRS)",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "log shipping",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "transactional replication",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQL Data Sync",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "139",
    "question": "You have an Azure subscription.\nYou plan to deploy an instance of SQL Server on Azure Virtual Machines that\nsupports Write Accelerator.\nWhich virtual machine series should you use?",
    "options": [
      {
        "letter": "A",
        "text": "E-series",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "G-series",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "H-series",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "M-series",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "140",
    "question": "You have an on-premises Microsoft SQL Server 2019 instance that hosts a\ndatabase named DB1.\nYou have an Azure subscription that contains an Azure SQL database named\nSQLDB1.\nYou need to replicate DB1 to SQLDB1.\nWhich type of replication should you use?",
    "options": [
      {
        "letter": "A",
        "text": "transactional",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "peer-to-peer",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "snapshot",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "merge",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "141",
    "question": "HOTSPOT\n-\nYou have two on-premises servers that run Windows Server 2019 and host a\nMicrosoft SQL server 2017 Always On availability group named AG1. AG1 contains\na single database named DB1.\nYou have an Azure subscription. The subscription contains a virtual machine\nnamed VM1 that runs Linux.\nYou need to migrate DB1 to a SQL Server 2019 instance on VM1. The solution\nmust minimize the downtime of DB1 during the migration.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one-point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nDisregard my previous discussion, Answer that makes sense here is 1) Create a SQL 2019\nAG on VM1 and then use 2) Distributed availability group for actual cutover/migration\nAdd a secondary replica to Ag1 and Azure migrate are correct answers basing on sql\nversion supportability in distributed AG in below url https://learn.microsoft.com/en-us/\nsql/database-engine/availability-groups/windows/distributed-availability-groups?\nview=sql-server-ver16\nAnd upgrade of the on-premises sql server?\nTo prepare for the migration, you should upgrade the on-premises SQL servers to SQL\nServer 2019 and add a secondary replica to AG1. This will ensure that the on-premises\nservers are compatible with the SQL Server 2019 instance on VM1 and that AG1 can\nsupport cross-platform migration. To perform the migration, you should use a distributed\navailability group. This is a feature that allows you to create a distributed availability\ngroup that spans two availability groups, one on Windows and one on Linux. You can\nthen perform a manual failover of the distributed availability group to VM1 and remove\nthe on-premises replicas. This will minimize the downtime of DB1 during the migration.\nStep 1: Create a new SQL Server 2019 instance on VM1. You can create a new SQL Server\n2019 instance on VM1 by using the Azure portal, the Azure CLI, or the Azure PowerShell\nmodules. Step 2: Use the Azure SQL Migration extension for Azure Data Studio to migrate\nDB1 to the new SQL Server 2019 instance on VM1.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "142",
    "question": "DRAG DROP\n-\nYou have an Azure SQL database named DB1.\nYou need to create a partitioned table in DB1.\nWhich three objects should you create in sequence? To answer, move the\nappropriate objects from the list of objects to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nA partition function A partition scheme A table\nhttps://learn.microsoft.com/en-us/sql/relational-databases/partitions/partitioned-\ntables-and-indexes?view=sql-server-ver16\nReference: https://learn.microsoft.com/en-us/sql/relational-databases/partitions/\ncreate-partitioned-tables-and-indexes?view=sql-server-ver16 Obviously, the answer is: A\npartition function => A partition scheme => A table\nA partition function A partition scheme A table. I think you need to have partioned table\ncreated before creating an aligned index. As its Azure sql database, it DOESNOT support\nmultiple filegroup's. I believe above is the correct sequence\nA filegroup Partition Function Partition Scheme",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "143",
    "question": "HOTSPOT\n-\nYou have an Azure subscription.\nYou need to deploy a logical SQL server by using PowerShell. The solution must\nensure that the logical SQL server can create Azure AD users and provide\nTransparent Data Encryption (TDE) with a customer-managed key.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\n\n\nfrom URL, I see the first option should be \"Assigned Identity\". Answer should be \"Assigned\nIdentity\" and \"Key id\" https://learn.microsoft.com/en-us/powershell/module/az.sql/\nnew-azsqlserver?view=azps-10.2.0#code-try-3\nProvided answer seems wrong. There was no indication of cross-tenant CMK in the\nquestion so FederatedClientId would be absurd. AssignIdentity and keyId seems like the\ndefault good answer\nTo create a new Azure SQL database with TDE enabled and CMK (customer-managed key)\nyou need to specify -AssignIdentity and -KeyId parameters. https://learn.microsoft.com/\nen-us/powershell/module/az.sql/new-azsqlserver?view=azps-10.1.0#example-3-create-a-\nnew-azure-sql-database-server-with-tde-cmk\nhttps://learn.microsoft.com/en-us/powershell/module/az.sql/new-azsqlserver?\nview=azps-10.1.0 -federated parameter is only for cross-account CMK. - -AssignIdentity\nGenerate and assign an Azure Active Directory Identity for this server for use with key\nmanagement services like Azure KeyVault.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "144",
    "question": "You have an Azure subscription.\nYou create a logical SQL server that hosts four databases. Each database will be\nused by a separate customer.\nYou need to ensure that each customer can access only its own database. The\nsolution must minimize administrative effort.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Deny public access.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a private endpoint.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a database-level firewall rule.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a network security group (NSG).\nE) Create a server-level firewall rule.\nPage 267 of 818\n268 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "145",
    "question": "DRAG DROP\n-\nYou have an Azure key vault named Vault1 and a SQL server on Azure Virtual\nMachines instance named SQL1. SQL1 hosts a database named DB1.\nYou need to configure Transparent Data Encryption (TDE) on DB1 to use a key in\nVault1.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nProvide some logic useing Chat: 1. Create an Azure AD service principal and grant the\nservice principal permissions for Vault1: This step is necessary because SQL Server on an\nAzure VM will use this service principal to authenticate to Azure Key Vault and use the\nkeys within it. 2. On SQL1, create a cryptographic provider and a Microsoft SQL Server\ncredential: This step involves configuring SQL Server to be able to use Azure Key Vault as\nan Extensible Key Management (EKM) provider, which includes setting up the necessary\ncredentials for authentication. 3. On SQL1, create an asymmetric key: This will be the key\nfrom the Azure Key Vault that you will use for TDE. SQL Server will use this key to encrypt\nthe database encryption key. 4. On SQL1, create a login from the asymmetric key: This\ncreates a SQL Server login that is based on the asymmetric key stored in Azure Key Vault,\nwhich is necessary for the SQL Server to use that key for TDE.\nAnswer looks correct https://www.sqlservercentral.com/blogs/transparent-data-\nencryption-with-azure-key-vault-1\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-\nencryption-byok-overview?view=azuresql-mi In order for the Azure SQL server to use TDE\nprotector stored in AKV for encryption of the DEK, the key vault administrator needs to\ngive the following access rights to the server using its unique Azure Active Directory\n(Azure AD) identity: get - for retrieving the public part and properties of the key in the\nKey Vault wrapKey - to be able to protect (encrypt) DEK unwrapKey - to be able to\nunprotect (decrypt) DEK",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "146",
    "question": "You have an Azure subscription that contains an instance of SQL Server on an\nAzure virtual machine named VM1 and an Azure Active Directory Domain\nServices (Azure AD DS) domain that contains two users named User1 and User 2.\nOn the default instance of SQL Server on VM1, you create a credential named\nCredential1 for User1.\nYou need to ensure that User2 can create a SQL Server Agent proxy that will use\nCredential1. The solution must use the principle of least privilege.\nWhich role should you assign to User2?",
    "options": [
      {
        "letter": "A",
        "text": "SQLAgentUserRole",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQLAgentReaderRole",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQLAgentOperatorRole",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "sysadmin",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "147",
    "question": "DRAG DROP\n-\nYou have an Azure subscription that contains an instance of SQL Server on\nAzure Virtual Machines named SQLVM1 and a virtual machine named Server1\nthat runs Windows Server. SQLVM1 and Server1 are joined to an Active Directory\nDomain Services (AD DS) domain. Server1 hosts a file share named Share1.\nYou need to ensure that a SOL Server Agent job step on SQLVM1 can access the\nfiles in Share1. The solution must use the principle of least privilege.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCheck limitations: https://learn.microsoft.com/en-us/sql/ssms/agent/create-a-sql-\nserver-agent-proxy?view=sql-server-ver16\nSomeone has a link for this?\nCorrect.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "148",
    "question": "You have an Azure subscription.\nYou need to deploy an instance of SQL Server on Azure Virtual Machines. The\nsolution must meet the following requirements:\n• Custom performance configuration, such as IOPS, capacity, and throughout,\nmust be supported.\n• Costs must be minimized.\nWhich type of disk should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Premium SSD v2",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Premium SSD",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Standard SSD",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Ultra SSD\nPage 273 of 818\n274 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "149",
    "question": "You have an on-premises datacenter that contains a 14-TB Microsoft SQL Server\ndatabase.\nYou plan to create an Azure SQL managed instance and migrate the on-premises\ndatabase to the new instance.\nWhich three service tiers support the SQL managed instance? Each correct\nanswer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "General Purpose Standard",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Business Critical Memory Optimized Premium",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "General Purpose Premium",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Business Critical Premium\nE) Business Critical Standard",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "BC"
  },
  {
    "number": "150",
    "question": "DRAG DROP\n-\nYou have an instance of SQL Server on Azure Virtual Machines named SQL1. SQL1\ncontains a database named DB1.\nYou need to enable Transparent Data Encryption (TDE) for DB1.\nWhich three objects should you create in sequence? To answer, move the\nappropriate objects from the list of objects to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nanswer is right https://learn.microsoft.com/en-us/sql/relational-databases/security/\nencryption/transparent-data-encryption?view=sql-server-ver16#enable-tde\nAnswer is correct 1.Create a master key. 2.Create or obtain a certificate protected by the\nmaster key. 3.Create a database encryption key and protect it by using the certificate.\n4.Set the database to use encryption.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "151",
    "question": "You have SQL Server on an Azure virtual machine that contains a database\nnamed DB1.\nYou have an application that queries DB1 to generate a sales report.\nYou need to see the parameter values from the last time the query was\nexecuted.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Enable Last_Query_Plan_Stats in the master database",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable Lightweight_Query_Profiling in DB1",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Enable Last_Query_Plan_Stats in DB1",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Enable Lightweight_Query_Profiling in the master database\nE) Enable PARAMETER_SNIFFING in DB1",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nLast_Query_Plan_Stats allows you to enable or disable collection of the last query plan"
  },
  {
    "number": "152",
    "question": "HOTSPOT -\nYou have SQL Server on an Azure virtual machine that contains a database\nnamed Db1.\nYou need to enable automatic tuning for Db1.\nHow should you complete the statements? To answer, select the appropriate\nanswer in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: SET QUERY_STORE = ON (OPERATION MODE = READ_WRITE);\nMust enable the Query Store.\nIncorrect:\nIf the server may be Azure SQL or Managed Instance then the response should be SET\nAUTOMATIC_TUNNIG=AUTO, but as it is a SQL server the Query store needs to be first\nenabled.\nBox 2: SET AUTOMATIC_TUNING (FORCE_LAST_GOOD_PLAN = ON)\nTo configure individual automatic tuning options via T-SQL, connect to the database and\nexecute the query such as this one:\nALTER DATABASE current SET AUTOMATIC_TUNING (FORCE_LAST_GOOD_PLAN = ON)\nSetting the individual tuning option to ON will override any setting that database\ninherited and enable the tuning option. Setting it to OFF will also override any setting\nthat database inherited and disable the tuning option.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/automatic-tuning/\nautomatic-tuning https://docs.microsoft.com/en-us/azure/azure-sql/database/\nautomatic-tuning-enable\nCommunity Discussion\nSET QUERY_STORE = ON (OPERATION_MODE = READ_WRITE); https://docs.microsoft.com/\nen-us/sql/relational-databases/performance/monitoring-performance-by-using-the-\n\n\nquery-store?view=sql-server-ver15 SET AUTOMATIC_TUNING ( FORCE_LAST_GOOD_PLAN =\nON ); https://docs.microsoft.com/en-us/sql/relational-databases/automatic-tuning/\nautomatic-tuning?view=sql-server-ver15\nThe right answer is as Tomi1234 say because is a VM SQL Server: Box1 - SET QUERY_STORE\n= ON (OPERATION MODE = READ_WRITE); Box2 - SET AUTOMATIC_TUNNING\n(FORCE_LAST_GOOD_PLAN = ON); https://www.sqlshack.com/understanding-automatic-\ntuning-in-sql-server-2017/ If the server may be Azure SQL or Managed Instance then the\nresponse should be: Box1. SET AUTOMATIC_TUNNIG=AUTO Box2. SET AUTOMATIC_TUNNING\n(FORCE_LAST_GOOD_PLAN = ON) and other options are SET AUTOMATIC_TUNING\n(FORCE_LAST_GOOD_PLAN = ON, CREATE_INDEX = ON, DROP_INDEX = OFF) https://\ndocs.microsoft.com/es-es/azure/azure-sql/database/automatic-tuning-enable\nCorrect Answer: SET QUERY_STORE = ON (OPERATION MODE = READ_WRITE); SET\nAUTOMATIC_TUNING ( FORCE_LAST_GOOD_PLAN = ON ); Reason: As this is SQL Server\nhosted on Azure VM and not Azure SQL DB / MI, Query store needs to be first enabled\nand then automatic plan needs to be applied.\nThe reference is wrong as it applies only to SQL Database and SQL managed instance.\nTomi1234 provided the right answer\nAnswers in green (in example) are correct. https://docs.microsoft.com/en-us/azure/\nazure-sql/database/automatic-tuning-enable",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "153",
    "question": "You deploy a database to an Azure SQL Database managed instance.\nYou need to prevent read queries from blocking queries that are trying to write\nto the database.\nWhich database option should set?",
    "options": [
      {
        "letter": "A",
        "text": "PARAMETERIZATION to FORCED",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "PARAMETERIZATION to SIMPLE",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Delayed Durability to Forced",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "READ_COMMITTED_SNAPSHOT to ON",
        "is_correct": true
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "154",
    "question": "You have an Azure SQL database.\nYou discover that the plan cache is full of compiled plans that were used only\nonce.\nYou run the select * from sys.database_scoped_configurations Transact-SQL\ncommand and receive the results shown in the following table.\nYou need relieve the memory pressure.\nWhat should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "LEGACY_CARDINALITY_ESTIMATION",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "QUERY_OPTIMIZER_HOTFIXES",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "OPTIMIZE_FOR_AD_HOC_WORKLOADS \nPage 283 of 818\n284 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "ACCELERATED_PLAN_FORCING",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "OPTIMIZE_FOR_AD_HOC_WORKLOADS = { ON | OFF }\nEnables or disables a compiled plan stub to be stored in cache when a batch is compiled\nfor the first time. The default is OFF. Once the database scoped configuration\nOPTIMIZE_FOR_AD_HOC_WORKLOADS is enabled for a database, a compiled plan stub will\nbe stored in cache when a batch is compiled for the first time. Plan stubs have a smaller\nmemory footprint compared to the size of the full compiled plan.\nIncorrect Answers:\nA: LEGACY_CARDINALITY_ESTIMATION = { ON | OFF | PRIMARY }\nEnables you to set the query optimizer cardinality estimation model to the SQL Server\n2012 and earlier version independent of the compatibility level of the database. The\ndefault is OFF, which sets the query optimizer cardinality estimation model based on the\ncompatibility level of the database.\nB: QUERY_OPTIMIZER_HOTFIXES = { ON | OFF | PRIMARY }\nEnables or disables query optimization hotfixes regardless of the compatibility level of\nthe database. The default is OFF, which disables query optimization hotfixes that were\nreleased after the highest available compatibility level was introduced for a specific\nversion (post-RTM).\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/alter-database-scoped-\nconfiguration-transact-sql"
  },
  {
    "number": "155",
    "question": "You have SQL Server on an Azure virtual machine that contains a database\nnamed DB1.\nYou view a plan summary that shows the duration in milliseconds of each\nexecution of query 1178902 as shown in the following exhibit:\nWhat should you do to ensure that the query uses the execution plan which\nexecutes in the least amount of time?",
    "options": [
      {
        "letter": "A",
        "text": "Force the query execution plan for plan 1221065.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Run the DBCC FREEPROCCACHE command.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Force the query execution plan for plan 1220917.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Disable parameter sniffing.\nPage 285 of 818\n286 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "As per exhibit, the execution plan 1221065 has lower execution time compared to plan\n1220917.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/query-store-\nusage-scenarios"
  },
  {
    "number": "156",
    "question": "HOTSPOT -\nYou have an Azure SQL database named DB1. The automatic tuning options for\nDB1 are configured as shown in the following exhibit.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nBy default CREATE INDEX is disabled. It is here configured as INHERIT so it is disabled.\nBox 2: No -\nBy default DROP INDEX is disabled.\nBox 3: Yes -\nFORCE LAST GOOD PLAN (automatic plan correction) - Identifies Azure SQL queries using\nan execution plan that is slower than the previous good plan, and queries using the last\nknown good plan instead of the regressed plan.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/automatic-tuning-enable\nCommunity Discussion\nAnswer given is correct. Refer to current state, is ON\nShould be No, NO, Yes, As Azure defaults are FORCE_LAST_GOOD_PLAN is enabled,\nCREATE_INDEX is disabled, and DROP_INDEX is disabled. https://docs.microsoft.com/en-\nus/azure/azure-sql/database/automatic-tuning-enable\n\n\nYes - It will create nonclustered indexes No - It can't add columns to existing indexes Yes\n- It will revert to prior plans automatically in case of regression\nThe defaults have been overridden in this scenario.\nThe defaults have been overridden in this scenario.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "157",
    "question": "You have an Azure SQL database named DB1. You run a query while connected\nto DB1.\nYou review the actual execution plan for the query, and you add an index to a\ntable referenced by the query.\nYou need to compare the previous actual execution plan for the query to the\nLive Query Statistics.\nWhat should you do first in Microsoft SQL Server Management Studio (SSMS)?",
    "options": [
      {
        "letter": "A",
        "text": "For DB1, set QUERY_CAPTURE_MODE of Query Store to All.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Run the SET SHOWPLAN_ALL Transact-SQL statement.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Save the actual execution plan.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Enable Query Store for DB1.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The Plan Comparison menu option allows side-by-side comparison of two different\nexecution plans, for easier identification of similarities and changes that explain the\ndifferent behaviors for all the reasons stated above. This option can compare between:\nTwo previously saved execution plan files (.sqlplan extension).\nOne active execution plan and one previously saved query execution plan.\nTwo selected query plans in Query Store."
  },
  {
    "number": "158",
    "question": "You have an Azure SQL database.\nUsers report that the executions of a stored procedure are slower than usual.\nYou suspect that a regressed query is causing the performance issue.\nYou need to view the query execution plan to verify whether a regressed query\nis causing the issue. The solution must minimize effort.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Performance Recommendations in the Azure portal",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Extended Events in Microsoft SQL Server Management Studio (SSMS)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Query Store in Microsoft SQL Server Management Studio (SSMS)",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Query Performance Insight in the Azure portal\nPage 290 of 818\n291 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "in the Azure portal.\nPage 291 of 818\n292 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Use the Query Store Page in SQL Server Management Studio.\nQuery performance regressions caused by execution plan changes can be non-trivial and\ntime consuming to resolve.\nSince the Query Store retains multiple execution plans per query, it can enforce policies\nto direct the Query Processor to use a specific execution plan for a query.\nThis is referred to as plan forcing. Plan forcing in Query Store is provided by using a\nmechanism similar to the USE PLAN query hint, but it does not require any change in\nuser applications. Plan forcing can resolve a query performance regression caused by a\nplan change in a very short period of time.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/monitoring-\nperformance-by-using-the-query-store"
  },
  {
    "number": "159",
    "question": "You have an Azure SQL database. The database contains a table that uses a\ncolumnstore index and is accessed infrequently.\nYou enable columnstore archival compression.\nWhat are two possible results of the configuration? Each correct answer\npresents a complete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Queries that use the index will consume more disk I/O.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Queries that use the index will retrieve fewer data pages.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "The index will consume more disk space.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "The index will consume more memory.\nE) Queries that use the index will consume more CPU resources.",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "E\nFor rowstore tables and indexes, use the data compression feature to help reduce the\nsize of the database. In addition to saving space, data compression can help improve\nperformance of I/O intensive workloads because the data is stored in fewer pages and\nqueries need to read fewer pages from disk.\nUse columnstore archival compression to further reduce the data size for situations\nwhen you can afford extra time and CPU resources to store and retrieve the data.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/data-compression/data-\ncompression"
  },
  {
    "number": "160",
    "question": "You are designing a dimension table in an Azure Synapse Analytics dedicated\nSQL pool.\nYou need to create a surrogate key for the table. The solution must provide the\nfastest query performance.\nWhat should you use for the surrogate key?",
    "options": [
      {
        "letter": "A",
        "text": "an IDENTITY column",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a GUID column",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a sequence object",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "- Sparse\nColumns - Surrogate Keys. Implement with Identity (OPTION A). - Synonyms - Triggers -\nUnique Indexes - User-Defined Types https://docs.microsoft.com/en-us/azure/synapse-\nanalytics/sql-data-warehouse/sql-data-warehouse-tables-overview#unsupported-table-\nfeatures",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Dedicated SQL pool supports many, but not all, of the table features offered by other\ndatabases.\nSurrogate keys are not supported. Implement it with an Identity column.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-\ndata-warehouse-tables-overview"
  },
  {
    "number": "161",
    "question": "You are designing a star schema for a dataset that contains records of online\norders. Each record includes an order date, an order due date, and an order\nship date.\nYou need to ensure that the design provides the fastest query times of the\nrecords when querying for arbitrary date ranges and aggregating by fiscal\ncalendar attributes.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Create a date dimension table that has a DateTime key.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a date dimension table that has an integer key in the format of YYYYMMDD. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Use built-in SQL functions to extract date attributes.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Use integer columns for the date fields. \nE) Use DateTime columns for the date fields.\nPage 294 of 818\n295 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "D\nWhy use a Date Dimension Table in a Data Warehouse.\nThe Date dimension is one of these dimension tables related to the Fact. Here is a simple\nData Diagram for a Data Mart of Internet Sales information for the\nAdventure Works DW database which can be obtained for free from CodePlex or other\nonline sources.\nThe relationship is created by the surrogate keys columns (integer data type) rather than\nthe date data type.\nThe query users have to write against a Data Mart are much simpler than against a\ntransaction database. There are less joins because of the one to many relationships\nbetween the fact dimension table(s). The dimension tables are confusing to someone\nwho has been normalizing databases as a career. The dimension is a flattened or de-\nnormalized table. This creates cases of duplicate data, but the simplistic query overrides\nthe duplicate data in a dimensional model.\nReference:\nhttps://www.mssqltips.com/sqlservertip/3117/defining-role-playing-dimensions-for-sql-\nserver-analysis-services/ https://community.idera.com/database-tools/blog/b/\ncommunity_blog/posts/why-use-a-date-dimension-table-in-a-data-warehouse"
  },
  {
    "number": "162",
    "question": "HOTSPOT -\nYou are designing an enterprise data warehouse in Azure Synapse Analytics that\nwill store website traffic analytics in a star schema.\nYou plan to have a fact table for website visits. The table will be approximately\n5 GB.\nYou need to recommend which distribution type and index type to use for the\ntable. The solution must provide the fastest query performance.\nWhat should you recommend? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Hash -\nConsider using a hash-distributed table when:\nThe table size on disk is more than 2 GB.\nThe table has frequent insert, update, and delete operations.\nBox 2: Clustered columnstore -\nClustered columnstore tables offer both the highest level of data compression and the\nbest overall query performance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-\ndata-warehouse-tables-distribute https://docs.microsoft.com/en-us/azure/synapse-\nanalytics/sql-data-warehouse/sql-data-warehouse-tables-index\nCommunity Discussion\nlooks correct\nThis question is related to the DP-203 exam: Data Engineering on Microsoft Azure.\nDP-203\nThis is for DP-203 exam\ndo203 question",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "163",
    "question": "You have an Azure Data Factory pipeline that is triggered hourly.\nThe pipeline has had 100% success for the past seven days.\nThe pipeline execution fails, and two retries that occur 15 minutes apart also\nfail. The third failure returns the following error.\nWhat is a possible cause of the error?",
    "options": [
      {
        "letter": "A",
        "text": "From 06:00 to 07:00 on January 10, 2021, there was no data in wwi/BIKES/CARBON.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "The parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "From 06:00 to 07:00 on January 10, 2021, the file format of data in wwi/BIKES/CARBON was\nincorrect.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "The pipeline was triggered too early.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "A file is missing.\nIncorrect:\nNot A, not C, not D: Time of the error is 07:45."
  },
  {
    "number": "164",
    "question": "HOTSPOT -\nYou have an Azure SQL database.\nYou are reviewing a slow performing query as shown in the following exhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Live Query Statistics -\nLive Query Statistics as it a percentage of the execution.\nBox 2: Key Lookup -\nThe use of a Key Lookup operator in a query plan indicates that the query might benefit\nfrom performance tuning. For example, query performance might be improved by adding\na covering index.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/live-query-\nstatistics?view=sql-server-ver15 https://docs.microsoft.com/en-us/sql/relational-\ndatabases/showplan-logical-and-physical-operators-reference\nCommunity Discussion\nKeyLookup - yes, but this is Live Query Statistics (e.g. percentage of execution)\nKeyLookup - yes, but this is Live Query Statistics (e.g. percentage of execution)\nEstimated execution plan and Key Lookup\nAs per below, 2nd selection has to be \"Key Lookup\" (not Nested loop). The Key Lookup\noperator is a bookmark lookup on a table with a clustered index. The Argument column\ncontains the name of the clustered index and the clustering key used to look up the row\nin the clustered index. Key Lookup is always accompanied by a Nested Loops operator. If\nthe WITH PREFETCH clause appears in the Argument column, the query processor has\n\n\ndetermined that it is optimal to use asynchronous prefetching (read-ahead) when\nlooking up bookmarks in the clustered index. The use of a Key Lookup operator in a\nquery plan indicates that the query might benefit from performance tuning. For example,\nquery performance might be improved by adding a covering index. https://\ndocs.microsoft.com/en-us/sql/relational-databases/showplan-logical-and-physical-\noperators-reference?view=sql-server-ver15\nLive statistics can be recognized from the Query Progress box top left. The nested loop\noperator is used wheneven no appropriate index is available for key lookup or index\nseek. An index is always faster, so Nested Loop means opportunity for performance\nimprovement.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "165",
    "question": "You have an Azure SQL managed instance.\nYou need to gather the last execution of a query plan and its runtime statistics.\nThe solution must minimize the impact on currently running queries.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Generate an estimated execution plan.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Generate an actual execution plan.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Run sys.dm_exec_query_plan_stats.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Generate Live Query Statistics.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-\nmanagement-views/sys-dm-exec-query-plan-stats-transact-sql?view=sql-server-ver15"
  },
  {
    "number": "166",
    "question": "HOTSPOT -\nYou have an Azure SQL database named db1 on a server named server1.\nYou use Query Performance Insight to monitor db1.\nYou need to modify the Query Store configuration to ensure that performance\nmonitoring data is available as soon as possible.\nWhich configuration setting should you modify and which value should you\nconfigure? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: INTERVAL_LENGTH_MINUTES -\nINTERVAL_LENGTH_MINUTES defines size of time window during which collected runtime\nstatistics for query plans are aggregated and persisted. Every active query plan has at\nmost one row for a period of time defined with this configuration.\nDefault: 60 -\nBox 2: 1 -\nStatistics Collection Interval (INTERVAL_LENGTH_MINUTES): Defines the level of\ngranularity for the collected runtime statistic, expressed in minutes. The default is 60\nminutes. Consider using a lower value if you require finer granularity or less time to\ndetect and mitigate issues.\nUse SQL Server Management Studio or Transact-SQL to set a different value for Statistics\nCollection Interval:\nALTER DATABASE [QueryStoreDB]\nSET QUERY_STORE (INTERVAL_LENGTH_MINUTES = 60);\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/best-practice-\nwith-the-query-store\n\n\nCommunity Discussion\nIts OK. . Consider using a lower value if you require finer granularity or less time to detect\nand mitigate issues. Statistics Collection Interval: ALTER DATABASE [QueryStoreDB] SET\nQUERY_STORE (INTERVAL_LENGTH_MINUTES = 60); It is the defaulf value, put 1 instead of\n60 https://docs.microsoft.com/en-us/sql/relational-databases/performance/best-\npractice-with-the-query-store?view=sql-server-ver15\nits ok?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "167",
    "question": "You have an Azure SQL Database managed instance.\nThe instance starts experiencing performance issues.\nYou need to identify which query is causing the issue and retrieve the execution\nplan for the query. The solution must minimize administrative effort.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "SQL Profiler",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Extended Events",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Query Store",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "dynamic management views",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Use the dynamic management view sys.dm_exec_requests to track currently executing\nqueries and the associated worker time.\nIncorrect:\nNot C: DMVs that track Query Store and wait statistics show results for only successfully\ncompleted and timed-out queries.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/identify-query-performance-issues"
  },
  {
    "number": "168",
    "question": "You have an Azure SQL database named DB1.\nYou need to display the estimated execution plan of a query by using the query\neditor in the Azure portal.\nWhat should you do first?",
    "options": [
      {
        "letter": "A",
        "text": "Run the SET SHOWPLAN_ALL Transact-SQL statement.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "For DB1, set QUERY_CAPTURE_MODE of Query Store to All.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Run the SET FORCEPLAN Transact-SQL statement.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Enable Query Store for DB1.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The SET SHOWPLAN_ALL command causes Microsoft SQL Server not to execute Transact-\nSQL statements. Instead, SQL Server returns detailed information about how the\nstatements are executed and provides estimates of the resource requirements for the\nstatements.\nReference:"
  },
  {
    "number": "169",
    "question": "HOTSPOT -\nYou have an Azure SQL database.\nYou have a query and the associated execution plan as shown in the following\nexhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Key Lookup -\nThe Key Lookup cost is 99% so that is the performance bottleneck.\nBox 2: nonclustered index -\nThe key lookup on the clustered index is used because the nonclustered index does not\ninclude the required columns to resolve the query. If you add the required columns to\nthe nonclustered index, the key lookup will not be required.\nCommunity Discussion\nThe given answer looks correct\nYes, the query is just pulling from one table so by making the nonclustered index a\ncovering index including al the additional required fields will eliminate the need for the\nlookup.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "170",
    "question": "You have an instance of SQL Server on Azure Virtual Machines that has a\ndatabase named DB1.\nYou plan to implement Azure SQL Data Sync for DB1.\nWhich isolation level should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "SERIALIZABLE",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SNAPSHOT",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "READ UNCOMMITTED\nPage 308 of 818\n309 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "READ COMMITTED",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Data Sync general requirements include:\n* Snapshot isolation must be enabled for both Sync members and hub.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-\nserver-sql-database"
  },
  {
    "number": "171",
    "question": "HOTSPOT -\nYou have SQL Server on an Azure virtual machine.\nYou review the query plan shown in the following exhibit.\nFor each of the following statements, select yes if the statement is true.\nOtherwise, select no.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nThere is only one query plan available. Force has no effect.\nBox 2: No -\nAdding an index will not increase IO usage.\nBox 3: Yes -\nThe performance would improve.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/monitoring-\nperformance-by-using-the-query-store\nCommunity Discussion\nFor me all the three are NO: 1.If you force that query plan, the behaviour will be always\nthe same, so you will not reduce or increase the I/O compared with what we see in the\nimage. Only in the case you see in the chart another and worst query plan used by the\nquery, choosing this one will reduce the average I/O. 2.You can create another index in\nthe table but if using it is worse than using the current one, the sql engine will not\nchoose it in the plan. 3.PK is the clustered index and it has all columns by definition.\n\n\nAnswers are No, No and Yes. 1) force plan - there is only one execution plan. Forcing it\nwill not change the run time. 2) No - adding another index can incur maintenance\noverhead, but it is not going to INCREASE the IO. If it did the optimizer would use the\nplan it already has. 3) Yes - Agreed with whomever said the extra columns should not be\nadded to the primary key, but design is not the question. Question is will it make the\nplan more efficient. Yes it will. Currently we have a lookup. Adding those columns will\nmake the index a covering index for this query and eliminate the lookup.\nI manually tested the query run on sample adventureworks DB and observed saving in\nLogical reads. logical reads reduced from 689 to 5 which confirms the correct answer for\nthird part is Yes. (it may not be a good design decision but it does provide improved\nperformance). Correct answer as per my analysis - No, No, Yes\nI manually tested the query run on sample adventureworks DB and observed saving in\nLogical reads. logical reads reduced from 689 to 5 which confirms the correct answer for\nthird part is Yes. (it may not be a good design decision but it does provide improved\nperformance). Correct answer as per my analysis - No, No, Yes\nUp-voted by mistake.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "172",
    "question": "A data engineer creates a table to store employee information for a new\napplication. All employee names are in the US English alphabet. All addresses\nare locations in the United States. The data engineer uses the following\nstatement to create the table.\nYou need to recommend changes to the data types to reduce storage and\nimprove performance.\nWhich two actions should you recommend? Each correct answer presents part\nof the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "Change Salary to the money data type.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Change PhoneNumber to the float data type.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Change LastHireDate to the datetime2(7) data type.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Change PhoneNumber to the bigint data type.\nE) Change LastHireDate to the date data type.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "E\nA: Money takes less space compared to VARCHAR(20)\nE: Date takes less space compared to Datetime.\nReference:\nhttps://docs.microsoft.com/eN-Us/sql/t-sql/data-types/data-types-transact-sql"
  },
  {
    "number": "173",
    "question": "You have an Azure SQL database.\nYou identify a long running query.\nYou need to identify which operation in the query is causing the performance\nissue.\nWhat should you use to display the query execution plan in Microsoft SQL\nServer Management Studio (SSMS)?",
    "options": [
      {
        "letter": "A",
        "text": "Live Query Statistics",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "an estimated execution plan",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an actual execution plan",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Client Statistics\nPage 314 of 818\n315 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "SQL Server Management Studio provides the ability to view the live execution plan of an\nactive query. This live query plan provides real-time insights into the query execution\nprocess as the controls flow from one query plan operator to another. The live query\nplan displays the overall query progress and operator-level run-time execution statistics\nsuch as the number of rows produced, elapsed time, operator progress, etc. Because this\ndata is available in real time without needing to wait for the query to complete, these\nexecution statistics are extremely useful for debugging query performance issues.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/performance/live-query-\nstatistics"
  },
  {
    "number": "174",
    "question": "You have a version-8.0 Azure Database for MySQL database.\nYou need to identify which database queries consume the most resources.\nWhich tool should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Query Store",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Metrics",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Query Performance Insight",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Alerts",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The Query Store feature in Azure Database for MySQL provides a way to track query\nperformance over time. Query Store simplifies performance troubleshooting by helping\nyou quickly find the longest running and most resource-intensive queries. Query Store\nautomatically captures a history of queries and runtime statistics, and it retains them for\nyour review. It separates data by time windows so that you can see database usage\npatterns. Data for all users, databases, and queries is stored in the mysql schema\ndatabase in the Azure Database for MySQL instance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/mysql/concepts-query-store"
  },
  {
    "number": "175",
    "question": "HOTSPOT -\nYou have an Azure Data Factory instance named ADF1 and two Azure Synapse\nAnalytics workspaces named WS1 and WS2.\nADF1 contains the following pipelines:\n✑ P1: Uses a copy activity to copy data from a nonpartitioned table in a\ndedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account\n✑ P2: Uses a copy activity to copy data from text-delimited files in an Azure Data\nLake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of\nWS2\nYou need to configure P1 and P2 to maximize parallelism and performance.\nWhich dataset settings should you configure for the copy activity of each\npipeline? To answer, select the appropriate options in the answer area.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nP1: Set the Partition option to Dynamic Range.\nThe SQL Server connector in copy activity provides built-in data partitioning to copy data\nin parallel.\nP2: Set the Copy method to PolyBase\nPolybase is the most efficient way to move data into Azure Synapse Analytics. Use the\nstaging blob feature to achieve high load speeds from all types of data stores, including\nAzure Blob storage and Data Lake Store. (Polybase supports Azure Blob storage and Azure\nData Lake Store by default.)\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-\nwarehouse https://docs.microsoft.com/en-us/azure/data-factory/load-azure-sql-data-\nwarehouse\nCommunity Discussion\nThis question is for DP-203 exam (Data Engineer).\nLooks correct\nDP-203\nP1 : PolyBase P2 : Bulk Insert\nFor P1: Set the Copy method to PolyBase For P2: Set the Copy method to Bulk insert",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "176",
    "question": "You have the following Azure Data Factory pipelines:\n✑ Ingest Data from System1\nIngest Data from System2 -\n✑ Populate Dimensions\n✑ Populate Facts\nIngest Data from System1 and Ingest Data from System2 have no dependencies.\nPopulate Dimensions must execute after Ingest Data from System1 and Ingest\nData from System2. Populate Facts must execute after the Populate Dimensions\npipeline. All the pipelines must execute every eight hours.\nWhat should you do to schedule the pipelines for execution?",
    "options": [
      {
        "letter": "A",
        "text": "Add a schedule trigger to all four pipelines.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add an event trigger to all four pipelines.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a parent pipeline that contains the four pipelines and use an event trigger.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a parent pipeline that contains the four pipelines and use a schedule trigger.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Reference:\nhttps://www.mssqltips.com/sqlservertip/6137/azure-data-factory-control-flow-activities-\noverview/"
  },
  {
    "number": "177",
    "question": "You have an Azure Data Factory pipeline that performs an incremental load of\nsource data to an Azure Data Lake Storage Gen2 account.\nData to be loaded is identified by a column named LastUpdatedDate in the\nsource table.\nYou plan to execute the pipeline every four hours.\nYou need to ensure that the pipeline execution meets the following\nrequirements:\n✑ Automatically retries the execution when the pipeline run fails due to\nconcurrency or throttling limits.\n✑ Supports backfilling existing data in the table.\nWhich type of trigger should you use?",
    "options": [
      {
        "letter": "A",
        "text": "tumbling window",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "on-demand",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "event",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "schedule",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The Tumbling window trigger supports backfill scenarios. Pipeline runs can be scheduled\nfor windows in the past.\nIncorrect Answers:\nD: Schedule trigger does not support backfill scenarios. Pipeline runs can be executed\nonly on time periods from the current time and the future.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-\ntriggers"
  },
  {
    "number": "178",
    "question": "You have an Azure Data Factory that contains 10 pipelines.\nYou need to label each pipeline with its main purpose of either ingest,\ntransform, or load. The labels must be available for grouping and filtering when\nusing the monitoring experience in Data Factory.\nWhat should you add to each pipeline?",
    "options": [
      {
        "letter": "A",
        "text": "an annotation",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a resource tag",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a run group ID",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a user property\nE) a correlation ID",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure Data Factory annotations help you easily filter different Azure Data Factory objects\nbased on a tag. You can define tags so you can see their performance or find errors\nfaster.\nReference:\nhttps://www.techtalkcorner.com/monitor-azure-data-factory-annotations/"
  },
  {
    "number": "179",
    "question": "HOTSPOT -\nYou have an Azure data factory that has two pipelines named PipelineA and\nPipelineB.\nPipelineA has four activities as shown in the following exhibit.\nPipelineB has two activities as shown in the following exhibit.\nYou create an alert for the data factory that uses Failed pipeline runs metrics\nfor both pipelines and all failure types. The metric has the following settings:\n✑ Operator: Greater than\n✑ Aggregation type: Total\n✑ Threshold value: 2\n✑ Aggregation granularity (Period): 5 minutes\n✑ Frequency of evaluation: Every 5 minutes\nData Factory monitoring records the failures shown in the following table.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nJust one failure within the 5-minute interval.\nBox 2: No -\nJust two failures within the 5-minute interval.\nBox 3: No -\nJust two failures within the 5-minute interval.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-metric-overview\nCommunity Discussion\nNot related to DP-300 exam\nDP-203\nsame here, do not understand the explanation\nbouncer for me..\nThe answer No-No-No is correct. Every 5 minutes period you need at least 3 errors in\norder to trigger the alert.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "180",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging\nzone, transform the data by executing an R script, and then insert the\ntransformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat executes mapping data flow, and then inserts the data into the data\nwarehouse.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Correct solution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat executes an Azure Databricks notebook, and then inserts the data into the data\nwarehouse.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data"
  },
  {
    "number": "181",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging\nzone, transform the data by executing an R script, and then insert the\ntransformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You schedule an Azure Databricks job that executes an R notebook,\nand then inserts the data into the data warehouse.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Must use an Azure Data Factory, not an Azure Databricks job.\nCorrect solution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat executes an Azure Databricks notebook, and then inserts the data into the data\nwarehouse.\nReference:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data"
  },
  {
    "number": "182",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging\nzone, transform the data by executing an R script, and then insert the\ntransformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat executes an Azure Databricks notebook, and then inserts the data into the\ndata warehouse.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No\nPage 329 of 818\n330 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "An Azure Data Factory can trigger a Databricks notebook.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-\nnotebook"
  },
  {
    "number": "183",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure Data Lake Storage account that contains a staging zone.\nYou need to design a daily process to ingest incremental data from the staging\nzone, transform the data by executing an R script, and then insert the\ntransformed data into a data warehouse in Azure Synapse Analytics.\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat copies the data to a staging table in the data warehouse, and then uses a\nstored procedure to execute the R script.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure Synapse Analytics does not support R script.\nCorrect solution: You use an Azure Data Factory schedule trigger to execute a pipeline\nthat executes an Azure Databricks notebook, and then inserts the data into the data\nwarehouse.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/r-\ndevelopers-guide"
  },
  {
    "number": "184",
    "question": "DRAG DROP -\nYou have an Azure subscription that contains an Azure SQL managed instance\nnamed SQLMi1 and a SQL Agent job named Backupdb. Backupdb performs a\ndaily backup of the databases hosted on SQLMi1.\nYou need to be notified by email if the job fails.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit\nfor any of the correct orders you select.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nStep 1: Enable Database Mail -\nIf it isn't already enabled, first you would need to configure the Database Mail feature on\nSQL Managed Instance.\nBox 2: Create an operator.\nYou can notify the operator that something happened with your SQL Agent jobs. An\noperator defines contact information for an individual responsible for the maintenance\nof one or more instances in SQL Managed Instance.\nBox 3: Add a failure notification to the job,\nYou can then modify any SQL Agent job and assign operators that will be notified via\nemail if the job completes, fails, or succeeds using SSMS or the following T-\nSQL script:\nEXEC msdb.dbo.sp_update_job @job_name=N'Load data using SSIS',\n@notify_level_email=3, -- Options are: 1 on succeed, 2 on failure, 3 on complete\n@notify_email_operator_name=N'AzureSQLTeam';\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/job-automation-\nmanaged-instance\nCommunity Discussion\nNOTE: More than one order of answer choices is correct.\nNOTE: More than one order of answer choices is correct.\nCorrect. https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/job-\nautomation-managed-instance?view=azuresql\n\n\nNot a fair question. An operator can be created before Database Mail is enabled or after,\nit doesnt matter which way around. As long as the Database Mail is enabled and an\nOperator created before adding a failure notification to a job.\nlooks correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "185",
    "question": "DRAG DROP -\nYou have SQL Server on an Azure virtual machine.\nYou need to use Policy-Based Management in Microsoft SQL Server to identify\nstored procedures that do not comply with your naming conventions.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nFirst create a condition, then a custom policy based on the condition, finally run a policy\nevaluation.\nReference:\nhttps://www.mssqltips.com/sqlservertip/2298/enforce-sql-server-database-naming-\nconventions-using-policy-based-management/\nCommunity Discussion\nyou should create the condition before the policy https://www.red-gate.com/simple-\ntalk/blogs/sql-server-policy-based-management-creating-a-custom-condition/\nyou should create the condition before the policy https://www.red-gate.com/simple-\ntalk/blogs/sql-server-policy-based-management-creating-a-custom-condition/\nCorrect Answer -> Create a custom condition based on a built-in facet -> Create a custom\npolicy based on a condition -> Run a policy evaluation https://www.mssqltips.com/\nsqlservertip/2298/enforce-sql-server-database-naming-conventions-using-policy-based-\nmanagement/\nI think the correct answer is: B-C-F\nRemember CP: firts the Condition, then the Policy.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "186",
    "question": "You have an Azure SQL managed instance named SQLMI1 that hosts 10\ndatabases.\nYou need to implement alerts by using Azure Monitor. The solution must meet\nthe following requirements:\n✑ Minimize costs.\n✑ Aggregate Intelligent Insights telemetry from each database.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "From the Diagnostic settings of each database, select Send to Log Analytics. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From the Diagnostic settings of each database, select Stream to an event hub.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "From the Diagnostic settings of SQLMI1, select Send to Log Analytics.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "From the Diagnostic settings of SQLMI1, select Stream to an event hub.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Databases in Azure SQL Managed Instance\nYou can set up an instance database resource to collect the following diagnostic\ntelemetry:\nTo enable streaming of diagnostic telemetry for an instance database, follow these steps:\n1. Go to instance database resource within managed instance.\n2. Select Diagnostics settings.\n3. Select Turn on diagnostics if no previous settings exist, or select Edit setting to edit a\nprevious setting.\n4. Etc.\n5. Repeat the above steps for each instance database you want to monitor.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/metrics-diagnostic-\ntelemetry-logging-streaming-export-configure?tabs=azure-portal#configure-the-\nstreaming-export-of-diagnostic-telemetry"
  },
  {
    "number": "187",
    "question": "You have an Azure SQL managed instance that hosts multiple databases.\nYou need to configure alerts for each database based on the diagnostics\ntelemetry of the database.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Analytics alerts based on metrics",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "SQL Health Check alerts based on diagnostics logs",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "SQL Health Check alerts based on metrics",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Analytics alerts based on diagnostics logs",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "You can use Azure SQL Analytics for monitoring and alerting.\nYou can easily create alerts with the data coming from Azure SQL Database resources.\nHere are some useful log queries that you can use with a log alert:\nExample, HIGH CPU:\nAzureMetrics -\n| where ResourceProvider==\"MICROSOFT.SQL\"\n| where ResourceId contains \"/DATABASES/\"\n| where MetricName==\"cpu_percent\"\n| summarize AggregatedValue = max(Maximum) by bin(TimeGenerated, 5m)\n| render timechart\nNote: Azure Monitor Logs is based on Azure Data Explorer, and log queries are written\nusing the same Kusto query language (KQL).\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/metrics-diagnostic-\ntelemetry-logging-streaming-export-configure?tabs=azure-portal#configure-the-\nstreaming-export-of-diagnostic-telemetry"
  },
  {
    "number": "188",
    "question": "You have an Azure SQL managed instance.\nYou need to enable SQL Agent Job email notifications.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Use the Agent XPs option.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable the SQL Server Agent.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Run the sp_configure command.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Run the sp_set_agent_properties command.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "You would need to enable Database email extended procedure using Database Mail XPs\nconfiguration option:\nEXEC sp_configure 'show advanced options', 1;\nGO -\nRECONFIGURE;\nGO -\nEXEC sp_configure 'Database Mail XPs', 1;\nGO -\nRECONFIGURE -\nGO -\nNow you can test the configuration by sending emails using sp_send and"
  },
  {
    "number": "189",
    "question": "You have four Azure subscriptions. Each subscription contains multiple Azure\nSQL databases.\nYou need to update the column and index statistics for the databases.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "an Azure Automation runbook",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a SQL Agent job",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Analytics",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "automatic tuning in Azure SQL Database",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "You can create a runbook for index maintenance in an Azure SQL database.\nReference:\nhttps://www.sqlshack.com/automate-azure-sql-database-indexes-and-statistics-\nmaintenance/"
  },
  {
    "number": "190",
    "question": "DRAG DROP -\nYou have SQL Server on an Azure virtual machine named SQL1.\nSQL1 has an agent job to back up all databases.\nYou add a user named dbadmin1 as a SQL Server Agent operator.\nYou need to ensure that dbadmin1 receives an email alert if a job fails.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Enable Database Mail -\n\n\nDatabase Mail must be enabled.\nStep 2: Enable the email settings for the SQL Server Agent.\nTo send a notification in response to an alert, you must first configure SQL Server Agent\nto send mail.\nStep 3: Create a job notification\nExample:\n-- adds an e-mail notification for the specified alert (Test Alert)\n-- This example assumes that Test Alert already exists\n-- and that Franֳ§ois Ajenstat is a valid operator name.\nUSE msdb ;\nGO -\nEXEC dbo.sp_add_notification -\n@alert_name = N'Test Alert',\n@operator_name = N'Franֳ§ois Ajenstat',\n@notification_method = 1 ;\nGO -\nReference:\nhttps://docs.microsoft.com/en-us/sql/ssms/agent/notify-an-operator-of-job-status\nhttps://docs.microsoft.com/en-us/sql/ssms/agent/assign-alerts-to-an-operator\nCommunity Discussion\nC - D -B\nthe answer is C - D -B, to send an email for a failed job you need: 1) one operator , 2)\ndatabase mail feature on, 3) sql job set to use the email profile created in 2) , and last a\nnotification sending the email to the operator created in 1), by the way in the options\nthere is no choice for one operator, so C - D - B.\nCDB is the correct order.\nCDB is the correct order.\n1 Enable mail 2 Enable mail setting in SQL Agent 3 Enable mail in notification tab in a job",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "191",
    "question": "DRAG DROP -\nYou need to apply 20 built-in Azure Policy definitions to all new and existing\nAzure SQL Database deployments in an Azure subscription. The solution must\nminimize administrative effort.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\nStep 1: Create an Azure Policy Initiative\nThe first step in enforcing compliance with Azure Policy is to assign a policy definition. A\n\n\npolicy definition defines under what condition a policy is enforced and what effect to\ntake.\nWith an initiative definition, you can group several policy definitions to achieve one\noverarching goal. An initiative evaluates resources within scope of the assignment for\ncompliance to the included policies.\nStep 2: Create an Azure Policy Initiative assignment\nAssign the initiative definition you created in the previous step.\nStep 3: Run Azure Policy remediation tasks\nTo apply the Policy Initiative to the existing SQL databases.\nReference:\nhttps://docs.microsoft.com/en-us/azure/governance/policy/tutorials/create-and-\nmanage\nCommunity Discussion\ncorrect.\ncorrect",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "192",
    "question": "You have an Azure SQL Database managed instance named SQLMI1. A Microsoft\nSQL Server Agent job runs on SQLMI1.\nYou need to ensure that an automatic email notification is sent once the job\ncompletes.\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "From SQL Server Configuration Manager (SSCM), enable SQL Server Agent",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From SQL Server Management Studio (SSMS), run sp_set_sqlagent_properties",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "From SQL Server Management Studio (SSMS), create a Database Mail profile (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "From the Azure portal, create an Azure Monitor action group that has an Email/SMS/Push/\nVoice action",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "193",
    "question": "You need to trigger an Azure Data Factory pipeline when a file arrives in an\nAzure Data Lake Storage Gen2 container.\nWhich resource provider should you enable?",
    "options": [
      {
        "letter": "A",
        "text": "Microsoft.EventHub",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Microsoft.EventGrid",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Microsoft.Sql",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Microsoft.Automation",
        "is_correct": false
      },
      {
        "letter": "A",
        "text": "is a common data integration pattern that involves\nproduction, detection, consumption, and reaction to events. Data integration scenarios\noften require Data Factory customers to trigger pipelines based on events happening in\nstorage account, such as the arrival or deletion of a file in Azure\nBlob Storage account. Data Factory natively integrates with Azure Event Grid, which lets\nyou trigger pipelines on such events.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger\nPage 348 of 818\n349 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Event-driven architecture (EDA) is a common data integration pattern that involves\nproduction, detection, consumption, and reaction to events. Data integration scenarios\noften require Data Factory customers to trigger pipelines based on events happening in\nstorage account, such as the arrival or deletion of a file in Azure\nBlob Storage account. Data Factory natively integrates with Azure Event Grid, which lets\nyou trigger pipelines on such events.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger"
  },
  {
    "number": "194",
    "question": "You plan to move two 100-GB databases to Azure.\nYou need to dynamically scale resources consumption based on workloads. The\nsolution must minimize downtime during scaling operations.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "An Azure SQL Database elastic pool",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "SQL Server on Azure virtual machines",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an Azure SQL Database managed instance",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL databases",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure SQL Database elastic pools are a simple, cost-effective solution for managing and\nscaling multiple databases that have varying and unpredictable usage demands. The\ndatabases in an elastic pool are on a single server and share a set number of resources\nat a set price.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"
  },
  {
    "number": "195",
    "question": "You have 10 Azure virtual machines that have SQL Server installed.\nYou need to implement a backup strategy to ensure that you can restore\nspecific databases to other SQL Server instances. The solution must provide\ncentralized management of the backups.\nWhat should you include in the backup strategy?",
    "options": [
      {
        "letter": "A",
        "text": "Automated Backup in the SQL virtual machine settings",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Backup",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Site Recovery\nPage 350 of 818\n351 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQL Server Agent jobs",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure Backup provides an Enterprise class backup capability for SQL Server on Azure\nVMs. All backups are stored and managed in a Recovery Services vault.\nThere are several advantages that this solution provides, especially for Enterprises.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/backup-\nrestore#azbackup"
  },
  {
    "number": "196",
    "question": "You need to recommend an availability strategy for an Azure SQL database. The\nstrategy must meet the following requirements:\n✑ Support failovers that do not require client applications to change their\nconnection strings.\n✑ Replicate the database to a secondary Azure region.\n✑ Support failover to the secondary region.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "failover groups",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "transactional replication",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Availability Zones",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "geo-replication",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "The auto-failover groups feature allows you to manage the replication and failover of\nsome or all databases on a logical server to another region.\nIncorrect Answers:\nNot C: Availability Zones are unique physical locations within a region. Each zone is made\nup of one or more datacenters equipped with independent power, cooling, and\nnetworking.\nNot D: Geo-replication failovers require client applications to change their connection\nstrings.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-\noverview https://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-\nreplication-overview"
  },
  {
    "number": "197",
    "question": "DRAG DROP -\nYou have SQL Server on an Azure virtual machine that contains a database\nnamed DB1. DB1 is 30 TB and has a 1-GB daily rate of change.\nYou back up the database by using a Microsoft SQL Server Agent job that runs\nTransact-SQL commands. You perform a weekly full backup on Sunday, daily\ndifferential backups at 01:00 in the morning, and transaction log backups every\nfive minutes.\nThe database fails on Wednesday at 10:00 in the morning.\nWhich three backups should you restore in sequence? To answer, move the\nappropriate backups from the list of backups to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/differential-\nbackups-sql-server\nCommunity Discussion\nAnswer is correct.\nFull -> Differential -> Log\nCorrect Answer as A differential backup is based on the most recent, previous full data\nbackup. https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/\ndifferential-backups-sql-server?view=sql-server-ver15\ncorrect\nAnswer is correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "198",
    "question": "You are building a database backup solution for a SQL Server database hosted\non an Azure virtual machine.\nIn the event of an Azure regional outage, you need to be able to restore the\ndatabase backups. The solution must minimize costs.\nWhich type of storage accounts should you use for the backups?",
    "options": [
      {
        "letter": "A",
        "text": "locally-redundant storage (LRS)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "read-access geo-redundant storage (RA-GRS)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "zone-redundant storage (ZRS)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "geo-redundant storage (GRS)",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Geo-redundant storage (GRS) is cheaper compared to read-access geo-redundant\nstorage (RA-GRS).\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical\nlocation in the secondary region to protect against regional outages. Data is available to\nbe read only if the customer or Microsoft initiates a failover from the primary to\nsecondary region.\nIncorrect Answers:\nA: Locally redundant storage (LRS) copies your data synchronously three times within a\nsingle physical location in the primary region. LRS is the least expensive replication\noption, but is not recommended for applications requiring high availability.\nB: RA-GRS is more expensive than GRS.\nNote: Geo-redundant storage (with GRS or GZRS) replicates your data to another physical\nlocation in the secondary region to protect against regional outages.\nHowever, that data is available to be read only if the customer or Microsoft initiates a\nfailover from the primary to secondary region. When you enable read access to the\nsecondary region, your data is available to be read if the primary region becomes\nunavailable. For read access to the secondary region, enable read-access geo-redundant\nstorage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).\nC: Zone-redundant storage (ZRS) copies your data synchronously across three Azure\navailability zones in the primary region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"
  },
  {
    "number": "199",
    "question": "You have SQL Server on Azure virtual machines in an availability group.\nYou have a database named DB1 that is NOT in the availability group.\nYou create a full database backup of DB1.\nYou need to add DB1 to the availability group.\nWhich restore option should you use on the secondary replica?",
    "options": [
      {
        "letter": "A",
        "text": "Restore with Recovery",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Restore with Norecovery",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Restore with Standby",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Prepare a secondary database for an Always On availability group requires two steps:\n1. Restore a recent database backup of the primary database and subsequent log\nbackups onto each server instance that hosts the secondary replica, using\nRESTORE WITH NORECOVERY -\n2. Join the restored database to the availability group.\nReference:\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/\nmanually-prepare-a-secondary-database-for-an-availability-group-sql-server"
  },
  {
    "number": "200",
    "question": "You are planning disaster recovery for the failover group of an Azure SQL\nDatabase managed instance.\nYour company's SLA requires that the database in the failover group become\navailable as quickly as possible if a major outage occurs.\nYou set the Read/Write failover policy to Automatic.\nWhat are two results of the configuration? Each correct answer presents a\ncomplete solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "In the event of a datacenter or Azure regional outage, the databases will fail over\nautomatically.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "In the event of an outage, the databases in the primary instance will fail over immediately.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "In the event of an outage, you can selectively fail over individual databases.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "In the event of an outage, you can set a different grace period to fail over each database.\nE) In the event of an outage, the minimum delay for the databases to fail over in the primary\ninstance will be one hour.",
        "is_correct": true
      }
    ],
    "correct_answer": "A",
    "explanation": "E\nA: Auto-failover groups allow you to manage replication and failover of a group of\ndatabases on a server or all databases in a managed instance to another region.\nE: Because verification of the scale of the outage and how quickly it can be mitigated\ninvolves human actions by the operations team, the grace period cannot be set below\none hour. This limitation applies to all databases in the failover group regardless of their\ndata synchronization state.\nIncorrect Answers:\nC: individual SQL Managed Instance databases cannot be added to or removed from a\nfailover group.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auto-failover-group-\noverview"
  },
  {
    "number": "201",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database Premium",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure SQL Database serverless",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database Basic",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Database Standard",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "By default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW). The routing to\na specific gateway ring is controlled by Azure Traffic Manager\n(ATM). Because the zone redundant configuration in the Premium or Business Critical\nservice tiers does not create additional database redundancy, you can enable it at no\nextra cost. By selecting a zone redundant configuration, you can make your Premium or"
  },
  {
    "number": "202",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure SQL database named Sales.\nYou need to implement disaster recovery for Sales to meet the following\nrequirements:\n✑ During normal operations, provide at least two readable copies of Sales.\n✑ Ensure that Sales remains available if a datacenter fails.\nSolution: You deploy an Azure SQL database that uses the General Purpose\nservice tier and geo-replication.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "on a single node. High availability is achieved by replicating both compute\nand storage to additional nodes creating a three to four- node cluster.\nBy default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW).\nPage 362 of 818\n363 Microsoft - DP-300 Practice Questions - SecExams.com\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead deploy an Azure SQL database that uses the Business Critical service tier and\nAvailability Zones.\nNote: Premium and Business Critical service tiers leverage the Premium availability\nmodel, which integrates compute resources (sqlservr.exe process) and storage (locally\nattached SSD) on a single node. High availability is achieved by replicating both compute\nand storage to additional nodes creating a three to four- node cluster.\nBy default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW)."
  },
  {
    "number": "203",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure SQL database named Sales.\nYou need to implement disaster recovery for Sales to meet the following\nrequirements:\n✑ During normal operations, provide at least two readable copies of Sales.\n✑ Ensure that Sales remains available if a datacenter fails.\nSolution: You deploy an Azure SQL database that uses the Business Critical\nservice tier and Availability Zones.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "on a single node. High availability is achieved by replicating both\ncompute and storage to additional nodes creating a three to four-node cluster.\nBy default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW).\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Premium and Business Critical service tiers leverage the Premium availability model,\nwhich integrates compute resources (sqlservr.exe process) and storage\n(locally attached SSD) on a single node. High availability is achieved by replicating both\ncompute and storage to additional nodes creating a three to four-node cluster.\nBy default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW).\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla"
  },
  {
    "number": "204",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have an Azure SQL database named Sales.\nYou need to implement disaster recovery for Sales to meet the following\nrequirements:\n✑ During normal operations, provide at least two readable copies of Sales.\n✑ Ensure that Sales remains available if a datacenter fails.\nSolution: You deploy an Azure SQL database that uses the General Purpose\nservice tier and failover groups.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "on a single node. High availability is achieved by replicating both compute\nand storage to additional nodes creating a three to four- node cluster.\nBy default, the cluster of nodes for the premium availability model is created in the\nsame datacenter. With the introduction of Azure Availability Zones, SQL\nDatabase can place different replicas of the Business Critical database to different\navailability zones in the same region. To eliminate a single point of failure, the control\nring is also duplicated across multiple zones as three gateway rings (GW).\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Instead deploy an Azure SQL database that uses the Business Critical service tier and"
  },
  {
    "number": "205",
    "question": "You plan to move two 100-GB databases to Azure.\nYou need to dynamically scale resources consumption based on workloads. The\nsolution must minimize downtime during scaling operations.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "two Azure SQL Databases in an elastic pool",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "two databases hosted in SQL Server on an Azure virtual machine",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "two databases in an Azure SQL Managed instance",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "two single Azure SQL databases",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure SQL Database elastic pools are a simple, cost-effective solution for managing and\nscaling multiple databases that have varying and unpredictable usage demands. The\ndatabases in an elastic pool are on a single server and share a set number of resources\nat a set price.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"
  },
  {
    "number": "206",
    "question": "You have an on-premises app named App1 that stores data in an on-premises\nMicrosoft SQL Server 2019 database named DB1.\nYou plan to deploy additional instances of App1 to separate Azure regions. Each\nregion will have a separate instance of App1 and DB1. The separate instances of\nDB1 will sync by using Azure SQL Data Sync.\nYou need to recommend a database service for the deployment. The solution\nmust minimize administrative effort.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Managed instance",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL Database single database",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Azure Database for PostgreSQL",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQL Server on Azure virtual machines",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Azure SQL Database single database supports Data Sync.\nIncorrect Answers:\nA: Azure SQL Managed instance does not support Data Sync.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/features-comparison"
  },
  {
    "number": "207",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have two Azure SQL Database servers named Server1 and Server2. Each\nserver contains an Azure SQL database named Database1.\nYou need to restore Database1 from Server1 to Server2. The solution must\nreplace the existing Database1 on Server2.\nSolution: From Microsoft SQL Server Management Studio (SSMS), you rename\nDatabase1 on Server2 as Database2. From the Azure portal, you create a new\ndatabase on Server2 by restoring the backup of Database1 from Server1, and\nthen you delete Database2.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "This procedure is necessary and correct, as there is no option in Azure SQL Database to\ndirectly restore a database.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/restore-statements-transact-sql"
  },
  {
    "number": "208",
    "question": "You have an Azure subscription that uses a domain named contoso.com.\nYou have two Azure VMs named DBServer1 and DBServer2. Each of them hosts a\ndefault SQL Server instance. DBServer1 is in the East US Azure region and\ncontains a database named DatabaseA. DBServer2 is in the West US Azure\nregion.\nDBServer1 has a high volume of data changes and low latency requirements for\ndata writes.\nYou need to configure a new availability group for DatabaseA. The secondary\nreplica will reside on DBServer2.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Configure the primary endpoint as TCP://DBServer1.contoso.com:445, configure the secondary\nendpoint as TCP://DBServer2.contoso.com:445, and set the availability mode to Asynchronous.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure the primary endpoint as TCP://DBServer1.contoso.com:445, configure the secondary\nendpoint as TCP://DBServer2.contoso.com:445, and set the availability mode to Synchronous.\nPage 370 of 818\n371 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure the primary endpoint as TCP://DBServer1.contoso.com:5022, configure the\nsecondary endpoint as TCP://DBServer2.contoso.com:5022, and set the availability mode to\nAsynchronous.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Configure the primary endpoint as TCP://DBServer1.contoso.com:5022, configure the\nsecondary endpoint as TCP://DBServer2.contoso.com:5022, and set the availability mode to\nSynchronous.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Use TCP port 5022, and use asynchronous mode for low latency.\nReference:\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/\navailability-modes-always-on-availability-groups?view=sql-server-ver15"
  },
  {
    "number": "209",
    "question": "You have an on-premises multi-tier application named App1 that includes a web\ntier, an application tier, and a Microsoft SQL Server tier. All the tiers run on\nHyper-\nV virtual machines.\nYour new disaster recovery plan requires that all business-critical applications\ncan be recovered to Azure.\nYou need to recommend a solution to fail over the database tier of App1 to\nAzure. The solution must provide the ability to test failover to Azure without\naffecting the current environment.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Backup",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Information Protection",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Windows Server Failover Cluster",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Site Recovery",
        "is_correct": true
      },
      {
        "letter": "A",
        "text": "of Azure VMs from a\nprimary to a secondary region.\nPage 372 of 818\n373 Microsoft - DP-300 Practice Questions - SecExams.com\nIts correct, you have a multi-tier application that includes a web tier, an application tier\nand MSQL server tier\nlooks good, what do you think?",
        "is_correct": false
      }
    ],
    "correct_answer": "D",
    "explanation": "You can use Azure Site Recovery to test failover.\nYou run a test failover to validate your replication and disaster recovery strategy, without\nany data loss or downtime. A test failover doesn't impact ongoing replication, or your\nproduction environment. You can run a test failover on a specific virtual machine (VM), or\non a recovery plan containing multiple VMs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/site-recovery/site-recovery-test-failover-to-\nazure"
  },
  {
    "number": "210",
    "question": "HOTSPOT -\nYou plan to migrate on-premises Microsoft SQL Server databases to Azure.\nYou need to identify which deployment and resiliency options meet the\nfollowing requirements:\n✑ Support user-initiated backups.\n✑ Support multiple automatically replicated instances across Azure regions.\n✑ Minimize administrative effort to implement and maintain business\ncontinuity.\nWhat should you identify? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Azure SQL Managed Instance\nManaged Instance will support native BACKUP like SQL Server.\nCOPY_ONLY Backup -\nOnce you create a credential, you can backup any database using standard BACKUP T-\nSQL command:\nBACKUP DATABASE tpcc2501 -\nTO URL = 'https://myacc.blob.core.windows.net/testcontainer/tpcc2501.bak'\nWITH COPY_ONLY -\nThe only mandatory property is COPY_ONLY. Azure SQL Managed Instance owns the\nbackup chain. This means that you cannot perform database backup that breaks the\nbackup chain. Azure SQL Managed Instance allows you to perform only COPY_ONLY\nbackups.\nBox 2: Auto-failover group -\nFor geographic failover of instances of SQL Managed Instance, use Auto-failover groups.\nThe auto-failover groups feature allows you to manage the replication and failover of\nsome or all databases on a logical server to another region.\nIncorrect:\nNot Active geo-replication: Active geo-replication is not supported by Azure SQL Managed\nInstance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/automated-backups-\n\n\noverview https://docs.microsoft.com/en-us/azure/azure-sql/database/auto-failover-\ngroup-sql-db\nCommunity Discussion\nThe answers are: Azure SQL Managed Instance Auto-failover group The requirement is to\nhave user initiated backup hence managed instance supports it. Since its managed\ninstanced, Geo-replication is not supported hence auto-failover groups\nI would go Azure SQL Managed Instance\" and \"Auto-failover group\". \"Active geo-\nreplication\" is applied for Azure SQL Database.\nThe correct Answers is \"Azure SQL Databases\" and \"Geo-replication\". \"Minimize\nadministrative effort to implement and maintain business continuity.\" is not in Virtual\nMachine and the question does not especify that type of Fail Over you want, but especify\nmultiple replications.\n\"Active geo-replication is not supported by Azure SQL Managed Instance but Auto\nFailover groups is supported.\" https://docs.microsoft.com/en-us/answers/questions/\n37772/failover-groups-or-geo-replication.html\nAzure SQL Managed Instance & Auto-failover group 1. Azure SQL Database single\ndatabase does not support user-initiated backups: - https://learn.microsoft.com/en-us/\nazure/azure-sql/database/features-comparison?view=azuresql Therefore, the answer\nmust be Azure SQL Managed Instance or SQL Server on Azure Virtual Machines. 2. Active\ngeo-replication is only supported by Azure SQL Database - https://learn.microsoft.com/\nen-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql\nTherefore, the answer must be either Auto-failover group or Zone-redundant\ndeployment. 3. Zone-redundant deployment does not work across multiple Azure\nRegions - https://learn.microsoft.com/en-us/azure/storage/common/storage-\nredundancy Therefore, the answer must be Auto-failover group. 4. Auto-failover group is\nonly supported by Azure SQL Managed Instance - https://learn.microsoft.com/en-us/\nazure/azure-sql/managed-instance/auto-failover-group-sql-mi?\nview=azuresql&tabs=azure-powershell Therefore, the answer must be Azure SQL\nManaged Instance & Auto-failover group.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "211",
    "question": "HOTSPOT -\nYou configure a long-term retention policy for an Azure SQL database as shown\nin the exhibit. (Click the Exhibit tab.)\nThe first weekly backup occurred on January 4, 2020. The dates for the first 10\nweekly backups are:\n✑ January 4, 2020\n✑ January 11, 2020\n✑ January 18, 2020\n✑ January 25, 2020\n✑ February 1, 2020\n✑ February 8, 2020\n\n\n✑ February 15, 2020\n✑ February 22, 2020\n✑ February 29, 2020\n✑ March 7, 2020\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: 12 months -\nThe January 4, 2020 backup is the first backup of the month, and it will keep for 12\nmonths.\nBox 2: 10 years.\nThe January 11, 2020 backup is the Week 2 backup. This backup will be the yearly backup,\nand it will be retained for 10 years.\nNote: Long-term backup retention (LTR) leverages the full database backups that are\n\n\nautomatically created to enable point in time restore (PITR). If an LTR policy is\nconfigured, these backups are copied to different blobs for long-term storage. The copy is\na background job that has no performance impact on the database workload. The LTR\npolicy for each database in SQL Database can also specify how frequently the LTR\nbackups are created.\nTo enable LTR, you can define a policy using a combination of four parameters: weekly\nbackup retention (W), monthly backup retention (M), yearly backup retention (Y), and\nweek of year (WeekOfYear). If you specify W, one backup every week will be copied to the\nlong-term storage. If you specify M, the first backup of each month will be copied to the\nlong-term storage. If you specify Y, one backup during the week specified by WeekOfYear\nwill be copied to the long-term storage. If the specified WeekOfYear is in the past when\nthe policy is configured, the first LTR backup will be created in the following year. Each\nbackup will be kept in the long-term storage according to the policy parameters that are\nconfigured when the LTR backup is created.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/long-term-retention-\noverview?view=azuresql\nCommunity Discussion\nAnswer is correct\nAnswer is correct\nlooks correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "212",
    "question": "You have a new Azure subscription.\nYou create an Azure SQL Database instance named DB1 on an Azure SQL\nDatabase server named Server1.\nYou need to ensure that users can connect to DB1 in the event of an Azure\nregional outage. In the event of an outage, applications that connect to DB1\nmust be able to connect without having to update the connection strings.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "From the properties of DB1, configure geo-replication.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "From the properties of Server1, add a failover group.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a new Azure SQL Database server named Server2.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "From the properties of Server1, configure retention for DB1.\nE) Create a new Azure SQL Database instance named DB2.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "C\nB: The auto-failover groups feature allows you to manage the replication and failover of\nsome or all databases on a logical server to another region.\nC: The auto-failover group must be configured on the primary server and will connect it\nto the secondary server in a different Azure region. T"
  },
  {
    "number": "213",
    "question": "HOTSPOT -\nYou have an Azure SQL database.\nYou run the following PowerShell script.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nShort term retention is set to 21 days.\nNote: Short term retention policy is used to address point-in-time restores whereas a\nlong-term retention policy is used to address restores from long-term or older backups\nfor various audit and compliance purposes. We can also save these backup files as part\nof a short-term retention policy for up to 7-35 days.\nBox 2: Yes -\nWeekly retention is set to P52W, which stands for a periodic frequency of 52 weeks (364\ndays).\nNote: -WeeklyRetention -\nThe Weekly Retention. If just a number is passed instead of an ISO 8601 string, days will\nbe assumed as the units. There is a minimum of 7 days and a maximum of 10 years.\nBox 3: No -\nYearly retention is set to PSY.\nNote: -YearlyRetention -\nThe Yearly Retention. If just a number is passed instead of an ISO 8601 string, days will\nbe assumed as the units. There is a minimum of 7 days and a maximum of 10 years.\nReference:\nhttps://strata.opengamma.io/apidocs/com/opengamma/strata/basics/schedule/\nFrequency.html https://docs.microsoft.com/en-us/powershell/module/az.sql/set-\nazsqldatabasebackupshorttermretentionpolicy?view=azps-7.2.0 https://\ndocs.microsoft.com/en-us/powershell/module/az.sql/set-\nazsqldatabasebackuplongtermretentionpolicy?view=azps-7.2.0\nCommunity Discussion\nThe PSY should be P5Y\n\n\nAnswers given are correct. As mentioned by jackdong PSY should be P5Y. Examples: #\ncreate LTR policy with WeeklyRetention = 12 weeks. MonthlyRetention and\nYearlyRetention = 0 by default. Set-AzSqlDatabaseBackupLongTermRetentionPolicy -\nServerName $serverName -DatabaseName $dbName ` -ResourceGroupName\n$resourceGroup -WeeklyRetention P12W # create LTR policy with WeeklyRetention = 12\nweeks, YearlyRetention = 5 years and WeekOfYear = 16 (week of April 15).\nMonthlyRetention = 0 by default. Set-AzSqlDatabaseBackupLongTermRetentionPolicy -\nServerName $serverName -DatabaseName $dbName ` -ResourceGroupName\n$resourceGroup -WeeklyRetention P12W -YearlyRetention P5Y -WeekOfYear 16 https://\nlearn.microsoft.com/en-us/azure/azure-sql/database/long-term-backup-retention-\nconfigure?view=azuresql&tabs=powershell\nAnswer is correct\nWhat is \"PSY\" ?\nnot seeing valid reference",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "214",
    "question": "HOTSPOT -\nYou have an Azure SQL managed instance.\nYou need to restore a database named DB1 by using Transact-SQL.\nWhich command should you run? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nSyntax for Azure SQL Managed Instance for the restore command:\n--To Restore an Entire Database from a Full database backup (a Complete Restore):\nRESTORE DATABASE { database_name | @database_name_var }\nFROM URL = { 'physical_device_name' | @physical_device_name_var } [ ,...n ]\n[;]\nArguments -\nDATABASE -\nSpecifies the target database.\nFROM URL -\nSpecifies one or more backup devices placed on URLs that will be used for the restore\noperation. The URL format is used for restoring backups from the Microsoft\nAzure storage service.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/restore-statements-transact-\nsql?view=azuresqldb-mi-current&preserve-view=true\nCommunity Discussion\nhttps://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/restore-sample-\ndatabase-quickstart?view=azuresql\nAnswer is correct\nExample applies to SSMS GUI and not T-SQL commands.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "215",
    "question": "DRAG DROP -\nYou have an Azure subscription that contains the resources shown in the\nfollowing table.\nYou need to back up db1 to mysqlbackups, and then restore the backup to a\nnew database named db2 that is hosted on SQL1. The solution must ensure that\ndb1 is backed up to a stripe set.\nWhich three Transact-SQL statements should you execute in sequence? To\nanswer, move the appropriate statements from the list of statements to the\nanswer area and arrange them in the correct order.\n\n\nSelect and Place:\n\n\n\n\nExplanation\nCorrect Answer:\n\n\nBox 1: CREATE CREDENTIAL -\n\n\n[https://<\nCreate a credential.\nThe following examples create SQL Server credentials for authentication to the Microsoft\nAzure Blob Storage service.\nCREATE CREDENTIAL [https://<mystorageaccountname>.blob.core.windows.net/\n<mystorageaccountcontainername>]\nWITH IDENTITY = 'SHARED ACCESS SIGNATURE',\nSECRET = '<SAS_TOKEN>';\nIncorrect:\nThe other CREATE CREDENTIAL option does not refer to mysqlbackups - the name of the\naccount.\nBox 2: BACKUP..\nDo no include WITH CREDENTIAL when backing up To URL using Shared Access Signature.\nExample:\nBACKUP DATABASE AdventureWorks2016\nTO URL = 'https://<mystorageaccountname>.blob.core.windows.net/<mycontainername>/\nAdventureWorks2016.bak';\nGO -\nBox 3: RESTORE -\nDo no include WITH CREDENTIAL when restoring up To URL using Shared Access\nSignature.\nExample: BACKUP DATABASE AdventureWorks2016\nTO URL = 'https://<mystorageaccountname>.blob.core.windows.net/<mycontainername>/\nAdventureWorks2016.bak';\nGO -\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/sql-server-\nbackup-to-url?view=sql-server-ver15\nCommunity Discussion\nAnswer is correct\nmeaning the WITH CREDENTIAL in this case is not supported\nlet's just say that M$ does not employ the brightest bulbs to write these questions...\n\n\nlet's just say that M$ does not employ the brightest bulbs to write these questions...\nmeaning the WITH CREDENTIAL in this case is not supported",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "216",
    "question": "HOTSPOT -\nYou have an Azure subscription that contains the resources shown in the\nfollowing table.\nYou need to create a read-only replica of DB1 and configure the App1 instances\nto use the replica.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Create a replicate on the same logical server.\nA High Availability (HA) replica uses the same page servers as the primary replica.\nA named replica, just like an HA replica, uses the same page servers as the primary\nreplica.\nBox 2: Add an ApplicationIntent entry to the connection string.\nConnecting to an HA replica.\nIn Hyperscale databases, the ApplicationIntent argument in the connection string used\nby the client dictates whether the connection is routed to the read-write primary replica\nor to a read-only HA replica. If ApplicationIntent is set to ReadOnly and the database\ndoesn't have a secondary replica, connection will be routed to the primary replica and\nwill default to the ReadWrite behavior.\nExample:\n-- Connection string with application intent\nServer=tcp:<myserver>.database.windows.net;Database=<mydatabase>;ApplicationIntent=ReadOnly;User\nID=<myLogin>;Password=<myPassword>;Trusted_Connection=False; Encrypt=True;\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-\nreplicas?view=azuresql&tabs=tsql https://sqlserverguides.com/read-only-replica-azure-\nsql/\nCommunity Discussion\nIt is correct, but geo-replication is also a valid option for creating a readable secondary\naccording to the article https://docs.microsoft.com/en-us/azure/azure-sql/database/\nservice-tier-hyperscale-replicas?view=azuresql&tabs=tsql https://sqlserverguides.com/\nread-only-replica-azure-sql/\nFor Hypescale Service Tier, you can create High Availability replica. High Availability\nreplica A High Availability (HA) replica uses the same page servers as the primary replica,\nso no data copy is required to add an HA replica. HA replicas are mainly used to increase\ndatabase availability; they act as hot standbys for failover purposes. https://\nlearn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-replicas?\nview=azuresql&tabs=tsql#high-availability-replica\n\n\nFor Hypescale Service Tier, you can create High Availability replica. High Availability\nreplica A High Availability (HA) replica uses the same page servers as the primary replica,\nso no data copy is required to add an HA replica. HA replicas are mainly used to increase\ndatabase availability; they act as hot standbys for failover purposes. https://\nlearn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-replicas?\nview=azuresql&tabs=tsql#high-availability-replica\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-\nreplicas?view=azuresql&tabs=tsql#high-availability-replica - answer is correct. Replica\nwith ApplicationIntent\nThe answer is not correct. We can't create a replica on the same logical server. We need\nto create or use a different logical server and create a geo-replica (Data management >\nReplica)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "217",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have two Azure SQL Database servers named Server1 and Server2. Each\nserver contains an Azure SQL database named Database1.\nYou need to restore Database1 from Server1 to Server2. The solution must\nreplace the existing Database1 on Server2.\nSolution: From the Azure portal, you delete Database1 from Server2, and then\nyou create a new database on Server2 by using the backup of Database1 from\nServer1.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "218",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have two Azure SQL Database servers named Server1 and Server2. Each\nserver contains an Azure SQL database named Database1.\nYou need to restore Database1 from Server1 to Server2. The solution must\nreplace the existing Database1 on Server2.\nSolution: You run the Remove-AzSqlDatabase PowerShell cmdlet for Database1\non Server2. You run the Restore-AzSqlDatabase PowerShell cmdlet for\nDatabase1 on Server2.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No. ???\nSorry, answer is actually Yes. You can do it using the \"-FromGeoBackup\" parameter.\nTested myself.\nSorry, answer is actually Yes. You can do it using the \"-FromGeoBackup\" parameter.\nTested myself.\nNo. Azure SQL Database does not currently support cross server restore. The source and\ntarget server names must be the same. https://docs.microsoft.com/en-us/powershell/\nmodule/servicemanagement/azure.service/start-azuresqldatabaserestore?\nview=azuresmps-4.0.0",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Correct solution: From the Azure portal, you delete Database1 from Server2, and then you\ncreate a new database on Server2 by using the backup of Database1 from Server1.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/recovery-using-backups"
  },
  {
    "number": "219",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have two Azure SQL Database servers named Server1 and Server2. Each\nserver contains an Azure SQL database named Database1.\nYou need to restore Database1 from Server1 to Server2. The solution must\nreplace the existing Database1 on Server2.\nSolution: You restore the backup for Database1 from Server1 to the Server2 by\nusing the RESTORE Transact-SQL command with the REPLACE option.\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "220",
    "question": "You have an Always On availability group deployed to Azure virtual machines.\nThe availability group contains a database named DB1 and has two nodes\nnamed\nSQL1 and SQL2. SQL1 is the primary replica.\nYou need to initiate a full backup of DB1 on SQL2.\nWhich statement should you run?",
    "options": [
      {
        "letter": "A",
        "text": "BACKUP DATABASE DB1 TO URL='https://mystorageaccount.blob.core.windows.net/\nmycontainer/DB1.bak' with (Differential, STATS=5, COMPRESSION);",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "BACKUP DATABASE DB1 TO URL='https://mystorageaccount.blob.core.windows.net/\nmycontainer/DB1.bak' with (COPY_ONLY, STATS=5, COMPRESSION);",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "BACKUP DATABASE DB1 TO URL='https://mystorageaccount.blob.core.windows.net/\nmycontainer/DB1.bak' with (File_Snapshot, STATS=5, COMPRESSION);",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "BACKUP DATABASE DB1 TO URL='https://mystorageaccount.blob.core.windows.net/\nmycontainer/DB1.bak' with (NoInit, STATS=5, COMPRESSION);",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "BACKUP DATABASE supports only copy-only full backups of databases, files, or filegroups\nwhen it's executed on secondary replicas. Copy-only backups don't impact the log chain\nor clear the differential bitmap.\nIncorrect Answers:\nA: Differential backups are not supported on secondary replicas. The software displays\nthis error because the secondary replicas support copy-only database backups.\nReference:\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/\nactive-secondaries-backup-on-secondary-replicas-always-on-availability-groups"
  },
  {
    "number": "221",
    "question": "HOTSPOT -\nYou have a SQL Server on Azure Virtual Machines instance named VM1 that\nhosts a database named DB1.\nYou run the following query.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: No -\nA copy-only log backup preserves the existing log archive point and, therefore, does not\naffect the sequencing of regular log backups.\nBox 2: No -\nBox 3: Yes -\nIf the database is online and you plan to perform a restore operation on the database,\nbegin by backing up the tail of the log. To avoid an error for an online database, you\nmust use the ... WITH NORECOVERY option of the BACKUP Transact-SQL statement.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/tail-log-\nbackups-sql-server https://docs.microsoft.com/en-us/sql/relational-databases/backup-\nrestore/copy-only-backups-sql-server\nCommunity Discussion\nhttps://learn.microsoft.com/en-us/sql/relational-databases/backup-restore/tail-log-\nbackups-sql-server?view=sql-server-ver16 Correct\nwith NORECOVERY option, the db will be placed in RESTORING state, so while technically\nnot offline, still inaccessible...\nabsolutely correct - there is a major difference between offline and not accessible\n(restoring state). so the answer is correct.\n\n\nabsolutely correct - there is a major difference between offline and not accessible\n(restoring state). so the answer is correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "222",
    "question": "You have an Azure SQL database named DB3.\nYou need to provide a user named DevUser with the ability to view the\nproperties of DB3 from Microsoft SQL Server Management Studio (SSMS) as\nshown in the exhibit. (Click the Exhibit tab.)\nWhich Transact-SQL command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "GRANT SHOWPLAN TO DevUser",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "GRANT VIEW DEFINITION TO DevUser",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "GRANT VIEW DATABASE STATE TO DevUser",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "GRANT SELECT TO DevUser\nPage 404 of 818\n405 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The exhibits displays Database [State] properties.\nTo query a dynamic management view or function requires SELECT permission on object\nand VIEW SERVER STATE or VIEW DATABASE STATE permission.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/databases/database-\nproperties-options-page"
  },
  {
    "number": "223",
    "question": "HOTSPOT -\nYou have SQL Server on an Azure virtual machine that contains a database\nnamed DB1.\nThe database reports a CHECKSUM error.\nYou need to recover the database.\nHow should you complete the statements? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: SINGLE_USER -\nThe specified database must be in single-user mode to use one of the following repair\noptions.\nBox 2: REPAIR_ALLOW_DATA_LOSS -\nREPAIR_ALLOW_DATA_LOSS tries to repair all reported errors. These repairs can cause\nsome data loss.\nNote: The REPAIR_ALLOW_DATA_LOSS option is a supported feature but it may not always\nbe the best option for bringing a database to a physically consistent state. If successful,\nthe REPAIR_ALLOW_DATA_LOSS option may result in some data loss. In fact, it may result\nin more data lost than if a user were to restore the database from the last known good\nbackup.\nIncorrect Answers:\nREPAIR_FAST -\nMaintains syntax for backward compatibility only. No repair actions are performed.\nBox 3: MULTI_USER -\nMULTI_USER -\nAll users that have the appropriate permissions to connect to the database are allowed.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-\ncheckdb-transact-sql\n\n\nCommunity Discussion\nfor Box 2, the answer is correct. However the wrong option is high-lighted.\nFor any Admins or Admins to be...these answers present the wrong way of doing this in\nthe real world. The correct real world methodology (assuming you have all the time in\nthe world which you seldom will have) is to recover from last known good backup. If you\nwant to fix the database then first set the database in emergency mode not single_user.\nThis prevents another thread grabbing the access to the database before you. Then you\nwould run DBCC CHECKDB with the REPAIR_REBUILD option. This will not cause data loss.\nAfter that because you're the only one with access from a query window now issue the\nALTER DATABASE SET SINGLE_USER statement and, if approved, use the\nREPAIR_ALLOW_DATA_LOSS statement and then set the database back to multiuser.\nSINGLE_USER REPAIR_ALLOW_DATA_LOSS MULTI USER REPAIR_FAST doesn't actually make\nany changes.\nYes you are tight\nSo misleading.. some of the discussions here. The correct answer is mentioned in the\nexplanation itself. Incorrect Answers: REPAIR_FAST - Maintains syntax for backward\ncompatibility only. No repair actions are performed. REPAIR_ALLOW_DATA_LOSS is correct\nanswer for Box2",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "224",
    "question": "HOTSPOT -\nYou have an Azure SQL Database managed instance named sqldbmi1 that\ncontains a database name Sales.\nYou need to initiate a backup of Sales.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: TO URL = 'https://storage1.blob.core.windows.net/blob1/Sales.bak'\nNative database backup in Azure SQL Managed Instance.\nYou can backup any database using standard BACKUP T-SQL command:\nBACKUP DATABASE tpcc2501 -\nTO URL = 'https://myacc.blob.core.windows.net/testcontainer/tpcc2501.bak'\nWITH COPY_ONLY -\nBox 2: COPY_ONLY -\nReference:\nhttps://techcommunity.microsoft.com/t5/azure-sql-database/native-database-backup-\nin-azure-sql-managed-instance/ba-p/386154\nCommunity Discussion\n1.URL : SQLMI is paas on cloud, so backup must stored on blob storage 2.COPY_ONLY:\nSQLMI has \"managed\" everything, includes backup chain, so user can not break it's chain\nby using different method than COPY_ONLY.\nOption 2 is wrong because in Azure SQL PaaS you don't have any access to any disk or\nthe operating system at all\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/transact-sql-tsql-\ndifferences-sql-server SQL Managed Instance has automatic backups, so users can create\nfull database COPY_ONLY backups. Differential, log, and file snapshot backups aren't\n\n\nsupported. With a SQL Managed Instance, you can back up an instance database only to\nan Azure Blob storage account: Only BACKUP TO URL is supported. FILE, TAPE, and backup\ndevices aren't supported. Most of the general WITH options are supported. COPY_ONLY is\nmandatory. FILE_SNAPSHOT isn't supported. Tape options: REWIND, NOREWIND, UNLOAD,\nand NOUNLOAD aren't supported. Log-specific options: NORECOVERY, STANDBY, and\nNO_TRUNCATE aren't supported.\nit is trying to store backup file within the blob storage. what is the problem!\nI think the comments on here do more harm than good...",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "225",
    "question": "You have the following Transact-SQL query.\nWhich column returned by the query represents the free space in each file?",
    "options": [
      {
        "letter": "A",
        "text": "ColumnA",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "ColumnB",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "ColumnC",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "ColumnD",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Example:\nFree space for the file in the below query result set will be returned by the FreeSpaceMB"
  },
  {
    "number": "226",
    "question": "HOTSPOT -\nYou have an Azure Data Lake Storage Gen2 account named account1 that stores\nlogs as shown in the following table.\nYou do not expect that the logs will be accessed during the retention periods.\nYou need to recommend a solution for account1 that meets the following\nrequirements:\n✑ Automatically deletes the logs at the end of each retention period\n✑ Minimizes storage costs\nWhat should you include in the recommendation? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Store the infrastructure logs in the Cool access tier the application logs in the\n\n\nArchive access tier\nHot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30\ndays.\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180\ndays with flexible latency requirements, on the order of hours.\nBox 2: Azure Blob storage lifecycle management rules\nBlob storage lifecycle management offers a rich, rule-based policy that you can use to\ntransition your data to the best access tier and to expire data at the end of its lifecycle.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\nCommunity Discussion\nGiven answer is correct.\nYou are wrong. Data in the Archive tier should be stored for a minimum of 180 days.\nYou are wrong. Data in the Archive tier should be stored for a minimum of 180 days.\nWhy the cool tier for the infrastructure logs? If the costs must be minimized it would be\ncheaper to put both in archive\nThis question belongs to the exam DP-203",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "227",
    "question": "HOTSPOT -\nYou have an Azure Data Lake Storage Gen2 container.\nData is ingested into the container, and then transformed by a data integration\napplication. The data is NOT modified after that. Users can read files in the\ncontainer but cannot modify the files.\nYou need to design a data archiving solution that meets the following\nrequirements:\n✑ New data is accessed frequently and must be available as quickly as possible.\n✑ Data that is older than five years is accessed infrequently but must be\navailable within one second when requested.\n✑ Data that us older than seven years is NOT accessed. After seven years, the\ndata must be persisted at the lowest cost possible.\n✑ Costs must be minimized while maintaining the required availability.\nHow should you manage the data? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: Move to cool storage -\nThe cool access tier has lower storage costs and higher access costs compared to hot\nstorage. This tier is intended for data that will remain in the cool tier for at least 30 days.\nExample usage scenarios for the cool access tier include:\nShort-term backup and disaster recovery\nOlder data not used frequently but expected to be available immediately when accessed\nLarge data sets that need to be stored cost effectively, while more data is being gathered\nfor future processing\nNote: Hot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30\ndays.\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180\ndays with flexible latency requirements, on the order of hours.\nBox 2: Move to archive storage -\nExample usage scenarios for the archive access tier include:\n✑ Long-term backup, secondary backup, and archival datasets\n✑ Original (raw) data that must be preserved, even after it has been processed into final\nusable form\n✑ Compliance and archival data that needs to be stored for a long time and is hardly\never accessed\n\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers\nCommunity Discussion\nGiven answer is correct.\nThis question belongs to the exam DP-203\nCorrect\nAnswer is correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "228",
    "question": "You plan to perform batch processing in Azure Databricks once daily.\nWhich type of Databricks cluster should you use?",
    "options": [
      {
        "letter": "A",
        "text": "automated",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "interactive",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "High Concurrency",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Azure Databricks makes a distinction between all-purpose clusters and job clusters. You\nuse all-purpose clusters to analyze data collaboratively using interactive notebooks. You\nuse job clusters to run fast and robust automated jobs.\nThe Azure Databricks job scheduler creates a job cluster when you run a job on a new\njob cluster and terminates the cluster when the job is complete.\nReference:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters"
  },
  {
    "number": "229",
    "question": "HOTSPOT -\nYou have two Azure virtual machines named VM1 and VM2 that run Windows\nServer 2019. VM1 and VM2 each host a default Microsoft SQL Server 2019\ninstance. VM1 contains a database named DB1 that is backed up to a file named\nD:\\DB1.bak.\nYou plan to deploy an Always On availability group that will have the following\nconfigurations:\n✑ VM1 will host the primary replica of DB1.\n✑ VM2 will host a secondary replica of DB1.\nYou need to prepare the secondary database on VM2 for the availability group.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: RESTORE -\nUse RESTORE WITH NORECOVERY for every restore operation.\nBOX 2: NORECOVERY -\nReference:\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/\nmanually-prepare-a-secondary-database-for-an-availability-group-sql-server? view=sql-\nserver-ver15\nCommunity Discussion\nAnswer is correct RESTORE WITH NORECOVERY\nThis is correct RESTORE WITH NORECOVERY\nit looks good, what do you think?\nIts OK, in the web say \"Use RESTORE WITH NORECOVERY for every restore operation.\nPrerequisites and restrictions Make sure that the system where you plan to place\ndatabase possesses a disk drive with sufficient space for the secondary databases. The\nname of the secondary database must be the same as the name of the primary\ndatabase. Use RESTORE WITH NORECOVERY for every restore operation. If the secondary\n\n\ndatabase needs to reside on a different file path (including the drive letter) than the\nprimary database, the restore command must also use the WITH MOVE option for each of\nthe database files to specify them to the path of the secondary database. If you restore\nthe database filegroup by filegroup, be sure to restore the whole database. After\nrestoring the database, you must restore (WITH NORECOVERY) every log backup created\nsince the last restored data backup. \"",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "230",
    "question": "DRAG DROP -\nYou have an Azure Active Directory (Azure AD) tenant named contoso.com that\ncontains a user named [email protected] and an Azure SQL managed instance\nnamed SQLMI1.\nYou need to ensure that [email protected] can create logins in SQLMI1 that map\nto Azure AD service principals.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: Grant SQLMI1 read access to Azure AD\nGive required permission.\nBox 2: Run CREATE LOGIN..\nCreate a login.\nBox 3: RUN ALTER SERVER..\nAdd a user to the login created in step 2.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/aad-security-\nconfigure-tutorial\nCommunity Discussion\nAnswer is correct: - Grant MI reader access to AD - Create login - Grant sysadmin or\nsecurity admin permissions\nAnswer is correct: - Grant MI reader access to AD - Create login - Grant sysadmin or\nsecurity admin permissions\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-\nconfigure?view=azuresql-mi&tabs=azure-powershell - grant read access first.\n- Run Create Login from External - Run Create User from login - Run Alter permissions for\nserver role add user\nDisregard, given answer is correct",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "231",
    "question": "HOTSPOT -\nYou have a 50-TB Microsoft SQL Server database named DB1.\nYou need to reduce the time it takes to perform database consistency checks of\nDB1.\nWhich Transact-SQL command should you run? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: NOINDEX -\nNOINDEX specifies that intensive checks of nonclustered indexes for user tables will not\nbe performed. This choice decreases the overall execution time.\nNOINDEX doesn't affect system tables because integrity checks are always performed on\nsystem table indexes.\nBox 2: PHYSICAL_ONLY -\nMany new checks have been introduced to include the new features.\nTherefore, using the PHYSICAL_ONLY option may cause a much shorter run-time for DBCC\nCHECKDB on large databases and is recommended for frequent use on production\nsystems.\n\n\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-\ncheckdb-transact-sql?view=sql-server-ver15\nCommunity Discussion\nAnswer is correct: NOINDEX Specifies that intensive checks of nonclustered indexes for\nuser tables will not be performed. This choice decreases the overall execution time.\nNOINDEX doesn't affect system tables because integrity checks are always performed on\nsystem table indexes. PHYSICAL_ONLY Limits the checking to the integrity of the physical\nstructure of the page and record headers and the allocation consistency of the database.\nThis check is designed to provide a small overhead check of the physical consistency of\nthe database, but it can also detect torn pages, checksum failures, and common\nhardware failures that can compromise a user's data. https://docs.microsoft.com/en-us/\nsql/t-sql/database-console-commands/dbcc-checkdb-transact-sql?view=sql-server-\nver15\nsorry the second option is PHYSICAL_ONLY\nsorry the second option is PHYSICAL_ONLY\nNOINDEX EXTENDED_LOGICAL_CHECKS",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "232",
    "question": "You have an Azure SQL database named DB1 that contains a private certificate\nnamed Sales. The private key for Sales is encrypted with a password.\nYou need to change the password for the private key.\nWhich Transact-SQL statement should you run?",
    "options": [
      {
        "letter": "A",
        "text": "ALTER CERTIFICATE Sales WITH PRIVATE KEY (DECRYPTION BY PASSWORD = ' EWYx9Xk+$#');",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "ALTER CERTIFICATE Sales WITH PRIVATE KEY (ENCRYPTION BY PASSWORD = ' 6YY9YcD!pV');",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "ALTER CERTIFICATE Sales WITH PRIVATE KEY (DECRYPTION BY PASSWORD = 'Mb^68K&*w%'),\nENCRYPTION BY PASSWORD = ' 6YY9YcD!pV');",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "ALTER CERTIFICATE Sales WITH PRIVATE KEY (FILE = 'D:\\importkeys\\SalesNew, (DECRYPTION BY\nPASSWORD = ' Mb^68K&*w%');\nPage 424 of 818\n425 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Azure SQL database ALTER CERTIFICATE changes the password used to encrypt the private\nkey of a certificate, removes the private key, or imports the private key if none is present.\nExample, changing the password that is used to encrypt the private key:\nALTER CERTIFICATE Shipping11 -\nWITH PRIVATE KEY (DECRYPTION BY PASSWORD = '95hkjdskghFDGGG4%',\nENCRYPTION BY PASSWORD = '34958tosdgfkh##38');\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/alter-certificate-transact-sql"
  },
  {
    "number": "233",
    "question": "HOTSPOT -\nYou have a SQL Server on Azure Virtual Machines instance that hosts a 10-TB\nSQL database named DB1.\nYou need to identify and repair any physical or logical corruption in DB1. The\nsolution must meet the following requirements:\n✑ Minimize how long it takes to complete the procedure.\n✑ Minimize data loss.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: REPAIR_REBUILD -\nREPAIR_REBUILD -\nPerforms repairs that have no possibility of data loss. This option may include quick\nrepairs, such as repairing missing rows in nonclustered indexes, and more time-\nconsuming repairs, such as rebuilding an index.\nBox 2: PHYSICAL_ONLY -\nPHYSICAL_ONLY -\n\n\nLimits the checking to the integrity of the physical structure of the page and record\nheaders and the allocation consistency of the database. This check is designed to\nprovide a small overhead check of the physical consistency of the database, but it can\nalso detect torn pages, checksum failures, and common hardware failures that can\ncompromise a user's data.\nIncorrect:\nTABLOCK -\nCauses DBCC CHECKDB to obtain locks instead of using an internal database snapshot.\nThis includes a short-term exclusive (X) lock on the database.\nTABLOCK will cause DBCC CHECKDB to run faster on a database under heavy load, but will\ndecrease the concurrency available on the database while DBCC\nCHECKDB is running.\nEXTENDED_LOGICAL_CHECKS -\nIf the compatibility level is 100 ( SQL Server 2008) or higher, performs logical consistency\nchecks on an indexed view, XML indexes, and spatial indexes, where present.\nReference:\nhttps://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-\ncheckdb-transact-sql\nCommunity Discussion\nthey are different, the key in this question: ✑ Minimize how long it takes to complete the\nprocedure. (physical only) ✑ Minimize data loss (Repair_Rebuild) So I think the answer is\ncorrect with those 2 requirements\nthey are different, the key in this question: ✑ Minimize how long it takes to complete the\nprocedure. (physical only) ✑ Minimize data loss (Repair_Rebuild) So I think the answer is\ncorrect with those 2 requirements\nThe answer is correct.\nNOINDEX - only check, no repair EXTENDED_LOGICAL_CHECKS - time-consuming TABLOCK\n- lock the whole table, time-consuming\nNOINDEX - only check, no repair EXTENDED_LOGICAL_CHECKS - time-consuming TABLOCK\n- lock the whole table, time-consuming",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "234",
    "question": "HOTSPOT -\nYou have an Azure SQL database named DB1.\nYou need to identify how much unused space in megabytes was allocated to\nDB1.\nHow should you complete the Transact-SQL query? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: 128.0 -\n-- Connect to database\n\n\n-- Database data space allocated in MB and database data space allocated unused in MB\nSELECT SUM(size/128.0) AS DatabaseDataSpaceAllocatedInMB,\nSUM(size/128.0 - CAST(FILEPROPERTY(name, 'SpaceUsed') AS int)/128.0) AS\nDatabaseDataSpaceAllocatedUnusedInMB\nFROM sys.database_files -\nGROUP BY type_desc -\nHAVING type_desc = 'ROWS'\nBox 2: sys.database_files -\nReference:\nhttps://techcommunity.microsoft.com/t5/azure-database-support-blog/how-to-get-\nazure-sql-database-size/ba-p/369189\nCommunity Discussion\nCorrect https://learn.microsoft.com/en-us/azure/azure-sql/database/file-space-\nmanage?view=azuresql",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "235",
    "question": "HOTSPOT -\nYou have an Azure SQL database.\nYou need to identify whether a delayed query execution is associated to a\nRESOURCE_SEMAPHORE wait.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: wait_type -\nDetermine if a RESOURCE_SEMAHPORE wait is a top wait\nUse the following query to determine if a RESOURCE_SEMAHPORE wait is a top wait\nSELECT wait_type,\nSUM(wait_time) AS total_wait_time_ms\nFROM sys.dm_exec_requests AS req\nJOIN sys.dm_exec_sessions AS sess\nON req.session_id = sess.session_id\nWHERE is_user_process = 1 -\nGROUP BY wait_type -\nORDER BY SUM(wait_time) DESC;\nBox 2: sys.dm_exec_requests -\nUse the sys.dm_exec_requests or sys.dm_os_waiting_tasks to see the wait_type and\nwait_time.\nAzure SQL RESOURCE_SEMAPHORE wait \"wait_time\" is_user_process\n\n\nCommunity Discussion\nFYI, the answer is correct however the Group By is incorrect, it should be wait_type not\nTARGET1 unless in the select you add the alias.\nCorrect https://www.sqlshack.com/sql-server-performance-tuning-resource_semaphore-\nwaits/",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "236",
    "question": "You have an Azure subscription that contains 50 instances of SQL Server on\nAzure Virtual Machines. The instances host 500 Azure SQL databases.\nYou need to ensure that all the databases have the same configuration. The\nsolution must meet the following requirements:\n✑ Auditing must be enabled.\n✑ Azure Defender must be enabled.\n✑ Public network access must be disabled.\n✑ Administrative effort must be minimized.\nWhich two resources should you create in the subscription? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "an Azure Automation runbook",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "an Azure Policy initiative",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "an Azure Policy assignment",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "an Azure Automation account\nE) an Azure Policy definition",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "E"
  },
  {
    "number": "237",
    "question": "You have an instance of SQL Server on Azure Virtual Machine named SQL1.\nYou need to monitor SQL1 and query the metrics by using Kusto query language.\nThe solution must minimize administrative effort.\nWhere should you store the metrics?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Event Hubs",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a Log Analytics workspace",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an Azure Blob storage container",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "238",
    "question": "DRAG DROP -\nYou have a resource group named App1Dev that contains an Azure SQL Database\nserver named DevServer1. DevServer1 contains an Azure SQL Database instance\nnamed DB1. The schema and permissions for DB1 are saved in a Microsoft SQL\nServer Data Tools (SSDT) database project.\nYou need to populate a new resource group named App1Test with the DB1\ndatabase and an Azure SQL server named TestServer1. The resources in\nApp1Test must have the same configurations as the resources in App1 Dev.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\ncorrect\n1. From Azure export ARM templates; 2. change the server name and related variables in\nthe template; 3. Deploy ARM templates; 4. From database project deploy the database\nschema and permissions.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "239",
    "question": "DRAG DROP -\nYou have two instances of SQL Server on Azure Virtual Machines named VM1 and\nVM2. VM1 hosts a database named db1.\nYou plan to create a database availability group (DAG) for db1. The solution\nmust use certificate authentication between VM1 and VM2.\nYou need to configure authentication for the outbound connections of VM1.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is correct.\nmaster key, certificate, endpoint https://sqlmastersconsulting.com.au/SQL-Server-Blog/\nconfiguring-availability-groups-to-use-certificates/\nhttps://learn.microsoft.com/en-us/sql/database-engine/database-mirroring/the-\ndatabase-mirroring-endpoint-sql-server?view=sql-server-ver16\nhttps://learn.microsoft.com/en-us/sql/database-engine/database-mirroring/database-\nmirroring-use-certificates-for-outbound-connections?view=sql-server-ver16\nhttps://learn.microsoft.com/en-us/sql/database-engine/database-mirroring/database-\nmirroring-use-certificates-for-outbound-connections?view=sql-server-ver16",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "240",
    "question": "You have an Azure subscription that contains the resources shown in the\nfollowing table.\nYou need to configure a connection between VM1 and MI1. The solution must\nmeet the following requirements:\n✑ The connection must be encrypted.\n✑ Network latency must be minimized.\nWhat should you implement?",
    "options": [
      {
        "letter": "A",
        "text": "a site-to-site VPN",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "virtual network peering",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "private endpoints",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "service endpoints\nPage 436 of 818\n437 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "241",
    "question": "HOTSPOT -\nYou have a database on a SQL Server on Azure Virtual Machines instance.\nThe current state of Query Store for the database is shown in the following\nexhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nCommunity Discussion\nThe right answer is: none of the Max Size (MB) The first answer is \"none of the\" because\nthe Operation Mode (actual) is \"Read only\" so it is not retaining any query for evaluation\ndue to the Query Store Available is 0.0 MB. When the actual state is read-only, use the\nreadonly_reason column to determine the root cause. Typically, you find that Query Store\ntransitioned to read-only mode because the size quota was exceeded. In that case, the\nreadonly_reason is set to 65536 Consider the following steps to switch Query Store to\nread-write mode and activate data collection: * Increase the maximum storage size by\nusing the MAX_STORAGE_SIZE_MB option of ALTER DATABASE or * Clean up Query Store\ndata https://learn.microsoft.com/en-us/sql/relational-databases/performance/best-\npractice-with-the-query-store?view=sql-server-ver16#Verify\nThe right answer should be: all Max Size (MB) If SQL Server Query Store is in read-only\nmode, it retains all the queries for evaluation. The purpose of the read-only mode is to\n\n\nprevent any modification to the data in the Query Store. However, it still allows all the\nread operations, including the ability to retrieve and analyze the stored queries. The\nQuery Store is a feature introduced in SQL Server 2016, which captures a history of\nqueries and their execution statistics. It provides the ability to monitor and evaluate\nquery performance, identify query regressions, and optimize queries. When the Query\nStore is set to read-only mode, it does not accept any modifications, but it still retains all\nthe queries that were executed on the server.\nThe right answer should be: all Max Size (MB) If SQL Server Query Store is in read-only\nmode, it retains all the queries for evaluation. The purpose of the read-only mode is to\nprevent any modification to the data in the Query Store. However, it still allows all the\nread operations, including the ability to retrieve and analyze the stored queries. The\nQuery Store is a feature introduced in SQL Server 2016, which captures a history of\nqueries and their execution statistics. It provides the ability to monitor and evaluate\nquery performance, identify query regressions, and optimize queries. When the Query\nStore is set to read-only mode, it does not accept any modifications, but it still retains all\nthe queries that were executed on the server.\nIt depends on the interpretation of \"will retain\" * If \"will retain\" means to keep the\nexisting queries then the answer would be different * If \"will retain\" means to keep the\nnew (most recent) queries then the answer is \"none of the\". In my opinion, it refers to\ncapture new queries, and yours?\nIt depends on the interpretation of \"will retain\" * If \"will retain\" means to keep the\nexisting queries then the answer would be different * If \"will retain\" means to keep the\nnew (most recent) queries then the answer is \"none of the\". In my opinion, it refers to\ncapture new queries, and yours?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "242",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion What should you use to migrate the PostgreSQL database?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Data Box",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "AzCopy",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Database Migration Service",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure Site Recovery",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "Database Migration Service is a fully managed service designed to enable seamless\nmigrations from multiple database sources to Azure data platforms with minimal\ndowntime (online migrations).\nYou can use Azure Database Migration Service to migrate the databases from an on-\npremises PostgreSQL instance to Azure Database for PostgreSQL with minimal downtime\nto the application.\nNote: The on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nMove the PostgreSQL database to Azure Database for PostgreSQL during the next six\nmonths.\nDowntime during the PostgreSQL database migration must be minimized."
  },
  {
    "number": "243",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\n\n\nnext six months.\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion You need to implement a solution to notify the administrators. The\nsolution must meet the monitoring requirements.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create an Azure Monitor alert rule that has a static threshold and assign the alert rule to an\naction group.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add a diagnostic setting that logs QueryStoreRuntimeStatistics and streams to an Azure\nevent hub.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Add a diagnostic setting that logs Timeouts and streams to an Azure event hub.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create an Azure Monitor alert rule that has a dynamic threshold and assign the alert rule to\nan action group.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Notify administrators when a PaaS database has a higher than average CPU usage.\nWith Dynamic Thresholds, we use a unique ML technology to identify the patterns and\ncome up with a single alert rule that has the right thresholds and accounts for\nseasonality patterns such as hourly, daily, or weekly.\nReference:"
  },
  {
    "number": "244",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1, SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1.\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nRequirements -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nDRAG DROP -\nYou need to implement statistics maintenance for SalesSQLDb1. The solution\nmust meet the technical requirements.\nWhich four actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nAutomating Azure SQL DB index and statistics maintenance using Azure Automation:\n1. Create Azure automation account (Step 1)\n2. Import SQLServer module (Step 2)\n3. Add Credentials to access SQL DB\nThis will use secure way to hold login name and password that will be used to access\nAzure SQL DB\n4. Add a runbook to run the maintenance (Step 3)\nSteps:\n1. Click on \"runbooks\" at the left panel and then click \"add a runbook\"\n2. Choose \"create a new runbook\" and then give it a name and choose \"Powershell\" as\nthe type of the runbook and then click on \"create\"\n\n\n5. Schedule task (Step 4)\nSteps:\n1. Click on Schedules\n2. Click on \"Add a schedule\" and follow the instructions to choose existing schedule or\ncreate a new schedule.\nReference:\nhttps://techcommunity.microsoft.com/t5/azure-database-support-blog/automating-\nazure-sql-db-index-and-statistics-maintenance-using/ba-p/368974\nCommunity Discussion\nI think you're wrong, because before creating and setting up a schedule, we have to\npublish the runbook. 1. Create an Azure Automation account; 2. Import the SqlServer\nmodule 3. Create a runbook that runs a PowerShell script; 4. Publish the runbook.\nrunbook needs to be published before you can run it: When you create or import a new\nrunbook, you have to publish it before you can run it. Each runbook in Azure Automation\nhas a Draft version and a Published version. Only the Published version is available to be\nrun, and only the Draft version can be edited. The Published version is unaffected by any\nchanges to the Draft version. When the Draft version should be made available, you\npublish it, overwriting the current Published version with the Draft version. https://\nlearn.microsoft.com/en-us/azure/automation/manage-runbooks\nYou are wrong. Create runbook includes publish.\nYou are wrong. Create runbook includes publish.\n\n\nLooks correct.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "245",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question You\nneed to provide an implementation plan to configure data retention for\nResearchDB1. The solution must meet the security and compliance\nrequirements.\nWhat should you include in the plan?",
    "options": [
      {
        "letter": "A",
        "text": "Configure the Deleted databases settings for ResearchSrv01.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Deploy and configure an Azure Backup server.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure the Advanced Data Security settings for ResearchDB1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Configure the Manage Backups settings for ResearchSrv01.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Reference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/long-term-backup-\nretention-configure"
  },
  {
    "number": "246",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nHOTSPOT -\nYou need to recommend a configuration for ManufacturingSQLDb1 after the\nmigration to Azure. The solution must meet the business requirements.\nWhat should you include in the recommendation? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Node majority with witness\nAs a general rule when you configure a quorum, the voting elements in the cluster\nshould be an odd number. Therefore, if the cluster contains an even number of voting\nnodes, you should configure a disk witness or a file share witness.\nNote: Mode: Node majority with witness (disk or file share)\nNodes have votes. In addition, a quorum witness has a vote. The cluster quorum is the\nmajority of voting nodes in the active cluster membership plus a witness vote. A quorum\nwitness can be a designated disk witness or a designated file share witness.\nBox 2: Azure Standard Load Balancer\nMicrosoft guarantees that a Load Balanced Endpoint using Azure Standard Load Balancer,\nserving two or more Healthy Virtual Machine Instances, will be available 99.99% of the\ntime.\nScenario: Business Requirements -\nLitware identifies business requirements include: meet an SLA of 99.99% availability for\nall Azure deployments.\nIncorrect Aswers:\nBasic Balancer: No SLA is provided for Basic Load Balancer.\nNote: There are two main options for setting up your listener: external (public) or\ninternal. The external (public) listener uses an internet facing load balancer and is\nassociated with a public Virtual IP (VIP) that is accessible over the internet. An internal\nlistener uses an internal load balancer and only supports clients within the same Virtual\nNetwork.\nReference:\nhttps://technet.microsoft.com/windows-server-docs/failover-clustering/deploy-cloud-\nwitness https://azure.microsoft.com/en-us/support/legal/sla/load-balancer/v1_0/\n\n\nCommunity Discussion\nBox 2: Azure Standard Load Balancer Microsoft guarantees that a Load Balanced Endpoint\nusing Azure Standard Load Balancer, serving two or more Healthy Virtual Machine\nInstances, will be available 99.99% of the time. Scenario: Business Requirements Litware\nidentifies business requirements include: meet an SLA of 99.99% availability for all Azure\ndeployments. Incorrect Aswers: Basic Balancer: No SLA is provided for Basic Load\nBalancer. Note: There are two main options for setting up your listener: external (public)\nor internal. The external (public) listener uses an internet facing load balancer and is\nassociated with a public Virtual IP (VIP) that is accessible over the internet. An internal\nlistener uses an internal load balancer and only supports clients within the same Virtual\nNetwork. Reference: https://technet.microsoft.com/windows-server-docs/failover-\nclustering/deploy-cloud-witness https://azure.microsoft.com/en-us/support/legal/sla/\nload-balancer/v1_0/\nStandard Balancer is right option, they don't want public access for Azure machines,\nBasic load balancer is open to the internet by default\nAnswer is correct: Quorum mode: Node majority with witness Availability Group: Standard\nLB\nQuorum mode: Node majority with witness Availability Group: Standard LB\nNo SLA is provided for Basic Load Balancer https://azure.microsoft.com/en-us/support/\nlegal/sla/load-balancer/v1_0/",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "247",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nWhat should you do after a failover of SalesSQLDb1 to ensure that the database\nremains accessible to SalesSQLDb1App1?",
    "options": [
      {
        "letter": "A",
        "text": "Configure SalesSQLDb1 as writable.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Update the connection strings of SalesSQLDb1App1.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Update the firewall rules of SalesSQLDb1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Update the users in SalesSQLDb1.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: SalesSQLDb1 uses database firewall rules and contained database users.\nIncorrect:\nNot C: When using public network access for connecting to the database, we recommend\nusing database-level IP firewall rules for geo-replicated databases.\nThese rules are replicated with the database, which ensures that all geo-secondaries\nhave the same IP firewall rules as the primary. This approach eliminates the need for\ncustomers to manually configure and maintain firewall rules on servers hosting the\nprimary and secondary databases.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-\noverview#keeping-credentials-and-firewall-rules-in-sync"
  },
  {
    "number": "248",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion What should you implement to meet the disaster recovery\nrequirements for the PaaS solution?",
    "options": [
      {
        "letter": "A",
        "text": "Availability Zones",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "failover groups",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Always On availability groups",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "geo-replication",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: In the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic failover.\nThe auto-failover groups feature allows you to manage the replication and failover of a\ngroup of databases on a server or all databases in a managed instance to another\nregion. It is a declarative abstraction on top of the existing active geo-replication feature,\ndesigned to simplify deployment and management of geo- replicated databases at scale.\nYou can initiate failover manually or you can delegate it to the Azure service based on a\nuser-defined policy.\nThe latter option allows you to automatically recover multiple related databases in a"
  },
  {
    "number": "249",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question You\nneed to implement authentication for ResearchDB1. The solution must meet the\nsecurity and compliance requirements.\nWhat should you run as part of the implementation?",
    "options": [
      {
        "letter": "A",
        "text": "CREATE LOGIN and the FROM WINDOWS clause",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "CREATE USER and the FROM CERTIFICATE clause",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "CREATE USER and the FROM LOGIN clause",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "CREATE USER and the ASYMMETRIC KEY clause\nE) CREATE USER and the FROM EXTERNAL PROVIDER clause",
        "is_correct": true
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "250",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nHOTSPOT -\nYou are planning the migration of the SERVER1 databases. The solution must\nmeet the business requirements.\nWhat should you include in the migration plan? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Premium 4-VCore -\nScenario:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012 and two\n1-TB databases.\n\n\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nMinimize downtime during the migration of the SERVER1 databases.\nPremimum 4-vCore is for large or business critical workloads. It supports online\nmigrations, offline migrations, and faster migration speeds.\nIncorrect Answers:\nThe Standard pricing tier suits most small- to medium- business workloads, but it\nsupports offline migration only.\nBox 2: A virtual network that has service endpoints\nScenario: The primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering.\nYou need to create a Microsoft Azure Virtual Network for the Azure Database Migration\nService by using the Azure Resource Manager deployment model, which provides site-to-\nsite connectivity to your on-premises source servers by using either ExpressRoute or\nVPN.\nNote: During virtual network setup, if you use ExpressRoute with network peering to\nMicrosoft, add the following service endpoints to the subnet in which the service will be\nprovisioned:\nTarget database endpoint (for example, SQL endpoint, Cosmos DB endpoint, and so on)\nStorage endpoint -\nService bus endpoint -\nThis configuration is necessary because Azure Database Migration Service lacks internet\nconnectivity.\nReference:\nhttps://azure.microsoft.com/pricing/details/database-migration/ https://\ndocs.microsoft.com/en-us/azure/dms/tutorial-sql-server-azure-sql-online\nCommunity Discussion\nPremium 4-vCore A VPN gateway\nAnswer is correct but selection is wrong\n1- A main office in Boston. The main office hosts a sales department 2- The main office\nhosts a sales department and the primary datacenter for the company. 3- The sales\ndepartment has the following database workload: An on-premises named SERVER1 hosts\nan instance of Microsoft SQL Server 2012 and two 1-TB databases. 4- The primary\ndatacenter has an ExpressRoute connection that uses both Microsoft peering and private\npeering. The private peering connects to an Azure virtual network named HubVNet.\nhttps://docs.microsoft.com/en-us/azure/dms/tutorial-sql-server-to-azure-sql Link Says :\n\n\nDuring virtual network setup, if you use ExpressRoute with network peering to Microsoft,\nadd the following service endpoints to the subnet in which the service will be\nprovisioned: Target database endpoint (for example, SQL endpoint, Cosmos DB endpoint,\nand so on) I think correct answers are : Premium - 4vCore ( Premium supports online\nmigration, but Standard not support) A virtual network that has service endpoint\nExplanation in the answer and what is selected as the answer in the visuals are not the\nsame.\nThe sales department is in the main office/primary datacenter which has an\nExpressRoute to Azure. Access to the SERVER1 Azure SQL Database would be by\nExpressRoute. Should this not be Virtual Network with service endpoints?",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "251",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nHOTSPOT -\nYou need to recommend the appropriate purchasing model and deployment\noption for the 30 new databases. The solution must meet the technical\nrequirements and the business requirements.\nWhat should you recommend? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\n\n\nBox 1: Vcore -\nScenario:\n✑ The 30 new databases must scale automatically.\n✑ Once all requirements are met, minimize costs whenever possible.\nYou can configure resources for the pool based either on the DTU-based purchasing\nmodel or the vCore-based purchasing model.\nIn short, for simplicity, the DTU model has an advantage. Plus, if you're just getting\nstarted with Azure SQL Database, the DTU model offers more options at the lower end of\nperformance, so you can get started at a lower price point than with vCore.\nBox 2: An Azure SQL database elastic pool\nAzure SQL Database elastic pools are a simple, cost-effective solution for managing and\nscaling multiple databases that have varying and unpredictable usage demands. The\ndatabases in an elastic pool are on a single server and share a set number of resources\nat a set price. Elastic pools in Azure SQL Database enable\nSaaS developers to optimize the price performance for a group of databases within a\nprescribed budget while delivering performance elasticity for each database.\nReference:\nhttps://azure.microsoft.com/es-es/blog/a-flexible-new-way-to-purchase-azure-sql-\ndatabase/ https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-\noverview https://docs.microsoft.com/en-us/azure/azure-sql/database/reserved-\ncapacity-overview\nCommunity Discussion\nHybrid Use Benefits is not supported for DTU\n\n\nYou are right Should be vCore https://azure.microsoft.com/es-es/blog/a-flexible-new-\nway-to-purchase-azure-sql-database/\nYou are right Should be vCore https://azure.microsoft.com/es-es/blog/a-flexible-new-\nway-to-purchase-azure-sql-database/\nIt also says: \"Azure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments\". Doesn't matter if they are new or migrated.\nIt also says: \"Azure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments\". Doesn't matter if they are new or migrated.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "252",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nContoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000\nretail stores across the United States and an emerging online presence.\nThe network contains an Active Directory forest named contoso.com. The forest\nis integrated with an Azure Active Directory (Azure AD) tenant named\ncontoso.com. Contoso has an Azure subscription associated to the contoso.com\nAzure AD tenant.\nExisting Environment -\nTransactional Data -\nContoso has three years of customer, transaction, operational, sourcing, and\n\n\nsupplier data comprised of 10 billion records stored across multiple on-\npremises\nMicrosoft SQL Server servers. The SQL Server instances contain data from\nvarious operations systems. The data is loaded into the instances by using SQL\nServer Integration Services (SSIS) packages.\nYou estimate that combining all product sales transactions into a company-\nwide sales transactions dataset will result in a single table that contains 5\nbillion rows, with one row per transaction.\nMost queries targeting the sales transactions data will be used to identify which\nproducts were sold in retail stores and which products were sold online during\ndifferent time periods. Sales transaction data that is older than three years will\nbe removed monthly.\nYou plan to create a retail store table that will contain the address of each retail\nstore. The table will be approximately 2 MB. Queries for retail store sales will\ninclude the retail store addresses.\nYou plan to create a promotional table that will contain a promotion ID. The\npromotion ID will be associated to a specific product. The product will be\nidentified by a product ID. The table will be approximately 5 GB.\nStreaming Twitter Data -\nThe ecommerce department at Contoso develops an Azure logic app that\ncaptures trending Twitter feeds referencing the company's products and pushes\nthe products to Azure Event Hubs.\nPlanned Changes and Requirements\nPlanned Changes -\nContoso plans to implement the following changes:\nLoad the sales transaction dataset to Azure Synapse Analytics.\nIntegrate on-premises data stores with Azure Synapse Analytics by using SSIS\npackages.\nUse Azure Synapse Analytics to analyze Twitter feeds to assess customer\nsentiments about products.\nSales Transaction Dataset Requirements\nContoso identifies the following requirements for the sales transaction dataset:\nPartition data that contains sales transaction records. Partitions must be\ndesigned to provide efficient loads by month. Boundary values must belong to\nthe partition on the right.\n\n\nEnsure that queries joining and filtering sales transaction records based on\nproduct ID complete as quickly as possible.\nImplement a surrogate key to account for changes to the retail store addresses.\nEnsure that data storage costs and performance are predictable.\nMinimize how long it takes to remove old records.\nCustomer Sentiment Analytics Requirements\nContoso identifies the following requirements for customer sentiment analytics:\nAllow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated\nSQL pool to query the content of the data records that host the Twitter feeds.\nData must be protected by using row-level security (RLS). The users must be\nauthenticated by using their own Azure AD credentials.\nMaximize the throughput of ingesting Twitter feeds from Event Hubs to Azure\nStorage without purchasing additional throughput or capacity units.\nStore Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will\nbe converted into Parquet files.\nEnsure that the data store supports Azure AD-based access control down to the\nobject level.\nMinimize administrative effort to maintain the Twitter feed data records.\nPurge Twitter feed data records that are older than two years.\nData Integration Requirements -\nContoso identifies the following requirements for data integration:\nUse an Azure service that leverages the existing SSIS packages to ingest on-\npremises data into datasets stored in a dedicated SQL pool of Azure Synapse\nAnalytics and transform the data.\nIdentify a process to ensure that changes to the ingestion and transformation\nactivities can be version-controlled and developed independently by multiple\ndata engineers. Question You need to design a data retention solution for the\nTwitter feed data records. The solution must meet the customer sentiment\nanalytics requirements.\nWhich Azure Storage functionality should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "time-based retention",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "change feed",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "lifecycle management",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "soft delete\nPage 512 of 818\n513 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "The lifecycle management policy lets you:\n✑ Delete blobs, blob versions, and blob snapshots at the end of their lifecycles\nScenario:\n✑ Purge Twitter feed data records that are older than two years.\n✑ Store Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will be\nconverted into Parquet files.\n✑ Minimize administrative effort to maintain the Twitter feed data records.\nIncorrect Answers:\nA: Time-based retention policy support: Users can set policies to store data for a\nspecified interval. When a time-based retention policy is set, blobs can be created and\nread, but not modified or deleted. After the retention period has expired, blobs can be\ndeleted but not overwritten.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-\nconcepts"
  },
  {
    "number": "253",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nContoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000\nretail stores across the United States and an emerging online presence.\nThe network contains an Active Directory forest named contoso.com. The forest\nis integrated with an Azure Active Directory (Azure AD) tenant named\ncontoso.com. Contoso has an Azure subscription associated to the contoso.com\nAzure AD tenant.\nExisting Environment -\nTransactional Data -\nContoso has three years of customer, transaction, operational, sourcing, and\n\n\nsupplier data comprised of 10 billion records stored across multiple on-\npremises\nMicrosoft SQL Server servers. The SQL Server instances contain data from\nvarious operations systems. The data is loaded into the instances by using SQL\nServer Integration Services (SSIS) packages.\nYou estimate that combining all product sales transactions into a company-\nwide sales transactions dataset will result in a single table that contains 5\nbillion rows, with one row per transaction.\nMost queries targeting the sales transactions data will be used to identify which\nproducts were sold in retail stores and which products were sold online during\ndifferent time periods. Sales transaction data that is older than three years will\nbe removed monthly.\nYou plan to create a retail store table that will contain the address of each retail\nstore. The table will be approximately 2 MB. Queries for retail store sales will\ninclude the retail store addresses.\nYou plan to create a promotional table that will contain a promotion ID. The\npromotion ID will be associated to a specific product. The product will be\nidentified by a product ID. The table will be approximately 5 GB.\nStreaming Twitter Data -\nThe ecommerce department at Contoso develops an Azure logic app that\ncaptures trending Twitter feeds referencing the company's products and pushes\nthe products to Azure Event Hubs.\nPlanned Changes and Requirements\nPlanned Changes -\nContoso plans to implement the following changes:\nLoad the sales transaction dataset to Azure Synapse Analytics.\nIntegrate on-premises data stores with Azure Synapse Analytics by using SSIS\npackages.\nUse Azure Synapse Analytics to analyze Twitter feeds to assess customer\nsentiments about products.\nSales Transaction Dataset Requirements\nContoso identifies the following requirements for the sales transaction dataset:\nPartition data that contains sales transaction records. Partitions must be\ndesigned to provide efficient loads by month. Boundary values must belong to\nthe partition on the right.\n\n\nEnsure that queries joining and filtering sales transaction records based on\nproduct ID complete as quickly as possible.\nImplement a surrogate key to account for changes to the retail store addresses.\nEnsure that data storage costs and performance are predictable.\nMinimize how long it takes to remove old records.\nCustomer Sentiment Analytics Requirements\nContoso identifies the following requirements for customer sentiment analytics:\nAllow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated\nSQL pool to query the content of the data records that host the Twitter feeds.\nData must be protected by using row-level security (RLS). The users must be\nauthenticated by using their own Azure AD credentials.\nMaximize the throughput of ingesting Twitter feeds from Event Hubs to Azure\nStorage without purchasing additional throughput or capacity units.\nStore Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will\nbe converted into Parquet files.\nEnsure that the data store supports Azure AD-based access control down to the\nobject level.\nMinimize administrative effort to maintain the Twitter feed data records.\nPurge Twitter feed data records that are older than two years.\nData Integration Requirements -\nContoso identifies the following requirements for data integration:\nUse an Azure service that leverages the existing SSIS packages to ingest on-\npremises data into datasets stored in a dedicated SQL pool of Azure Synapse\nAnalytics and transform the data.\nIdentify a process to ensure that changes to the ingestion and transformation\nactivities can be version-controlled and developed independently by multiple\ndata engineers. Question You need to implement the surrogate key for the retail\nstore table. The solution must meet the sales transaction dataset requirements.\nWhat should you create?",
    "options": [
      {
        "letter": "A",
        "text": "a table that has a FOREIGN KEY constraint",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a table the has an IDENTITY property",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "a user-defined SEQUENCE object",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a system-versioned temporal table\nPage 517 of 818\n518 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: Contoso requirements for the sales transaction dataset include:\n✑ Implement a surrogate key to account for changes to the retail store addresses.\nA surrogate key on a table is a column with a unique identifier for each row. The key is\nnot generated from the table data. Data modelers like to create surrogate keys on their\ntables when they design data warehouse models. You can use the IDENTITY property to\nachieve this goal simply and effectively without affecting load performance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-\ndata-warehouse-tables-identity"
  },
  {
    "number": "254",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nContoso, Ltd. is a clothing retailer based in Seattle. The company has 2,000\nretail stores across the United States and an emerging online presence.\nThe network contains an Active Directory forest named contoso.com. The forest\nis integrated with an Azure Active Directory (Azure AD) tenant named\ncontoso.com. Contoso has an Azure subscription associated to the contoso.com\nAzure AD tenant.\nExisting Environment -\nTransactional Data -\nContoso has three years of customer, transaction, operational, sourcing, and\n\n\nsupplier data comprised of 10 billion records stored across multiple on-\npremises\nMicrosoft SQL Server servers. The SQL Server instances contain data from\nvarious operations systems. The data is loaded into the instances by using SQL\nServer Integration Services (SSIS) packages.\nYou estimate that combining all product sales transactions into a company-\nwide sales transactions dataset will result in a single table that contains 5\nbillion rows, with one row per transaction.\nMost queries targeting the sales transactions data will be used to identify which\nproducts were sold in retail stores and which products were sold online during\ndifferent time periods. Sales transaction data that is older than three years will\nbe removed monthly.\nYou plan to create a retail store table that will contain the address of each retail\nstore. The table will be approximately 2 MB. Queries for retail store sales will\ninclude the retail store addresses.\nYou plan to create a promotional table that will contain a promotion ID. The\npromotion ID will be associated to a specific product. The product will be\nidentified by a product ID. The table will be approximately 5 GB.\nStreaming Twitter Data -\nThe ecommerce department at Contoso develops an Azure logic app that\ncaptures trending Twitter feeds referencing the company's products and pushes\nthe products to Azure Event Hubs.\nPlanned Changes and Requirements\nPlanned Changes -\nContoso plans to implement the following changes:\nLoad the sales transaction dataset to Azure Synapse Analytics.\nIntegrate on-premises data stores with Azure Synapse Analytics by using SSIS\npackages.\nUse Azure Synapse Analytics to analyze Twitter feeds to assess customer\nsentiments about products.\nSales Transaction Dataset Requirements\nContoso identifies the following requirements for the sales transaction dataset:\nPartition data that contains sales transaction records. Partitions must be\ndesigned to provide efficient loads by month. Boundary values must belong to\nthe partition on the right.\n\n\nEnsure that queries joining and filtering sales transaction records based on\nproduct ID complete as quickly as possible.\nImplement a surrogate key to account for changes to the retail store addresses.\nEnsure that data storage costs and performance are predictable.\nMinimize how long it takes to remove old records.\nCustomer Sentiment Analytics Requirements\nContoso identifies the following requirements for customer sentiment analytics:\nAllow Contoso users to use PolyBase in an Azure Synapse Analytics dedicated\nSQL pool to query the content of the data records that host the Twitter feeds.\nData must be protected by using row-level security (RLS). The users must be\nauthenticated by using their own Azure AD credentials.\nMaximize the throughput of ingesting Twitter feeds from Event Hubs to Azure\nStorage without purchasing additional throughput or capacity units.\nStore Twitter feeds in Azure Storage by using Event Hubs Capture. The feeds will\nbe converted into Parquet files.\nEnsure that the data store supports Azure AD-based access control down to the\nobject level.\nMinimize administrative effort to maintain the Twitter feed data records.\nPurge Twitter feed data records that are older than two years.\nData Integration Requirements -\nContoso identifies the following requirements for data integration:\nUse an Azure service that leverages the existing SSIS packages to ingest on-\npremises data into datasets stored in a dedicated SQL pool of Azure Synapse\nAnalytics and transform the data.\nIdentify a process to ensure that changes to the ingestion and transformation\nactivities can be version-controlled and developed independently by multiple\ndata engineers. Question HOTSPOT -\nYou need to design an analytical storage solution for the transactional data. The\nsolution must meet the sales transaction dataset requirements.\nWhat should you include in the solution? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: Hash -\nScenario:\nEnsure that queries joining and filtering sales transaction records based on product ID\ncomplete as quickly as possible.\nA hash distributed table can deliver the highest query performance for joins and\naggregations on large tables.\nBox 2: Round-robin -\nScenario:\nYou plan to create a promotional table that will contain a promotion ID. The promotion\nID will be associated to a specific product. The product will be identified by a product ID.\nThe table will be approximately 5 GB.\n\n\nA round-robin table is the most straightforward table to create and delivers fast\nperformance when used as a staging table for loads. These are some scenarios where\nyou should choose Round robin distribution:\n✑ When you cannot identify a single key to distribute your data.\n✑ If your data doesn't frequently join with data from other tables.\n✑ When there are no obvious keys to join.\nIncorrect Answers:\nReplicated: Replicated tables eliminate the need to transfer data across compute nodes\nby replicating a full copy of the data of the specified table to each compute node. The\nbest candidates for replicated tables are tables with sizes less than 2 GB compressed and\nsmall dimension tables.\nReference:\nhttps://rajanieshkaushikk.com/2020/09/09/how-to-choose-right-data-distribution-\nstrategy-for-azure-synapse/\nCommunity Discussion\nReplicated Hash",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "4",
    "question": "https://www.secexams.com/exams/microsoft/dp-203/view/\n26/\nis part of DP-203? really is't dp-300!\nis part of DP-203? really is't dp-300!\nRetail Store is only 2 MB so it should be replicated. Promotional table is greater than 2\nGB so it should be Hash\nDP-203",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "255",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nDRAG DROP -\nYou create all of the tables and views for ResearchDB1.\nYou need to implement security for ResearchDB1. The solution must meet the\nsecurity and compliance requirements.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nBox 1: - Register ResearchApp1 to Azure AD.\nBox 2: Create an Azure Key Vault instance and configure an access policy.\nNeed to configure the key vault with an access policy to enable ResearchApp1 retrieval of\nthe keys.\nBox 3: Run the Always Encrypt wizard.\nRun the Always Encrypt wizard to encrypt the columns and store the encryption keys in\nthe vault.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/always-encrypted-azure-\nkey-vault-configure?tabs=azure-powershell\nCommunity Discussion\nAnswer is correct\nAnswer looks good. Link to alwaysEncryption is clear. - Register the app in AD - Configure\nthe key vault with an access policy so the app can retrieve the keys - Run alwaysOn\nwizard to encrypt the columns and store the encryption keys in the vault\nCreate an Azure Key Vault instance and configure an access policy: This step ensures that\nyou have a secure place to store your encryption keys and that the appropriate access\npolicies are in place. It’s essential to have the Key Vault ready before you start\nconfiguring encryption for your database. Register ResearchApp1 to Azure AD: This step\nallows you to authenticate users using Azure Active Directory credentials. Registering the\napp to Azure AD ensures that you can manage access and permissions effectively. Run\nthe Always Encrypted wizard: This step encrypts the PII data in ResearchDB1 using the\nkeys stored in Azure Key Vault. Having the Key Vault and access policies configured\nbeforehand ensures a smooth encryption process. While reversing steps 1 and 2 might\n\n\nnot cause significant issues, following the recommended order helps maintain a logical\nand efficient workflow.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "256",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nDRAG DROP -\nYou need to configure user authentication for the SERVER1 databases. The\nsolution must meet the security and compliance requirements.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nSelect and Place:\nExplanation\nCorrect Answer:\n\n\nScenario: Authenticate database users by using Active Directory credentials.\nThe configuration steps include the following procedures to configure and use Azure\nActive Directory authentication.\n1. Create and populate Azure AD.\n2. Optional: Associate or change the active directory that is currently associated with\nyour Azure Subscription.\n3. Create an Azure Active Directory administrator. (Step 1)\n4. Connect to the databases using an Azure AD account (the Administrator account that\nwas configured in the previous step). (Step 2)\n5. Create contained database users in your database mapped to Azure AD identities.\n(Step 3)\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-\nconfigure?tabs=azure-powershell\nCommunity Discussion\nIt should be D,E,C - You have to connect to the databases ,with the AD account and then\nto create contained users . \" To create an Azure AD-based contained database user (other\nthan the server administrator that owns the database), connect to the database with an\nAzure AD identity, as a user with at least the ALTER ANY USER permission. \"\nAfter I thought about it, I think you are right: 1- Create an Azure AD Admin. 2- Create\ncontained DB users in your db mapped to Azure AD identities. 3- Connect to your DB by\nusing the Azure AD identities (which we mapped in step 2 to the contained db users).\nwith that said the provided answer has the wrong order for step 2 and 3\n\n\nAfter I thought about it, I think you are right: 1- Create an Azure AD Admin. 2- Create\ncontained DB users in your db mapped to Azure AD identities. 3- Connect to your DB by\nusing the Azure AD identities (which we mapped in step 2 to the contained db users).\nwith that said the provided answer has the wrong order for step 2 and 3\nAfter I thought about it, I think you are right: 1- Create an Azure AD Admin. 2- Create\ncontained DB users in your db mapped to Azure AD identities. 3- Connect to your DB by\nusing the Azure AD identities (which we mapped in step 2 to the contained db users).\nwith that said the provided answer has the wrong order for step 2 and 3\nAfter I thought about it, I think you are right: 1- Create an Azure AD Admin. 2- Create\ncontained DB users in your db mapped to Azure AD identities. 3- Connect to your DB by\nusing the Azure AD identities (which we mapped in step 2 to the contained db users).\nwith that said the provided answer has the wrong order for step 2 and 3",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "257",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion HOTSPOT -\nYou are evaluating the role assignments.\nFor each of the following statements, select Yes if the statement is true.\nOtherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:\n\n\nExplanation\nCorrect Answer:\nBox 1: No -\nDBAGroup1 is member of the Contributor role.\nThe Contributor role grants full access to manage all resources, but does not allow you\nto assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image\ngalleries.\nBox 2: No -\nContributor - Grants full access to manage all resources, but does not allow you to assign\nroles in Azure RBAC\nSQL DB Contributor - Lets you manage SQL databases, but not access to them. Also, you\ncan't manage their security-related policies or their parent SQL servers.\nBox 3: Yes -\nDBAGroup2 is member of the SQL DB Contributor role.\nThe SQL DB Contributor role lets you manage SQL databases, but not access to them.\nAlso, you can't manage their security-related policies or their parent SQL servers. As a\nmember of this role you can create and manage SQL databases.\nReference:\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles\nCommunity Discussion\nProposed answer is wrong. The correct answer is: NO - with contributor you cannot\naccess database, you can only manage them (except for security) NO (contributor cannot\nmanage security) YES (since you have this permission on each logical server, you can\ncreate database)\n\n\nMicrosoft states that the following rules apply: Contributor - Grants full access to manage\nall resources, but does not allow you to assign roles in Azure RBAC SQL DB Contributor -\nLets you manage SQL databases, but not access to them. Also, you can't manage their\nsecurity-related policies or their parent SQL servers. Answer looks good from this\nperspective: Yes-No-Yes\nThere is so much uncertainty in these questions, it goes to show how much of a shit\nshow azure is.\nNo, No, Yes Contributor - Grants full access to manage all resources, but does not allow\nyou to assign roles in Azure RBAC. All these permissions are to make changes to the\ndatabase setting at Azure Portal level. For the user to connect to database using SSMS/\nData Studio to perform any tasks at database level, user/group need to be given\npermissions by Admin first.\nNO, permission is only on THIS logical server. The group has no right to create databases\non another server (unless there is more privileges that are not shown here) so NO, NO,\nNO",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "258",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion You need to recommend a solution to ensure that the customers can\ncreate the database objects. The solution must meet the business goals.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "For each customer, grant the customer ddl_admin to the existing schema.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "For each customer, create an additional schema and grant the customer ddl_admin to the\nnew schema.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "For each customer, create an additional schema and grant the customer db_writer to the new\nschema.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "For each customer, grant the customer db_writer to the existing schema.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "Scenario: Ensure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied by\nContoso.\nMembers of the db_ddladmin fixed database role can run any Data Definition Language\n(DDL) command in a database.\nIncorrect:\nNot D: db_writer does not have permissions to create database objects."
  },
  {
    "number": "259",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion You are evaluating the business goals.\nWhich feature should you use to provide customers with the required level of\naccess based on their service agreement?",
    "options": [
      {
        "letter": "A",
        "text": "dynamic data masking",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Conditional Access in Azure",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "service principals",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "row-level security (RLS)",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Scenario: Provide the customers with different table and row access based on the\ncustomer's service agreement.\nRow-level security (RLS) is a feature added as of SQL Server 2016. Instead of encrypting\nor decrypting a database's table data, it restricts and filters a table's row- level data in\naccordance with security policies defined by the user. This enables the database engine\nto limit the number of exposed data rows.\nReference:\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?\nview=sql-server-ver15"
  },
  {
    "number": "260",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question\nHOTSPOT -\nYou need to implement the monitoring of SalesSQLDb1. The solution must meet\nthe technical requirements.\nHow should you collect and stream metrics? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:\nExplanation\nCorrect Answer:\nBox 1: The server, the elastic pool, and the database\nSenario:\n\n\nSalesSQLDb1 is in an elastic pool named SalesSQLDb1Pool.\nLitware technical requirements include: all SQL Server and Azure SQL Database metrics\nrelated to CPU and storage usage and limits must be analyzed by using\nAzure built-in functionality.\nBox 2: Azure Event hubs -\nScenario: Migrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nEvent hubs are able to handle custom metrics.\nIncorrect Answers:\nAzure Log Analytics -\nAzure metric and log data are sent to Azure Monitor Logs, previously known as Azure Log\nAnalytics, directly by Azure. Azure SQL Analytics is a cloud only monitoring solution\nsupporting streaming of diagnostics telemetry for all of your Azure SQL databases.\nHowever, because Azure SQL Analytics does not use agents to connect to Azure Monitor,\nit does not support monitoring of SQL Server hosted on-premises or in virtual machines.\nCommunity Discussion\nFurther to my previous comment the below is in the scenario: All SQL Server and Azure\nSQL Database metrics related to CPU and storage usage and limits must be analyzed by\nusing Azure built-in functionality.\nIMO, collecting 'basic' metric from the Elastic pool and the database to a Log Analytics\nWorkspace is enough to meet the tech requirements.\nA: Elastic pool and databases B: Log Analytics Streaming to log analytics is possible:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/metrics-diagnostic-\ntelemetry-logging-streaming-export-configure?view=azuresql&tabs=azure-portal You can\nstream metrics and resource logs to a Log Analytics workspace in Azure Monitor. Data\nstreamed here can be consumed by SQL Analytics (preview), which is a cloud only\nmonitoring solution that provides intelligent monitoring of your databases that includes\nperformance reports, alerts, and mitigation recommendations. Data streamed to a Log\nAnalytics workspace can be analyzed with other monitoring data collected and also\nenables you to leverage other Azure Monitor features such as alerts and visualizations.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/monitor-tune-overview?\nview=azuresql Collecting server metrics is not an option: https://learn.microsoft.com/en-\nus/azure/azure-sql/database/elastic-pool-resource-management?view=azuresql\n\n\nBecause you need to collect and stream data, it has to go the destination of Azure Event\nHub.\nLog Analytics does not support monitoring of SQL Server hosted on-premises or in\nvirtual machines.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "261",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion Based on the PaaS prototype, which Azure SQL Database compute tier\nshould you use?",
    "options": [
      {
        "letter": "A",
        "text": "Business Critical 4-vCore",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Hyperscale",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "General Purpose v-vCore",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Serverless",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Scenario: Exhibit show idle periods with intermittent spikes.\nServerless is a compute tier for single databases in Azure SQL Database that\nautomatically scales compute based on workload demand and bills for the amount of\ncompute used per second. The serverless compute tier also automatically pauses\ndatabases during inactive periods when only storage is billed and automatically resumes\ndatabases when activity returns.\nIncorrect Answers:\nB: Hyperscale is for large databases"
  },
  {
    "number": "262",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nGeneral Overview -\nContoso, Ltd. is a financial data company that has 100 employees. The company\ndelivers financial data to customers.\nPhysical Locations -\nContoso has a datacenter in Los Angeles and an Azure subscription. All Azure\nresources are in the US West 2 Azure region. Contoso has a 10-Gb ExpressRoute\nconnection to Azure.\nThe company has customers worldwide.\n\n\nExisting Environment -\nActive Directory -\nContoso has a hybrid Azure Active Directory (Azure AD) deployment that syncs\nto on-premises Active Directory.\nDatabase Environment -\nContoso has SQL Server 2017 on Azure virtual machines shown in the following\ntable.\nSQL1 and SQL2 are in an Always On availability group and are actively queried.\nSQL3 runs jobs, provides historical data, and handles the delivery of data to\ncustomers.\nThe on-premises datacenter contains a PostgreSQL server that has a 50-TB\ndatabase.\nCurrent Business Model -\nContoso uses Microsoft SQL Server Integration Services (SSIS) to create flat files\nfor customers. The customers receive the files by using FTP.\nRequirements -\nPlanned Changes -\nContoso plans to move to a model in which they deliver data to customer\ndatabases that run as platform as a service (PaaS) offerings. When a customer\nestablishes a service agreement with Contoso, a separate resource group that\ncontains an Azure SQL database will be provisioned for the customer. The\ndatabase will have a complete copy of the financial data. The data to which\neach customer will have access will depend on the service agreement tier. The\ncustomers can change tiers by changing their service agreement.\nThe estimated size of each PaaS database is 1 TB.\nContoso plans to implement the following changes:\nMove the PostgreSQL database to Azure Database for PostgreSQL during the\nnext six months.\n\n\nUpgrade SQL1, SQL2, and SQL3 to SQL Server 2019 during the next few months.\nStart onboarding customers to the new PaaS solution within six months.\nBusiness Goals -\nContoso identifies the following business requirements:\nUse built-in Azure features whenever possible.\nMinimize development effort whenever possible.\nMinimize the compute costs of the PaaS solutions.\nProvide all the customers with their own copy of the database by using the PaaS\nsolution.\nProvide the customers with different table and row access based on the\ncustomer's service agreement.\nIn the event of an Azure regional outage, ensure that the customers can access\nthe PaaS solution with minimal downtime. The solution must provide automatic\nfailover.\nEnsure that users of the PaaS solution can create their own database objects\nbut be prevented from modifying any of the existing database objects supplied\nby\nContoso.\nTechnical Requirements -\nContoso identifies the following technical requirements:\nUsers of the PaaS solution must be able to sign in by using their own corporate\nAzure AD credentials or have Azure AD credentials supplied to them by\nContoso. The solution must avoid using the internal Azure AD of Contoso to\nminimize guest users.\nAll customers must have their own resource group, Azure SQL server, and Azure\nSQL database. The deployment of resources for each customer must be done in\na consistent fashion.\nUsers must be able to review the queries issued against the PaaS databases and\nidentify any new objects created.\nDowntime during the PostgreSQL database migration must be minimized.\nMonitoring Requirements -\nContoso identifies the following monitoring requirements:\nNotify administrators when a PaaS database has a higher than average CPU\n\n\nusage.\nUse a single dashboard to review security and audit data for all the PaaS\ndatabases.\nUse a single dashboard to monitor query performance and bottlenecks across\nall the PaaS databases.\nMonitor the PaaS databases to identify poorly performing queries and resolve\nquery performance issues automatically whenever possible.\nPaaS Prototype -\nDuring prototyping of the PaaS solution in Azure, you record the compute\nutilization of a customer's Azure SQL database as shown in the following\nexhibit.\nRole Assignments -\nFor each customer's Azure SQL Database server, you plan to assign the roles\nshown in the following exhibit.\n\n\nQuestion Which audit log destination should you use to meet the monitoring\nrequirements?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Storage",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Event Hubs",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Log Analytics",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "Scenario: Use a single dashboard to review security and audit data for all the PaaS\ndatabases.\nWith dashboards can bring together operational data that is most important to IT across\nall your Azure resources, including telemetry from Azure Log Analytics.\nNote: Auditing for Azure SQL Database and Azure Synapse Analytics tracks database\nevents and writes them to an audit log in your Azure storage account, Log\nAnalytics workspace, or Event Hubs.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/visualize/tutorial-logs-\ndashboards"
  },
  {
    "number": "263",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a retailer that sells products through two sales channels:\nretail stores and a website.\nExisting Environment -\nADatum has one database server that has Microsoft SQL Server 2016 installed.\nThe server hosts three mission-critical databases named SALESDB, DOCDB, and\nREPORTINGDB.\nSALESDB collects data from the stores and the website.\nDOCDB stores documents that connect to the sales data in SALESDB. The\ndocuments are stored in two different JSON formats based on the sales channel.\nREPORTINGDB stores reporting data and contains several columnstore indexes.\n\n\nA daily process creates reporting data in REPORTINGDB from the data in\nSALESDB. The process is implemented as a SQL Server Integration Services\n(SSIS) package that runs a stored procedure from SALESDB.\nRequirements -\nPlanned Changes -\nADatum plans to move the current data infrastructure to Azure. The new\ninfrastructure has the following requirements:\nMigrate SALESDB and REPORTINGDB to an Azure SQL database.\nMigrate DOCDB to Azure Cosmos DB.\nThe sales data, including the documents in JSON format, must be gathered as it\narrives and analyzed online by using Azure Stream Analytics. The analytics\nprocess will perform aggregations that must be done continuously, without\ngaps, and without overlapping.\nAs they arrive, all the sales documents in JSON format must be transformed into\none consistent format.\nAzure Data Factory will replace the SSIS process of copying the data from\nSALESDB to REPORTINGDB.\nTechnical Requirements -\nThe new Azure data infrastructure must meet the following technical\nrequirements:\nData in SALESDB must encrypted by using Transparent Data Encryption (TDE).\nThe encryption must use your own key.\nSALESDB must be restorable to any given minute within the past three weeks.\nReal-time processing must be monitored to ensure that workloads are sized\nproperly based on actual usage patterns.\nMissing indexes must be created automatically for REPORTINGDB.\nDisk IO, CPU, and memory usage must be monitored for SALESDB. Question\nWhich windowing function should you use to perform the streaming\naggregation of the sales data?",
    "options": [
      {
        "letter": "A",
        "text": "Sliding",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Hopping",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Session",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Tumbling \nPage 584 of 818\n585 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "Scenario: The sales data, including the documents in JSON format, must be gathered as it\narrives and analyzed online by using Azure Stream Analytics. The analytics process will\nperform aggregations that must be done continuously, without gaps, and without\noverlapping.\nTumbling window functions are used to segment a data stream into distinct time\nsegments and perform a function against them, such as the example below. The key\ndifferentiators of a Tumbling window are that they repeat, do not overlap, and an event\ncannot belong to more than one tumbling window.\nReference:\nhttps://github.com/MicrosoftDocs/azure-docs/blob/master/articles/stream-analytics/\nstream-analytics-window-functions.md"
  },
  {
    "number": "264",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a retailer that sells products through two sales channels:\nretail stores and a website.\nExisting Environment -\nADatum has one database server that has Microsoft SQL Server 2016 installed.\nThe server hosts three mission-critical databases named SALESDB, DOCDB, and\nREPORTINGDB.\nSALESDB collects data from the stores and the website.\nDOCDB stores documents that connect to the sales data in SALESDB. The\ndocuments are stored in two different JSON formats based on the sales channel.\nREPORTINGDB stores reporting data and contains several columnstore indexes.\n\n\nA daily process creates reporting data in REPORTINGDB from the data in\nSALESDB. The process is implemented as a SQL Server Integration Services\n(SSIS) package that runs a stored procedure from SALESDB.\nRequirements -\nPlanned Changes -\nADatum plans to move the current data infrastructure to Azure. The new\ninfrastructure has the following requirements:\nMigrate SALESDB and REPORTINGDB to an Azure SQL database.\nMigrate DOCDB to Azure Cosmos DB.\nThe sales data, including the documents in JSON format, must be gathered as it\narrives and analyzed online by using Azure Stream Analytics. The analytics\nprocess will perform aggregations that must be done continuously, without\ngaps, and without overlapping.\nAs they arrive, all the sales documents in JSON format must be transformed into\none consistent format.\nAzure Data Factory will replace the SSIS process of copying the data from\nSALESDB to REPORTINGDB.\nTechnical Requirements -\nThe new Azure data infrastructure must meet the following technical\nrequirements:\nData in SALESDB must encrypted by using Transparent Data Encryption (TDE).\nThe encryption must use your own key.\nSALESDB must be restorable to any given minute within the past three weeks.\nReal-time processing must be monitored to ensure that workloads are sized\nproperly based on actual usage patterns.\nMissing indexes must be created automatically for REPORTINGDB.\nDisk IO, CPU, and memory usage must be monitored for SALESDB. Question\nWhich counter should you monitor for real-time processing to meet the\ntechnical requirements?",
    "options": [
      {
        "letter": "A",
        "text": "SU% Utilization",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "CPU% utilization",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Concurrent users",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Data Conversion Errors\nPage 589 of 818\n590 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "Scenario:\n* The sales data, including the documents in JSON format, must be gathered as it arrives\nand analyzed online by using Azure Stream Analytics.\n* Real-time processing must be monitored to ensure that workloads are sized properly\nbased on actual usage patterns.\nStreaming Units (SUs) represents the computing resources that are allocated to execute\na Stream Analytics job. The higher the number of SUs, the more CPU and memory\nresources are allocated for your job. This capacity lets you focus on the query logic and\nabstracts the need to manage the hardware to run your\nStream Analytics job in a timely manner.\nReference:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-\nunit-consumption"
  },
  {
    "number": "265",
    "question": "Introductory Info Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nLitware, Inc. is a renewable energy company that has a main office in Boston.\nThe main office hosts a sales department and the primary datacenter for the\ncompany.\nPhysical Locations -\nLitware has a manufacturing office and a research office is separate locations\nnear Boston. Each office has its own datacenter and internet connection.\nExisting Environment -\nNetwork Environment -\n\n\nThe manufacturing and research datacenters connect to the primary datacenter\nby using a VPN.\nThe primary datacenter has an ExpressRoute connection that uses both\nMicrosoft peering and private peering. The private peering connects to an Azure\nvirtual network named HubVNet.\nIdentity Environment -\nLitware has a hybrid Azure Active Directory (Azure AD) deployment that uses a\ndomain named litwareinc.com. All Azure subscriptions are associated to the\nlitwareinc.com Azure AD tenant.\nDatabase Environment -\nThe sales department has the following database workload:\nAn on-premises named SERVER1 hosts an instance of Microsoft SQL Server 2012\nand two 1-TB databases.\nA logical server named SalesSrv01A contains a geo-replicated Azure SQL\ndatabase named SalesSQLDb1. SalesSQLDb1 is in an elastic pool named\nSalesSQLDb1Pool. SalesSQLDb1 uses database firewall rules and contained\ndatabase users.\nAn application named SalesSQLDb1App1 uses SalesSQLDb1.\nThe manufacturing office contains two on-premises SQL Server 2016 servers\nnamed SERVER2 and SERVER3. The servers are nodes in the same Always On\navailability group. The availability group contains a database named\nManufacturingSQLDb1\nDatabase administrators have two Azure virtual machines in HubVnet named\nVM1 and VM2 that run Windows Server 2019 and are used to manage all the\nAzure databases.\nLicensing Agreement -\nLitware is a Microsoft Volume Licensing customer that has License Mobility\nthrough Software Assurance.\nCurrent Problems -\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date\nstatistics and frequent blocking queries.\nRequirements -\n\n\nPlanned Changes -\nLitware plans to implement the following changes:\nImplement 30 new databases in Azure, which will be used by time-sensitive\nmanufacturing apps that have varying usage patterns. Each database will be\napproximately 20 GB.\nCreate a new Azure SQL database named ResearchDB1 on a logical server\nnamed ResearchSrv01. ResearchDB1 will contain Personally Identifiable\nInformation (PII) data.\nDevelop an app named ResearchApp1 that will be used by the research\ndepartment to populate and access ResearchDB1.\nMigrate ManufacturingSQLDb1 to the Azure virtual machine platform.\nMigrate the SERVER1 databases to the Azure SQL Database platform.\nTechnical Requirements -\nLitware identifies the following technical requirements:\nMaintenance tasks must be automated.\nThe 30 new databases must scale automatically.\nThe use of an on-premises infrastructure must be minimized.\nAzure Hybrid Use Benefits must be leveraged for Azure SQL Database\ndeployments.\nAll SQL Server and Azure SQL Database metrics related to CPU and storage\nusage and limits must be analyzed by using Azure built-in functionality.\nSecurity and Compliance Requirements\nLitware identifies the following security and compliance requirements:\nStore encryption keys in Azure Key Vault.\nRetain backups of the PII data for two months.\nEncrypt the PII data at rest, in transit, and in use.\nUse the principle of least privilege whenever possible.\nAuthenticate database users by using Active Directory credentials.\nProtect Azure SQL Database instances by using database-level firewall rules.\nEnsure that all databases hosted in Azure are accessible from VM1 and VM2\nwithout relying on public endpoints.\nBusiness Requirements -\nLitware identifies the following business requirements:\nMeet an SLA of 99.99% availability for all Azure deployments.\n\n\nMinimize downtime during the migration of the SERVER1 databases.\nUse the Azure Hybrid Use Benefits when migrating workloads to Azure.\nOnce all requirements are met, minimize costs whenever possible. Question You\nneed to identify the cause of the performance issues on SalesSQLDb1.\nWhich two dynamic management views should you use? Each correct answer\npresents part of the solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "sys.dm_pdw_nodes_tran_locks",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "sys.dm_exec_compute_node_errors",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "sys.dm_exec_requests",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "sys.dm_cdc_errors\nE) sys.dm_pdw_nodes_os_wait_stats\nF) sys.dm_tran_locks",
        "is_correct": true
      }
    ],
    "correct_answer": "C",
    "explanation": "F\nSalesSQLDb1 experiences performance issues that are likely due to out-of-date statistics\nand frequent blocking queries.\nSeeing a Count of All Active SQL Server Wait Types.\nSometimes we're trying to diagnose a problem and we want to know if we're seeing a\nlarge number of wait types occurring. We can do this using sys.dm_exec_requests\nbecause the current wait type being experienced is presented. Therefore, if we filter out\nany background or sleeping tasks, we can get a picture of what the waits are for active\nrequests and we can also see if we have a problem. Here's the query:\nSELECT COALESCE(wait_type, 'None') AS wait_type, COUNT(*) AS Total\nFROM sys.dm_exec_requests -\nWHERE NOT status IN ('Background', 'Sleeping')\nGROUP BY wait_type -\nORDER BY Total DESC;\nHere is an example of the query output:"
  },
  {
    "number": "266",
    "question": "You have an Azure subscription that contains an Azure SQL database. The\ndatabase contains a table named table1.\nYou execute the following Transact-SQL statements.\nYou need to reduce the time it takes to perform analytic queries on the\ndatabase.\nWhich configuration should you enable?",
    "options": [
      {
        "letter": "A",
        "text": "ROW_MODE_MEMORY_GRANT_FEEDBACK",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "BATCH_MODE_MEMORY_GRANT_FEEDBACK",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "BATCH_MODE_ADAPTIVE_JOINS",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "BATCH_MODE_ON_ROWSTORE",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "267",
    "question": "DRAG DROP\n-\nYou create a new Azure SQL managed instance named SQL1 and enable\nDatabase Mail extended stored procedures.\nYou need to ensure that SQL Server Agent jobs running on SQL1 can notify\nadministrators when a failure occurs.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nGiven answer is correct, 1) Create a Database Mail account 2) Create a profile 3) Enable\nemail notification The database mail configuration wizard would start the profile creation\nfirst, however, need to complete the account creation first before the profile creation can\nbe completed. So the order is correct.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/job-automation-\nmanaged-instance?view=azuresql#job-notifications\nhttps://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/job-automation-\nmanaged-instance?view=azuresql#job-notifications",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "268",
    "question": "You deploy an instance of SQL Server on Azure Virtual Machines named VM1.\nYou need to create a SQL Server Agent job that will rebuild indexes of the\ndatabases hosted on VM1. The solution must use the principle of least privilege.\nWhat should you create first?",
    "options": [
      {
        "letter": "A",
        "text": "a local Windows account",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a user-assigned managed identity in Azure AD",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a system-assigned managed identity in Azure AD",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "an Elastic Job agent",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "269",
    "question": "HOTSPOT\n-\nYou need to deploy an Azure SQL Database elastic pool by using a Bicep\ntemplate.\nHow should you complete the template? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nServer minCapacity https://learn.microsoft.com/en-us/azure/templates/microsoft.sql/\nservers/databases?pivots=deployment-language-bicep\nhttps://learn.microsoft.com/en-us/azure/templates/microsoft.sql/servers/elasticpools?\npivots=deployment-language-bicep minCapacity Minimal capacity that serverless pool\nwill not shrink below, if not paused\ncorrect\nDeclare the resource type for the elastic pool in the template, which should be\nMicrosoft.Sql/servers/elasticPools.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "270",
    "question": "You have an Azure AD tenant and a logical Microsoft SQL server named SQL1\nthat hosts several Azure SQL databases.\nYou plan to assign Azure AD users permissions to the databases automatically\nby using Azure Automation.\nYou need to create the required Automation accounts.\nWhich two accounts should you create? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "From the Azure Active Directory admin center create a service principal.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "From the Azure Active Directory admin center, create a user-assigned managed identity for\nSQL1.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "On SQL1, create a SQL user in the databases.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "On SQL1, create a SQL login.\nE) From the Azure Active Directory admin center, create an external identity.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "C"
  },
  {
    "number": "271",
    "question": "You have an Azure subscription.\nYou plan to deploy an instance of SQL Server on Azure Virtual Machines by using\nan Azure Marketplace image.\nYou need to register the SQL Server IaaS Agent extension (SqlIaasExtension).\nThe solution must meet the following requirements:\n• Install critical updates for SQL Server automatically.\n• Minimize performance impact on the virtual machine.\nWhich management mode should you select?",
    "options": [
      {
        "letter": "A",
        "text": "full",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "lightweight",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "NoAgent",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "272",
    "question": "HOTSPOT\n-\nYou configure backups for an Azure SQL database as shown in the following\nexhibit.\nUse the drop-down menus to select the answer choice that completes each\nstatement based on the information presented in the graphic.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n52 backups: Azure uses the same weekly backup for the monthly backup and year\nbackup.\n52-weeks + 12-month + 1 (The 52nd week) = 65\nAgree, only 1 copy is in LTR storage: https://learn.microsoft.com/en-us/azure/azure-sql/\ndatabase/long-term-retention-overview?view=azuresql\nAgree, only 1 copy is in LTR storage: https://learn.microsoft.com/en-us/azure/azure-sql/\ndatabase/long-term-retention-overview?view=azuresql\n\n\nThe answer is correct. 52 Weekly backup + 12 Monthly backup + 1 Yearly backup",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "273",
    "question": "You have an Azure subscription that contains an Azure SQL database named\nSQL1.\nSQL1 is in an Azure region that does not support availability zones.\nYou need to ensure that you have a secondary replica of SQL1 in the same\nregion.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "log shipping",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "active geo-replication",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Microsoft SQL Server failover clusters",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "auto-failover groups",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "274",
    "question": "You have an Azure SOI database named SQLDb1 that contains the resources\nshown in the following table.\nColumn1 contains JSON data.\nYou need to compress Column1. The solution must minimize the amount of\nstorage used.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "the COMPRESS() function",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "columnstore archive compression",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "row compression",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "columnstore compression",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "275",
    "question": "You have an Azure subscription that contains two instances of SQL Server on\nAzure Virtual Machines named VM1 and VM2. Both instances run Microsoft SQL\nServer 2019 CU8.\nYou need to deploy a failover cluster instance (FCI) to VM1 and VM2 that will use\nAzure shared disks. The solution must maximize resiliency.\nWhich quorum option should you use?",
    "options": [
      {
        "letter": "A",
        "text": "node majority with a cloud witness",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "node majority with no witness",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "node majority with a file share witness",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "node majority with a disk witness",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "276",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database Premium",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure SQL Database serverless",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database Basic",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Database Hyperscale",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "277",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database Business Critical",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure SQL Database Standard",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database managed instance General Purpose",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Database Hyperscale",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "278",
    "question": "You plan to deploy two instances of SQL Server on Azure virtual machines in a\nhighly available configuration that will use an Always On availability group.\nYou need to recommend a deployment solution that meets the following\nrequirements:\n• Provides a Service Level Agreement (SLA) of at least 99.95%\n• Replicates databases in the same group synchronously\n• Minimizes the latency of database writes\nWhat should you recommend?",
    "options": [
      {
        "letter": "A",
        "text": "Create two proximity groups and two availability sets. Deploy each virtual machine to a\nunique availability set. Add one virtual machine to each proximity group.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create a proximity group and an availability set. Deploy each virtual machine to the\navailability set. Add both virtual machines to the proximity group.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Create a proximity group and two availability sets. Deploy each virtual machine to a unique\navailability set. Add both virtual machines to the proximity group.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create two proximity groups and a single availability set. Deploy both virtual machines to the\navailability set. Add one virtual machine to each proximity group.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "5. pass\nall Microsoft exams(AZURE,AWS) 6: CIA,IFRS, CERTIFICATIONS 7. APICS CERTIFICATIONS,\nCSCP, CPIM, CLTD Book for online proctor exam and we’ll remotely take the exam for you.\nPay us after confirmation of PASSED results ITTCA.org WhatsApp +1(409)223 7790",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "279",
    "question": "You have a single availability set that contains two SQL Server on Azure Virtual\nMachines instances.\nThe instances were deployed by using an Azure Marketplace SQL Server 2019\nEnterprise image that has the latest cumulative updates applied. The instances\nare configured as the nodes of a failover cluster instance (FCI) named FCI1.\nYou need to ensure that client applications can connect to FCI1. The solution\nmust meet the following requirements:\n• Provide an availability SLA.\n• Minimize costs.\nWhat should you create?",
    "options": [
      {
        "letter": "A",
        "text": "an Azure Standard Load Balancer",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a virtual network name (VNN) resource",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a Basic Azure Load Balancer",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a distributed network name (DNN) resource",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "280",
    "question": "DRAG DROP\n-\nYou have an Azure subscription.\nYou need to deploy an Azure SQL managed instance by using an Azure Resource\nManager (ARM) template. The solution must meet the following requirements:\n• The SQL managed instance must be assigned a unique identity.\n• The SQL managed instance must be available in the event of an Azure\ndatacenter outage.\nHow should you complete the template? To answer, drag the appropriate values\nto the correct targets. Each value may be used once, more than once, or not at\nall. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\n1. SQL MI must be assigned a unique identity: system-assigned identities can only be\nassigned to one resource, so that naturally makes it a unique ID 2. SQL MI must be\navailable if datacenter goes down: datacenter = availability zone So answer should be\nSystemAssigned, zoneRedundant",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "281",
    "question": "You have an Azure subscription that contains two Azure SQL managed instances\nnamed SQLMI1 and SQLMI2. SQLMI1 contains a database named DB1 and a user\nnamed User1.\nUser1 drops DB1.\nYou need to perform a point-in-time restore of DB1 to SQLMI2.\nWhat should you use to perform the restore?",
    "options": [
      {
        "letter": "A",
        "text": "Azure CLI",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Transact-SQL",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "the Azure portal",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure PowerShell",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "282",
    "question": "You deploy an instance of SQL Server on Azure Virtual Machines: named SQL1\nthat hosts multiple databases.\nYou configure the full recovery model for all the databases.\nYou perform a full backup of the master database on SQL1.\nYou need to a perform an additional backup of the master database on SQL1.\nThe solution must minimize how long it takes to perform the backup.\nWhich type of backup should you perform?",
    "options": [
      {
        "letter": "A",
        "text": "log",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "full",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "differential",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "tail-log",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "283",
    "question": "HOTSPOT\n-\nYou have a SQL Server on Azure Virtual Machines instance that hosts a database\nnamed DB1.\nYou need to configure the autogrow and autoshrink settings for DB1.\nWhich statements should you use? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nanswer is correct. To configure the autogrow setting: java Copy code ALTER DATABASE DB1\nMODIFY FILE (NAME = DB1_Data, FILEGROWTH = 10%) To configure the autoshrink setting:\nsql Copy code ALTER DATABASE DB1 SET AUTO_SHRINK ON\ncorrect",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "284",
    "question": "You have an Azure SQL database named DB1 in the General Purpose service tier.\nThe performance metrics for DB1 are shown in the following exhibit.\nYou need to reduce the Log IO percentage. The solution must minimize costs.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Change Service tier to Business Critical.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Increase the number of vCores.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Perform a checkpoint operation.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Change Recovery model to Simple.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "285",
    "question": "You have an Azure SQL database named DB1 that contains a nonclustered index\nnamed index1.\nEnd users report slow queries when they use index1.\nYou need to identify the operations that are being performed on the index.\nWhich dynamic management view should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Sys.dm_exec_query_plan_stats",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Sys.dm_db_index_physical_stats",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Sys.dm_db_index_operational_stats",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Sys.dm_db_index_useage_stats",
        "is_correct": true
      }
    ],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "286",
    "question": "HOTSPOT\n-\nYou have an Azure SQL managed instance named SQLMI1 that hosts multiple\ndatabases.\nYou need to monitor the performance of SQLMI1 and identify which database\nuses the most memory and the most disk I/O.\nWhich objects should you query? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nFor memory is sys.dm_os_buffer_descriptors. You can use this query to prove: SELECT\nDB_NAME(database_id) AS database_name, COUNT(*) * 8/1024.0 AS cached_MB FROM\nsys.dm_os_buffer_descriptors GROUP BY database_id ORDER BY cached_MB DESC; For I/\nO, the answer is correct. SELECT DB_NAME(database_id) AS database_name,\nSUM(num_of_reads + num_of_writes) AS total_io_operations FROM\nsys.dm_io_virtual_file_stats(NULL, NULL) AS virt_file_stats GROUP BY database_id ORDER\nBY total_io_operations DESC;",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "287",
    "question": "You have a Microsoft SQL Server 2019 database named DB1 and an Azure SQL\nmanaged instance named SQLMI1.\nYou need to move a SQL Server Agent job from DB1 to SQLMI1.\nWhich job attribute is unsupported in SQLMI1?",
    "options": [
      {
        "letter": "A",
        "text": "log to table",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "email notifications",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "schedules\nPage 630 of 818\n631 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "output files",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "288",
    "question": "DRAG DROP\n-\nYou create an Azure SQL managed instance and a job that performs backups.\nYou need to configure the job to notify a distribution group by email when the\njob fails. The solution must minimize administrative effort.\nWhich three actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit\nfor any of the correct orders you select.\nExplanation\nCorrect Answer:\nCommunity Discussion\nCorrect 1)configure database mail 2) create an operator 3) configure a job notification\n\n\nyep https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/job-\nautomation-managed-instance?view=azuresql\nyep https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/job-\nautomation-managed-instance?view=azuresql",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "289",
    "question": "You have an Azure SQL managed instance.\nYou need to configure the SQL Server Agent service to email job notifications.\nWhich statement should you execute?",
    "options": [
      {
        "letter": "A",
        "text": "EXECUTE msdb.dbo.sysmail_add_profile_sp @profile_name = ‘sysadmin_dbmail_profile’;",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "EXECUTE msdb.dbo.sysmail_add_profile_sp @profile_name = ‘application_dbmail_profile’;",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "EXECUTE msdb.dbo.sysmail_add_profile_sp @profile_name =\n‘AzureManagedInstance_dbmail_profile’;",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "EXECUTE msdb.dbo.sysmail_add_profile_sp @profile_name = ‘sys_dbmail_profile’;",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "290",
    "question": "You have an Azure SQL database named SQL1.\nYou need to implement a disaster recovery solution for SQL1. The solution must\nminimize the following:\n• The recovery point objective (RPO)\n• The recovery time objective (RTO)\n• Administrative effort\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Site Recovery",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "active geo-replication",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "availability groups",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "auto-failover groups",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "291",
    "question": "You need to recommend a disaster recovery solution for an on-premises\nMicrosoft SQL Server database. The solution must meet the following\nrequirements:\n• Support real-time data replication to a different geographic region.\n• Use Azure as a disaster recovery target.\n• Minimize costs and administrative effort.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "database mirroring on an instance of SQL Server on Azure Virtual Machines",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "availability groups for SQL Server on Azure Virtual Machines",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an Azure SQL Managed Instance link",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "transactional replication to an Azure SQL Managed Instance",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "292",
    "question": "You have an Azure subscription.\nYou need to deploy two instances of SQL Server on Azure virtual machines in a\nhighly available configuration that will use an Always On availability group. The\nsolution must meet the following requirements:\n• Minimize how long it takes to fail over.\n• Maintain existing connections to the primary replica during a failover.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Connect each virtual machine to a different subnet on a virtual network. Deploy a basic Azure\nload balancer.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Connect each virtual machine to a different subnet on a single virtual network.\nPage 636 of 818\n637 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Connect each virtual machine to a single subnet on a single virtual network. (Correct\nAnswer)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Connect each virtual machine to a single subnet on a virtual network. Deploy a standard\nAzure load balancer.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "293",
    "question": "You have an Azure SQL database.\nYou need to implement a disaster recovery solution that meets the following\nrequirements:\n• Minimizes how long it takes to recover the database if a datacenter fails\n• Minimizes administrative effort\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Site Recovery",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "active geo-replication",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "auto-failover groups",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure Backup",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "294",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database managed instance General Purpose",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL Database Hyperscale",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database Premium",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Azure SQL Database managed instance Business Critical",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "295",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains a resource group named RG1. RG1\ncontains an instance of SQL Server on Azure Virtual Machines named SQ1.\nYou need to use PowerShell to enable and configure automated patching for\nSQ1. The solution must include both SQL Server and Windows security updates.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect: Box 1: Set-AzVMSqlServerExtension, SQL in lowercase. Box 2: Full. SQL IAAS\nExtension only allows patching automatically in Full mode. Lightway gives a few metrics\nand NoAgent is for older versions. https://learn.microsoft.com/en-us/powershell/\nmodule/az.compute/set-azvmsqlserverextension?view=azps-9.7.0",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "296",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains an instance of SQL Server on\nAzure Virtual Machines. The virtual machine hosts a database named DB1.\nYou need to monitor DB1 by using Extended Events. The solution must meet the\nfollowing requirements:\n• Capture raw event data and store the data in Azure Storage.\n• Minimize the performance impact of capturing extended events.\nHow should you complete the Transact-SQL statement? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/xevent-db-diff-from-svr?\nview=azuresql-mi Event File target code for extended events in Azure SQL Database\nPhase 1 is PowerShell to create an Azure Storage container. Phase 2 is Transact-SQL that\nuses the Azure Storage container.\nCREATE EVENT SESSION [YourSession] ON SERVER ADD EVENT\nsqlserver.sql_statement_completed ( ACTION(sqlserver.sql_text) WHERE ( [sqlserver].\n[like_i_sql_unicode_string]([sqlserver].[sql_text], N'%SELECT%HAVING%') ) ) ADD TARGET\npackage0.event_file (SET filename = N'C:\\Junk\\YourSession_Target.xel', max_file_size = (2),\nmax_rollover_files = (2) ) WITH ( MAX_MEMORY = 2048 KB, EVENT_RETENTION_MODE =\nALLOW_MULTIPLE_EVENT_LOSS, MAX_DISPATCH_LATENCY = 3 SECONDS, MAX_EVENT_SIZE =\n0 KB, MEMORY_PARTITION_MODE = NONE, TRACK_CAUSALITY = OFF, STARTUP_STATE = OFF );\n\n\nGO https://learn.microsoft.com/en-us/sql/relational-databases/extended-events/quick-\nstart-extended-events-in-sql-server?view=sql-server-ver16",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "297",
    "question": "You have an Azure SQL database named DB1.\nYou need to query the fragmentation information of data and indexes for the\ntables in DB1.\nWhich command should you run?",
    "options": [
      {
        "letter": "A",
        "text": "sys.dm_db_index_usage_stats",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "DBCC CHECKALLOC",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "DBCC SHOWCONTIG",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "sts.dm_db_index_physical_stats",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "298",
    "question": "You have an Azure subscription that contains 20 Azure SQL databases.\nYou create a Transact-SQL statement to perform index maintenance on a\ndatabase.\nYou need to schedule the statement to run once daily against each database by\nusing Transact-SQL commands.\nWhat should you use to schedule the statement?",
    "options": [
      {
        "letter": "A",
        "text": "an Azure function",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a SQL Server Agent Job",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an elastic job",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Automation",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "299",
    "question": "HOTSPOT\n-\nYou have an Azure subscription.\nYou need to deploy a logical SQL server by using an Azure Resource Manager\n(ARM) template. The solution must ensure that the server will allow inbound\nconnectivity from any Azure resource.\nHow should you complete the template? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nhttps://learn.microsoft.com/en-us/azure/templates/microsoft.sql/servers/firewallrules?\npivots=deployment-language-bicep startIpAddress The start IP address of the firewall\nrule. Must be IPv4 format. Use value '0.0.0.0' for all Azure-internal IP addresses.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "300",
    "question": "You are modifying an existing disaster recovery solution for an Azure SQL\nmanaged instance that contains a failover group named FG1.\nYou need to ensure the maximum in-transit time for FG1 when an automatic\nfailover oсcurs.\nWhat should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "an availability group",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a secondary managed instance\nPage 648 of 818\n649 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a failover policy",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a grace period",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "301",
    "question": "You have an Azure subscription that contains three instances of SQL Server on\nAzure Virtual Machines.\nYou plan to implement a disaster recovery solution.\nYou need to be able to perform disaster recovery drills regularly. The solution\nmust meet the following requirements:\n• Minimize administrative effort for the recovery drills.\n• Isolate the recovery environment from the production environment\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "native Microsoft SQL Server backup",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure Site Recovery",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Recovery Services vaults",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Backup",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "302",
    "question": "You have an instance of SQL Server on Azure Virtual Machines named VM1.\nYou need to implement a disaster recovery solution that meets the following\nrequirements:\n• Returns the solution to an operational state within 15 minutes of a failure\n• Can perform disaster recovery testing in an isolated environment\n• Minimizes administrative effort\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "active geo-replication",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "auto-failover groups",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Site Recovery",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "a failover cluster instance (FCI)",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": ""
  },
  {
    "number": "303",
    "question": "You plan to deploy an Azure SQL managed instance.\nYou need to restore database backups across regions.\nWhich type of storage account should you use?",
    "options": [
      {
        "letter": "A",
        "text": "locally-redundant storage (LRS)",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "zone-redundant storage (ZRS)",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "geo-zone-redundant storage (GZRS)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "geo-redundant storage (GRS)",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "304",
    "question": "You have an Azure virtual machine named VM1 that runs Windows Server 2022\nand hosts a Microsoft SQL Server 2019 instance named SQL1.\nYou need to configure SQL1 to use mixed mode authentication.\nWhich procedure should you run?",
    "options": [
      {
        "letter": "A",
        "text": "sp_addremotelogin",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "xp_instance_regwrite",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "sp_change_users_login",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "xp_grant_login",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "305",
    "question": "You have an Azure SQL managed instance named SQLMI1 that has the following\nsettings:\n• vCores: 4\n• Service tier: General Purpose\n• Hardware generation: Standard-series (Gen5)\nYou discover that memory pressure on SQLMI1 is high.\nYou need to reduce the memory pressure on SQLMI1. The solution must\nminimize costs.\nWhat should to do?",
    "options": [
      {
        "letter": "A",
        "text": "Enable the Query Store.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Change vCores to 8.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Change Hardware generation to Premium-series.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Change Service tier to Business Critical.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": ""
  },
  {
    "number": "306",
    "question": "HOTSPOT\n-\nYou have an instance of SQL Server on Azure Virtual Machines named VM1.\nYou need to use an Azure Automation runbook to initiate a SQL Server database\nbackup on VM1.\nHow should you complete the command? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nthat is correct\nhttps://learn.microsoft.com/en-us/powershell/module/az.automation/start-\nazautomationrunbook?view=azps-12.5.0 EXAMPLE: Start-AzAutomationRunbook -\nAutomationAccountName \"Contoso17\" -Name \"Runbk01\" -ResourceGroupName\n\"ResourceGroup01\" -MaxWaitSeconds 1000 -Wait\nStart-AzAutomationRunbook -Name $Runbook -ResourceGroupName $rg -\nAutomationAccountName $Account -Wait It is not \"optimal\" but It will work",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "307",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database Premium",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure SQL Database Basic",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database managed instance General Purpose\nPage 656 of 818\n657 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Database Hyperscale",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "308",
    "question": "You have an Azure subscription that contains two instances of SQL Server on\nAzure Virtual Machines named VM1 and VM2. Both instances run Microsoft SQL\nServer 2019 CU8.\nYou need to deploy a failover cluster instance (FCI) to VM1 and VM2. The\nsolution must eliminate the need for the following:\n• A distributed network name (DNN)\n• A load balancer\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Deploy VM1 and VM2 to a single proximity placement group.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Deploy VM1 and VM2 to different proximity placement groups in the same Azure region.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Connect VM1 and VM2 to a single subnet.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Connect VM1 and VM2 to different subnets on a single virtual network.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": ""
  },
  {
    "number": "309",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains a storage account named\ndatabasebackups.\nYou have an Azure SQL managed instance named DB1.\nYou need to back up DB1 to databasebackups.\nHow should you complete the commands? To answer, select the appropriate\noptions in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nSAS makes sense, can anyone confirm that back would be to the url with \"COPY_ONLY\"? I\nam still searching for the correct answer, but I feel that it could be with CHECKSUM\nKeyword for checksum would be for online migration\nKeyword for checksum would be for online migration\nSQL Manage Instances support only copy only backups\n\n\nSAS + COPY_ONY, full tutorial here: https://techcommunity.microsoft.com/blog/\nazuresqlblog/how-to-take-secure-on-demand-backups-on-sql-managed-instance/\n3638369",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "310",
    "question": "You have an Azure SQL database named DB1.\nYou need to ensure that DB1 will support automatic failover without data loss if\na datacenter fails. The solution must minimize costs.\nWhich deployment option and pricing tier should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Azure SQL Database Premium",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Azure SQL Database serverless",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure SQL Database managed instance General Purpose",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Database Hyperscale",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": ""
  },
  {
    "number": "311",
    "question": "SIMULATION\n-\nYou need to configure db1 to pause automatically after one hour of inactivity.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "312",
    "question": "SIMULATION\n-\nYou need to ensure that any enhancements made to the Query Optimizer\nthrough patches are available to db1 and db2 on sql12345678.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "313",
    "question": "SIMULATION\n-\nYou plan to create an automation runbook that will create database users in db1\nfrom Azure AD identities.\nYou need to configure sql12345678 to support the creation of new database\nusers.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "314",
    "question": "SIMULATION\n-\nYou plan to perform performance testing of db1.\nYou need prevent db1 from reverting to the last known good query plan.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "315",
    "question": "SIMULATION\n-\nYou have a legacy application written for Microsoft SQL Server 2012. The\napplication will be the only application that accesses db1.\nYou need to ensure that db1 is compatible with all the features and syntax of\nSQL Server 2012.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "316",
    "question": "SIMULATION\n-\nYou need to add an Azure AD user named [email protected] to db1.\nUser2-12345678 must be able to read data from all the tables in db1 without\nbeing able to modify the data.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCould also grant the user the db_datareader fixed database role which would allow all\ntables to be read without the ability to modify the data.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "317",
    "question": "SIMULATION\n-\nYou need to ensure that db1 supports a minimum of one vCore, a maximum of\nfour vCores, and a database of up to 45 GB.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\n\n\nCommunity Discussion\nObjective - supports a minimum of one vCore, a maximum of four vCores This requires\ncompute tier in Serverless option, with standard-series (Gen5).\n------------------------------------------------------------------------------------------------------\nWith the suggested solution of using Provisioned compute resources are pre-allocated,\nbilled per hour for all 4vCores constantly, but the objective asks for the flexibility.\nOtherwise this question would make no sense as it is obvious that with 4vCores it\nsupports 1vCore as well (LOL)",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "318",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\n\n\nOverview\n-\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\n\n\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes\n-\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\n\n\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\n\n\nYou need to recommend which service and target endpoint to use when\nmigrating the databases from SVR1 to Instance1. The solution must meet the\navailability requirements.\nWhat should you recommend? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is correct: The link feature also facilitates migrating from SQL Server to SQL\nManaged Instance, which enables: The most performant, minimal downtime migration,\ncompared to all other solutions available today. True online migration to SQL Managed\nInstance in any service tier. https://learn.microsoft.com/en-us/azure/azure-sql/\nmanaged-instance/managed-instance-link-feature-overview?view=azuresql#migrate-to-\nazure Only VNet-local endpoint is supported to establish a link with SQL Managed\nInstance. https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/\nmanaged-instance-link-feature-overview?view=azuresql#limitations",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "319",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\n\n\nOverview\n-\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\n\n\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes\n-\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\n\n\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\n\n\nYou need to recommend a service tier and a method to offload analytical\nworkloads for the databases migrated from SVR1. The solution must meet the\navailability and business requirements.\nWhat should you recommend? To answer, select the appropriate options in the\nanswer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "320",
    "question": "You have an Azure subscription that contains the resources shown in the\nfollowing table.\nYou plan to use SQLDB11 as an elastic job database to run jobs on SQLDB11 and\nSQLDB22.\nWhat is the minimum number of database scoped credentials required for the\nelastic jobs?",
    "options": [
      {
        "letter": "A",
        "text": "1",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "2",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "3",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "4",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "321",
    "question": "SIMULATION\n-\nYou need to configure your user account as the Azure AD admin for the server\nnamed sql12345678.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\n\n\nCommunity Discussion\nIt would be something like these steps: https://docs.devolutions.net/rdm/data-sources/\ndata-sources-types/advanced-data-sources/microsoft-azure-sql/enable-azure-active-\ndirectory-authentication/configure-admin/",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "322",
    "question": "SIMULATION\n-\nYou need to create a new database role named role1 that can read the contents\nof the SalesLT.SalesOrderDetail table only in db1.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nAnswer is wrong. It describes creating a server role. You need to create a new database\nrole named \"role1\", with select permissions on SalesLT.SalesOrderDetail in db1",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "323",
    "question": "SIMULATION\n-\nYou need to classify the StandardCost and ListPrice columns of the\nSalesLT.Product table in db1. The columns must have the following\nconfigurations:\n• An information type of Financial\n• A sensitivity label of Confidential\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\n\n\nCommunity Discussion\nSteps would be similar to this: https://learn.microsoft.com/en-us/sql/relational-\ndatabases/security/sql-data-discovery-and-classification?view=sql-server-ver16&tabs=t-\nsql",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "324",
    "question": "SIMULATION\n-\nYou need to protect all the databases on sql12345678 from SQL injection\nattacks.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "325",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend a process to automate the management of DB3. The\nsolution must meet the management requirements.\nWhat should be the first step of the process?",
    "options": [
      {
        "letter": "A",
        "text": "Configure Microsoft Entra authentication for the logical server that hosts DB3.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Configure a private endpoint for connectivity to DB3.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create database-scoped credentials in DB3.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Create a database that has database-scoped credentials.\nPage 704 of 818\n705 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "326",
    "question": "DRAG DROP\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\n\n\nOverview\n-\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\n\n\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes\n-\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\n\n\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\n\n\nYou need to recommend an authentication solution for App1 access to DB1 and\nDB2 after their migration to Instance1. The solution must meet the availability\nrequirements.\nWhich actions should you perform in sequence? To answer, drag the appropriate\nactions to the correct order. Each action may be used once, more than once, or\nnot at all. You may need to drag the split bar between panes or scroll to view\ncontent.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\nCommunity Discussion\nI don't agree with the last step suggested - System-Assigned Service Principal. A system-\nassigned service principal is useful for scenarios where you need to grant specific Azure\nresources access to other resources. Enabling a system-assigned service principal can\nindeed be a good option for managing access and authentication in Azure. However, in\nthe context of your scenario, where App1 needs to maintain access to DB1 and DB2 after\n\n\ntheir migration to Azure Managed Instance (Instance1), using Microsoft Entra\nauthentication and granting admin consent to an app registration is more aligned with\nthe requirements for seamless integration and security. So the last step is to reuse App1\nregistration. ----------------------------------------------------------------------------- 1.\nEnable Microsoft Entra ID authentication on Instance1 2. Implement Microsoft Entra ID\nCloud Sync 3. Grant admin consent to an app registration (for App1) in Microsoft Entra ID\nFrom the case study: App1 runs on Microsoft Entra hybrid joined servers that run\nWindows Server 2022. App1 uses Kerberos authentication - for this to happen, there is\nalready Entra ID app registration for App1.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "327",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend a solution that will enable remote developers to\naccess DB1 and DB2. The solution must support the planned changes and meet\nthe security requirements.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "a public endpoint via a database-level firewall rule",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "a Point-to-Site (P2S) VPN",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "a public endpoint via a server-level firewall rule",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "a private endpoint \nPage 717 of 818\n718 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "328",
    "question": "You have five instances of SQL Server on Azure Virtual Machines.\nYou need to monitor Microsoft SQL Server performance for all the instances by\nconsolidating metrics into a single graphic display. The solution must minimize\nadministrative effort.\nWhat should you use?",
    "options": [
      {
        "letter": "A",
        "text": "Azure Monitor",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Log Analytics",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "SQL Insights",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure SQL Analytics",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "329",
    "question": "You have an instance of SQL Server on Azure Virtual Machines named SQL1.\nSQL1 contains an Extended Events session named session1 that captures\nMicrosoft SQL Server events.\nYou need to correlate the session events with events captured by Event Tracing\nfor Windows (ETW).\nWhat should you do for session1?",
    "options": [
      {
        "letter": "A",
        "text": "Modify the Set Session Event Filters settings.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Add a target.",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Add an action.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Modify the Specify Session Data Storage settings.",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "330",
    "question": "You have an Azure subscription that contains the following resources:\n• 10 Azure SQL databases\n• Five Azure SQL managed instances\n• Five instances of SQL Server on Azure Virtual Machines\nYou need to implement a centralized monitoring solution for all the Azure SQL\nresources. The solution must minimize administrative effort.\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Log Analytics",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL Analytics",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Query Performance Insight",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "SQL Insights",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "331",
    "question": "SIMULATION\n-\nYou need to ensure that all queries executed against db1 are captured in the\nQuery Store.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "332",
    "question": "SIMULATION\n-\nYou need to enable change data capture (CDC) for db1.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "333",
    "question": "SIMULATION\n-\nIn an Azure SQL database named db1, you need to enable page compression on\nthe PK_SalesOrderHeader_SalesOrderID clustered index of the\nSalesLT.SalesOrderHeader table.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "334",
    "question": "SIMULATION\n-\nYou need to rebuild the index for PK_Address_AddressID in the SalesLT.Address\ntable in db1. The solution must meet the following requirements:\n• Ensure that 30 percent of the index page is reserved for future updates and\ninserts.\n• Minimize page splits.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "335",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend a solution to meet the security requirements and the\nbusiness requirements for DB3.\nWhat should you recommend as the first step of the solution?",
    "options": [
      {
        "letter": "A",
        "text": "Run the sp_addarticle stored procedure.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Run the ALTER TABLE statement and specify the ENABLE CHANGE_TRACKING Clause.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Run the ALTER DATABASE statement and specify the SET CHANGE_TRACKING = ON Clause.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Run the sys.sp_cdc_enable_db stored procedure.\nPage 731 of 818\n732 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "for the database. CDC captures\ndetailed information about changes, including the values before and after the change,\nand records them in change tables. It's ideal for auditing and tracking all changes,\nincluding those within individual transactions. CDC provides comprehensive change\ntracking, making it suitable for scenarios where detailed auditing is required.\nPage 732 of 818\n733 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "336",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend a solution to ensure that the performance of DB3 is\noptimized after the migration to Azure SQL Database. The solution must meet\navailability requirements.\nWhat should you include in the recommendation?",
    "options": [
      {
        "letter": "A",
        "text": "vertical scaling",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "a custom resource pool",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Resource Governor",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "horizontal scaling\nPage 737 of 818\n738 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "337",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to identify the event_file target for monitoring DB3 after the migration\nto Azure SQL Database. The solution must meet the management requirements.\nWhat should you use as the event_file target?",
    "options": [
      {
        "letter": "A",
        "text": "a SQL Server filegroup",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "an Azure SQL database",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "an Azure Files share",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "an Azure Blob Storage container \nPage 743 of 818\n744 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "338",
    "question": "You manage 100 Azure SQL managed instances located across 10 Azure regions.\nYou need to receive voice message notifications when a maintenance event\naffects any of the 10 regions. The solution must minimize administrative effort.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "From the Azure portal, create a service health alert.",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "From the Azure portal, create an Azure Advisor operational excellence alert.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "From the Azure portal, configure an activity log alert.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "From Microsoft SQL Server Management Studio (SSMS), configure a SQL Server agent job.",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "339",
    "question": "HOTSPOT\n-\nYou plan to deploy three instances of SQL Server on Azure Virtual Machines that\nwill each contain 20 databases.\nYou need to recommend a solution that meets the following requirements:\n• Ensures that the deployment is highly available\n• Minimizes administrative effort to manage users, logins, permissions, and SQL\nServer Agent jobs across the instances\nWhat should you include in the recommendation? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nThe minimum SQL version for SQL on Azure VM is 2016 that supports Availability Groups.\nThere for the answer should be \"Always On\" and \"2017\"\nAlways On, SQL Server 2017",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "340",
    "question": "You have two Azure virtual machines named Server1 and Server2 that run\nWindows Server 2022 and are joined to an Active Directory Domain Services (AD\nDS) domain named contoso.com.\nBoth virtual machines have a default instance of Microsoft SQL Server 2019\ninstalled. Server1 is configured as a master server, and Server2 is configured as\na target server.\nOn Server1, you create a proxy account named contoso\\sqlproxy.\nYou need to ensure that the SQL Server Agent job steps can be downloaded\nfrom Server1 and run on Server2.\nWhich two actions should you perform? Each correct answer presents part of\nthe solution.\nNOTE: Each correct selection is worth one point.",
    "options": [
      {
        "letter": "A",
        "text": "On Server2, grant the contoso\\sqlproxy account the Impersonate a client after authentication\nuser right.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "On Server2, grant the contoso\\sqlproxy account the Access this computer from the network\nuser right.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "On Server2, create a proxy account.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "On Server1, set the AllowDownloadedJobsToMatchProxyName registry entry to 1.\nE) On Server2, set the AllowDownloadedJobsToMatchProxyName registry entry to 1. (Correct\nAnswer)",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "E ?"
  },
  {
    "number": "341",
    "question": "You have an Azure subscription. The subscription contains an instance of SQL\nServer on Azure Virtual Machines named SQL1 and an Azure Automation account\nnamed account1.\nYou need to configure account1 to restart the SQL Server Agent service if the\nservice stops.\nWhich setting should you configure?",
    "options": [
      {
        "letter": "A",
        "text": "Start/Stop VM",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Change tracking",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Update management",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "State configuration (DSC)",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "342",
    "question": "SIMULATION\n-\nYou need to ensure that any unused indexes of the databases on sql12345678\nare removed automatically.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "343",
    "question": "SIMULATION\n-\nYou need to generate an email alert to [email protected] when CPU percentage\nutilization for db1 is higher than average.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "344",
    "question": "HOTSPOT\n-\nYou plan to deploy an Always On failover cluster instance (FCI) on Azure virtual\nmachines.\nYou need to provision an Azure Storage account to host a cloud witness for the\ndeployment.\nHow should you configure the storage account? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nStandard general-purpose v2 and LRS is correct according to ChatGPT\nI think second option should be GRS",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "345",
    "question": "DRAG DROP\n-\nYou have an Azure virtual machine named Server1 that contains an instance of\nMicrosoft SQL Server 2022 named SQL1.\nSQL1 contains two databases named DB1 and DB2.\nYou need to take a snapshot backup of DB1 and DB2. The backup must NOT\ndisrupt the backup chain and must NOT affect other databases.\nWhich three Transact-SQL commands should you run in sequence? To answer,\nmove the appropriate commands from the list of commands to the answer area\nand arrange them in the correct order.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nALTER SERVER CONFIGURATION SET SUSPEND_FOR_SNAPSHOT_BACKUP = ON (GROUP =\n(testdb1, testdb2)); BACKUP GROUP testdb1, testdb2 TO DISK = 'D:\\Temp\\db.bkm' WITH\nMETADATA_ONLY, FORMAT; https://learn.microsoft.com/en-us/sql/relational-databases/\nbackup-restore/create-a-transact-sql-snapshot-backup?view=sql-server-ver16",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "346",
    "question": "SIMULATION\n-\nYou need to configure a disaster recovery solution for db1. When a failover\noccurs, the connection strings to the database must remain the same. The\nsecondary server must be in the West US 3 Azure region.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "347",
    "question": "SIMULATION\n-\nYou need to configure high availability for db1. The solution must tolerate the\nloss of an Azure datacenter without data loss or the need to modify application\nconnection strings.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nTo configure high availability for DB1 in a way that tolerates the loss of an Azure\ndatacenter without data loss and without needing to modify application connection\nstrings, you should use Failover Groups rather than a standby replica.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "348",
    "question": "SIMULATION\n-\nYou need to configure a monthly backup of db2. The monthly backups must be\nretained for five years, and then deleted.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "349",
    "question": "SIMULATION\n-\nYou need to provide a user named user2-12345678 with the ability to back up\nand restore databases and change the compute setting of the databases\nlocated on a server named sql12345678. User2-12345678 must be prevented from\nconnecting to the databases and modifying database server settings. The\nsolution must minimize administrative effort.\nTo complete this task, sign in to the virtual machine. You may need to use SQL\nServer Management Studio and the Azure portal.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nProvided solution is wrong! -------------------------------------- Here is the solution:\nObjective 1 - provide user2-12345678 ability to back up and restore databases; Objective 2\n- provide user2-12345678 ability to and change the compute setting of the databases\nlocated on a server named sql12345678. Objective 3 - User2-12345678 must be prevented\nfrom connecting to the databases Objective 4 - User2-12345678 must be prevented from\nmodifying database server settings. Objective 5 - The solution must minimize\nadministrative effort.\n------------------------------------------------------------------------------------------- In\nAzure, go to SQL Server and configure Access Control (IAM). Add new role assignment >\nSQL DB Contributor > Try - Query Editor - Login failed for user - Objective 3 OK Try - SSMS\n- Connect to server - export database / import database - Objective 1 OK Try - db\n\n\nCompute + storage - changes - Objective 2 Try - to modify SQL server settings - fails on\nvarious settings tried - Objective 4 OK Minimal effort to configure - add user to built-in\nrole - Objective 5 OK",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "350",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend a backup solution to restore DB3. The solution must\nmeet the availability requirements.\nWhich type of backup should you use?",
    "options": [
      {
        "letter": "A",
        "text": "differential",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "transaction log",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "long-term retention (LTR)",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "point-in-time restore (PITR)\nPage 769 of 818\n770 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "351",
    "question": "Case study -\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\nOverview -\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\n\n\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\n\n\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes -\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n\n\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\nYou need to recommend which configuration to perform twice to enable access\nto the primary and secondary replicas of DB3. The solution must meet the\navailability requirements.\nWhat should you recommend?",
    "options": [
      {
        "letter": "A",
        "text": "Enable database firewall rules.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Create database-scoped credentials.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Configure connection strings that reference the read-write listener.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Configure virtual network service endpoints.\nPage 775 of 818\n776 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "352",
    "question": "HOTSPOT\n-\nCase study\n-\nThis is a case study. Case studies are not timed separately. You can use as much\nexam time as you would like to complete each case. However, there may be\nadditional case studies and sections on this exam. You must manage your time\nto ensure that you are able to complete all questions included on this exam in\nthe time provided.\nTo answer the questions included in a case study, you will need to reference\ninformation that is provided in the case study. Case studies might contain\nexhibits and other resources that provide more information about the scenario\nthat is described in the case study. Each question is independent of the other\nquestions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you\nto review your answers and to make changes before you move to the next\nsection of the exam. After you begin a new section, you cannot return to this\nsection.\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the\nbuttons in the left pane to explore the content of the case study before you\nanswer the questions. Clicking these buttons displays information such as\nbusiness requirements, existing environment, and problem statements. If the\ncase study has an All Information tab, note that the information displayed is\nidentical to the information displayed on the subsequent tabs. When you are\nready to answer a question, click the Question button to return to the question.\n\n\nOverview\n-\nADatum Corporation is a financial services company that has a main office in\nNew York City.\nExisting Environment. Licensing Agreement\nADatum has a Microsoft Volume Licensing agreement that includes Software\nAssurance.\nExisting Environment. Network Infrastructure\nADatum has an on-premises datacenter and an Azure subscription named Sub1.\nSub1 contains a virtual network named Network1 in the East US Azure region.\nThe datacenter is connected to Network1 by using a Site-to-Site (S2S) VPN.\nExisting Environment. Identity Environment\nThe on-premises network contains an Active Directory Domain Services (AD DS)\nforest.\nThe forest contains a single domain named corp.adatum.com.\nThe corp.adatum.com domain syncs with a Microsoft Entra tenant named\nadatum.com.\nExisting Environment. Database Environment\nThe datacenter contains the servers shown in the following table.\n\n\nDB1 and DB2 are used for transactional and analytical workloads by an\napplication named App1.\nApp1 runs on Microsoft Entra hybrid joined servers that run Windows Server\n2022. App1 uses Kerberos authentication.\nDB3 stores compliance data used by two applications named App2 and App3.\nDB3 performance is monitored by using Extended Events sessions, with the\nevent_file target set to a file share on a local disk of SVR3.\nResource allocation for DB3 is managed by using Resource Governor.\nRequirements. Planned Changes\n-\nADatum plans to implement the following changes:\n• Deploy an Azure SQL managed instance named Instance1 to Network1.\n• Migrate DB1 and DB2 to Instance1.\n• Migrate DB3 to Azure SQL Database.\n• Following the migration of DB1 and DB2, hand over database development to\nremote developers who use Microsoft Entra joined Windows 11 devices.\n• Following the migration of DB3, configure the database to be part of an auto-\nfailover group.\nRequirements. Availability Requirements\n\n\nADatum identifies the following post-migration availability requirements:\n• For DB1 and DB2, offload analytical workloads to a read-only database replica\nin the same Azure region.\n• Ensure that if a regional disaster occurs, DB1 and DB2 can be recovered from\nbackups.\n• After the migration, App1 must maintain access to DB1 and DB2.\n• For DB3, manage potential performance issues caused by resource demand\nchanges by App2 and App3.\n• Ensure that DB3 will still be accessible following a planned failover.\n• Ensure that DB3 can be restored if the logical server is deleted.\n• Minimize downtime during the migration of DB1 and DB2.\nRequirements. Security Requirements\nADatum identifies the following security requirements for after the migration:\n• Ensure that only designated developers who use Microsoft Entra joined\nWindows 11 devices can access DB1 and DB2 remotely.\n• Ensure that all changes to DB3, including ones within individual transactions,\nare audited and recorded.\nRequirements. Management Requirements\nADatum identifies the following post-migration management requirements:\n• Continue using Extended Events to monitor DB3.\n• In Azure SQL Database, automate the management of DB3 by using elastic jobs\nthat have database-scoped credentials.\nRequirements. Business Requirements\nADatum identifies the following business requirements:\n• Minimize costs whenever possible, without affecting other requirements.\n• Minimize administrative effort.\n\n\nYou plan to deploy Instance by using the following script.\nYou need to specify the licenseType and storageRedundancy parameters. The\ndeployment must meet the availability requirements and the business\nrequirements for DB1 and DB2.\nTo what should you set each parameter? To answer, select the appropriate\noptions in the answer area.\n\n\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nThis is wrong. Correct answers are: BasePrice: By selecting BasePrice, you take advantage\nof your existing SQL Server licenses with Software Assurance, minimizing costs.\nGeoRedundantStorage: This storage redundancy option ensures that your data is\nreplicated across multiple geographic locations, providing high availability and disaster\nrecovery capabilities. --------------------------------------- With the Volume licensing with\nSoftware Assurance, and valid active on-premises licenses for Windows Servers and SQL\nServers that is the case here, Adatum is eligible for Azure Hybrid Benefit, and can and\nshould use BasePrice. https://learn.microsoft.com/en-us/samples/azure/azure-\nquickstart-templates/sqlmi-new-vnet/ https://azure.microsoft.com/en-us/pricing/\nhybrid-benefit/#faq",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "353",
    "question": "HOTSPOT\n-\nYou have an on-premises Microsoft SQL Server database named DB1.\nYou have an Azure subscription.\nYou need to migrate DB1 to an Azure SQL service that meets the following\nrequirements:\n• Protects the confidentiality of sensitive data from malware and high-privileged\nunauthorized database administrators\n• Supports pattern matching for server-side database operations\n• Uses a hardware-based encryption technology\nWhich Azure SQL service and attestation service should you include in the\nsolution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "354",
    "question": "DRAG DROP\n-\nYou have two on-premises Microsoft SQL Server instances named SQL1 and\nSQL2.\nYou have an Azure subscription.\nYou need to sync a subset of tables between the databases hosted on SQL1 and\nSQL2 by using SQL Data Sync.\nWhich five actions should you perform in sequence? To answer, move the\nappropriate actions from the list of actions to the answer area and arrange\nthem in the correct order.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "355",
    "question": "You have an on-premises Microsoft SQL Server 2022 instance that hosts a 60-TB\nproduction database named DB1.\nYou plan to migrate DB1 to Azure.\nYou need to recommend a hosting solution for DB1.\nWhich Azure SQL Database service tier should you use to host DB1?",
    "options": [
      {
        "letter": "A",
        "text": "Hyperscale",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "Business Critical",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "General Purpose",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "356",
    "question": "HOTSPOT\n-\nYou have an Azure SQL database named DB1.\nYou have 10 Azure virtual machines that connect to a virtual network subnet\nnamed Subnet1.\nYou need to implement a database-level firewall that meets the following\nrequirements:\n• Ensures that only the 10 virtual machines can access DB1\n• Follows the principle of least privilege\nHow should you configure the firewall rule, and how should you establish\nnetwork connectivity from the virtual machines to DB1?\nTo answer, select the appropriate options in the answer area.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nWRONG ASNWER Correct is: Allow traffic from a specific virtual network: This configuration\nrestricts access to DB1 to only the virtual machines within Subnet1, ensuring that only\nthe intended VMs can connect to the database. Create a service endpoint: Service\nendpoints provide secure and optimized connectivity to Azure SQL Database, ensuring\nthat the traffic remains within the Azure network and is not exposed to the public\ninternet. This configuration ensures that only the 10 virtual machines can access DB1,\nadheres to the principle of least privilege, and establishes secure network connectivity.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "357",
    "question": "You have an Azure subscription that contains an Azure SQL database named\nDB1.\nYou need to host elastic jobs by using DB1. DB1 will also be configured as a job\ntarget. The solution must support the use of location-based Conditional Access\npolicies.\nWhat should the elastic jobs use to access DB1?",
    "options": [
      {
        "letter": "A",
        "text": "a system-assigned managed identity",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Azure SQL sign-in credentials",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "database-scoped credentials",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "a user-assigned managed identity\nPage 790 of 818\n791 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "358",
    "question": "HOTSPOT\n-\nYou have an Azure virtual machine named Server1 that has Microsoft SQL Server\ninstalled. Server1 contains a database named DB1.\nYou have a logical SQL server named ASVR1 that contains an Azure SQL\ndatabase named ADB1.\nYou plan to use SQL Data Sync to migrate DB1 from Server1 to ASVR1.\nYou need to prepare the environment for the migration. The solution must\nensure that the connection from Server1 to ADB1 does NOT use a public\nendpoint.\nWhat should you do? To answer, select the appropriate options in the answer\narea.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "359",
    "question": "HOTSPOT\n-\nYou have an Azure SQL managed instance named Server1 and an Azure Blob\nStorage account named storage1 that contains Microsoft SQL Server database\nbackup files.\nYou plan to use Log Replay Service to migrate the backup files from storage1 to\nServer1. The solution must use the highest level of security when connecting to\nstorage1.\nWhich PowerShell cmdlet should you run, and which parameter should you\nspecify to secure the connection? To answer, select the appropriate options in\nthe answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "360",
    "question": "You have 25 Azure SQL databases.\nYou need to implement a centralized database management solution that uses\nTransact-SQL.\nWhat should you include in the solution?",
    "options": [
      {
        "letter": "A",
        "text": "elastic jobs",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "an Azure Automation runbook",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Azure Functions",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Azure Logic Apps\nPage 795 of 818\n796 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "361",
    "question": "You have an Azure virtual machine named Server1 that runs Windows Server\n2022. Server1 contains an instance of Microsoft SQL Server 2022 named SQL1 and\na database named DB1.\nYou create a master key in the master database of SQL1.\nYou need to create an encrypted backup of DB1.\nWhat should you do?",
    "options": [
      {
        "letter": "A",
        "text": "Create a symmetric key in DB1.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Enable virtualization-based security (VBS) on Server1.\nPage 796 of 818\n797 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Create a certificate in DB1.",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Create a certificate in the master database of SQL1.",
        "is_correct": true
      }
    ],
    "correct_answer": "D",
    "explanation": "?"
  },
  {
    "number": "362",
    "question": "HOTSPOT\n-\nYou have a SQL Server on Azure Virtual Machines instance named SQLVM1 that\ncontains two databases named DB1 and DB2. The database and log files for DB1\nand DB2 are hosted on managed disks.\nYou need to perform a snapshot backup of DB1 and DB2.\nHow should you complete the T-SQL statements? To answer, select the\nappropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nWRONG --------------- ALTER SERVER CONFIGURATION SET\nSUSPEND_FOR_SNAPSHOT_BACKUP = ON (GROUP = (testdb1, testdb2)); BACKUP GROUP\ntestdb1, testdb2 TO DISK = 'D:\\Temp\\db.bkm' WITH METADATA_ONLY, FORMAT; ALTER\nSERVER CONFIGURATION SET SUSPEND_FOR_SNAPSHOT_BACKUP = ON",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "363",
    "question": "You have an Azure subscription.\nYou plan to provision a single Azure SQL database.\nYou need to ensure that the database supports the autoscaling of compute\nresources.\nWhich service tier should you choose?",
    "options": [
      {
        "letter": "A",
        "text": "Premium",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "General Purpose",
        "is_correct": true
      },
      {
        "letter": "C",
        "text": "Business Critical",
        "is_correct": false
      },
      {
        "letter": "D",
        "text": "Standard",
        "is_correct": false
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "364",
    "question": "HOTSPOT\n-\nYou plan to deploy an instance of SQL Server on Linux Azure Virtual Machines.\nThe instance will run Microsoft SQL Server 2022 and use the SQL Server IaaS\nAgent extension for Linux.\nWhich Linux operating system should you deploy, and which benefit will the SQL\nServer IaaS Agent extension provide? To answer, select the appropriate options\nin the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nWRONG --------------- Linux SQL IaaS Agent extension has the following limitations: Only\nSQL Server VMs running on the Ubuntu Linux operating system are supported. Other\nLinux distributions are not currently supported. Benefit: Registration of a SQL virtual\nmachine resource in Azure",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "365",
    "question": "DRAG DROP\n-\nYou have a burstable Azure virtual machine named VM1 that hosts an instance\nof Microsoft SQL Server.\nYou need to attach an Azure ultra disk to VM1. The solution must minimize\ndowntime on VM1.\nIn which order should you perform the actions? To answer, move all actions\nfrom the list of actions to the answer area and arrange them in the correct\norder.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "366",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains a resource group named RG1. RG1\ncontains an Azure SQL Server named Server1 in the West US Azure region.\nYou need to ensure that any Azure services deployed to RG1 can access Server1.\nHow should you complete the Azure Command-Line Interface (CLI) command?\nTo answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nExplanation\nCorrect Answer:\n\n\nCommunity Discussion\nCorrect :) az sql server firewall-rule create \\ --resource-group RG1 \\ --server Server1 \\ --\nname allowazureservices \\ --start-ip-address 0.0.0.0 \\ --end-ip-address 0.0.0.0",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "367",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have the on-premises networks shown in the following table.\nYou have an Azure subscription that contains an Azure SQL Database server\nnamed SQL1. SQL1 contains two databases named DB1 and DB2.\nYou need to configure access to DB1 and DB2. The solution must meet the\nfollowing requirements:\n• Ensure that DB1 can be accessed only by users in Branch1.\n• Ensure that DB2 can be accessed only by users in Branch2.\nSolution: You connect to DB1 and run the following command.\nEXECUTE sp_set_firewall_rule ‘Allow db1 users’, ‘131.107.10.0’, ‘131.107.10.255’\nYou connect to DB2 and run the following command.\nEXECUTE sp_set_database_firewall_rule ‘Allow db2 users’, ‘131.107.11.0’,\n‘131.107.11.255’\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes \nPage 807 of 818\n808 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": false
      }
    ],
    "correct_answer": "A",
    "explanation": "?"
  },
  {
    "number": "368",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have the on-premises networks shown in the following table.\nYou have an Azure subscription that contains an Azure SQL Database server\nnamed SQL1. SQL1 contains two databases named DB1 and DB2.\nYou need to configure access to DB1 and DB2. The solution must meet the\nfollowing requirements:\n• Ensure that DB1 can be accessed only by users in Branch1.\n• Ensure that DB2 can be accessed only by users in Branch2.\nSolution: You connect to the master of SQL1 and run the following command.\nEXECUTE sp_set_firewall_rule ‘Allow db1 and db2 users’, ‘131.107.11.0’,\n‘131.107.11.255’\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No \nPage 809 of 818\n810 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "369",
    "question": "Note: This question is part of a series of questions that present the same\nscenario. Each question in the series contains a unique solution that might\nmeet the stated goals. Some question sets might have more than one correct\nsolution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it.\nAs a result, these questions will not appear in the review screen.\nYou have the on-premises networks shown in the following table.\nYou have an Azure subscription that contains an Azure SQL Database server\nnamed SQL1. SQL1 contains two databases named DB1 and DB2.\nYou need to configure access to DB1 and DB2. The solution must meet the\nfollowing requirements:\n• Ensure that DB1 can be accessed only by users in Branch1.\n• Ensure that DB2 can be accessed only by users in Branch2.\nSolution: You connect to DB1 and run the following command.\nEXECUTE sp_set_firewall_rule ‘Deny db1 users’, ‘131.107.11.0’, ‘131.107.11.255’\nYou connect to DB2 and run the following command.\nEXECUTE sp_set_database_firewall_rule ‘Deny db2 users’, ‘131.107.10.0’,\n‘131.107.10.255’\nDoes this meet the goal?",
    "options": [
      {
        "letter": "A",
        "text": "Yes\nPage 811 of 818\n812 Microsoft - DP-300 Practice Questions - SecExams.com",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "No",
        "is_correct": true
      }
    ],
    "correct_answer": "B",
    "explanation": "?"
  },
  {
    "number": "370",
    "question": "HOTSPOT\n-\nYou have an on-premises Microsoft SQL Server instance named SQLSVR1 that\nhosts a database named DB1.\nYou have an Azure subscription that contains an Azure SQL database named\nSQLDB1.\nYou need to perform the following actions:\n• Sync DB1 with SQLDB1 by using SQL Data Sync.\n• Configure an index in DB1 by using CREATE INDEX.\n• Add data to DB1 at regular intervals by using BULK INSERT.\nWhich option should you specify when you use BULK INSERT, and which\nparameter should you specify when you use CREATE INDEX? To answer, select\nthe appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "371",
    "question": "HOTSPOT\n-\nYou have an Azure subscription that contains an Azure SQL database named\nDB1.\nYou execute the following T-SQL statements against DB1.\nFor each of the following statements, select Yes if the statement is true,\notherwise select No.\nNOTE: Each correct selection is worth one point.\n\n\nExplanation\nCorrect Answer:\nCommunity Discussion\nWrong ---------------- All fields in column3 will occupy 10 bytes on disk: NO. The\nNVARCHAR(10) data type stores variable-length Unicode data. Each character in\nNVARCHAR takes 2 bytes, but since it's variable-length, it will only use as much space as\nneeded for the actual data stored, plus 2 bytes for the length prefix. Dictionary\ncompression will be used to reduce the size of table1 on disk: NO. The\nDATA_COMPRESSION = ROW option specifies row-level compression, not dictionary\ncompression. Row-level compression reduces the storage size by using techniques like\nprefix and dictionary compression, but it is not the same as dictionary compression\nalone. The data types stored in column1, column2 and column4 support compression:\nYES. The data types TINYINT, VARCHAR, and BINARY support row-level compression. Row-\nlevel compression can reduce the storage size of these data types by eliminating\nunnecessary bytes and using more efficient storage formats.",
    "options": [],
    "correct_answer": "",
    "explanation": ""
  },
  {
    "number": "372",
    "question": "You have an Azure subscription that contains a SQL Server on Azure Virtual\nMachines instance named SQLVM1. SQLVM1 has the following configurations:\n• Automated patching is enabled.\n• The SQL Server IaaS Agent extension is installed.\n• The Microsoft SQL Server instance on SQLVM1 is managed by using the Azure\nportal.\nYou need to automate the deployment of cumulative updates to SQLVM1 by\nusing Azure Update Manager. The solution must ensure that the SQL Server\ninstance on SQLVM1 can be managed by using the Azure portal.\nWhat should you do first on SQLVM1?",
    "options": [
      {
        "letter": "A",
        "text": "Install the Azure Monitor Agent.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Uninstall the SQL Server IaaS Agent extension.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Install the Log Analytics agent.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Set Automated patching to Disable.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  },
  {
    "number": "373",
    "question": "You have an Azure subscription. The subscription contains three virtual\nmachines that run Red Hat Enterprise Linux (RHEL).\nYou plan to implement a highly available deployment of Microsoft SQL Server\n2022 on the virtual machines by using failover cluster instances (FCIs).\nYou need to prepare for the implementation.\nWhat should you do first on each server?",
    "options": [
      {
        "letter": "A",
        "text": "Create the FCI disk and network resources.",
        "is_correct": false
      },
      {
        "letter": "B",
        "text": "Install a named SQL Server instance.",
        "is_correct": false
      },
      {
        "letter": "C",
        "text": "Install the cluster resource manager and the FCI resource agent.",
        "is_correct": true
      },
      {
        "letter": "D",
        "text": "Install the default SQL Server instance.",
        "is_correct": false
      }
    ],
    "correct_answer": "C",
    "explanation": "?"
  }
]